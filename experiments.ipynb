{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models import (\n",
    "    NoSkipConnectionSupervenientFeatureNetwork,\n",
    "    SkipConnectionSupervenientFeatureNetwork,\n",
    "    DownwardSmileMIEstimator\n",
    ")\n",
    "from datasets import BitStringDataset, ECoGDataset\n",
    "import lovely_tensors as lt\n",
    "import wandb\n",
    "import tqdm\n",
    "from trainers import train_feature_network\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# training model A's on ecog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdmcsharry\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/vol/bitbucket/dm2223/info-theory-experiments/wandb/run-20240520_223415-ddsz8baf</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dmcsharry/meme/runs/ddsz8baf' target=\"_blank\">lucky-deluge-1</a></strong> to <a href='https://wandb.ai/dmcsharry/meme' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dmcsharry/meme' target=\"_blank\">https://wandb.ai/dmcsharry/meme</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dmcsharry/meme/runs/ddsz8baf' target=\"_blank\">https://wandb.ai/dmcsharry/meme/runs/ddsz8baf</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  13%|█▎        | 4/30 [05:57<38:42, 89.31s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 67\u001b[0m\n\u001b[1;32m     65\u001b[0m project_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeme\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     66\u001b[0m model_dir_prefix \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmeme_modle\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m---> 67\u001b[0m skip_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_feature_network\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_network_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel_dir_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mNEURIPS-FINAL-ecog-f_network-A\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m     73\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/dm2223/info-theory-experiments/trainers.py:197\u001b[0m, in \u001b[0;36mtrain_feature_network\u001b[0;34m(config, trainloader, feature_network_training, project_name, feature_network_A, model_dir_prefix)\u001b[0m\n\u001b[1;32m    195\u001b[0m downward_optims[i]\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    196\u001b[0m channel_i \u001b[38;5;241m=\u001b[39m x0[:, i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m--> 197\u001b[0m downward_MI_i \u001b[38;5;241m=\u001b[39m \u001b[43mdownward_MI_estimators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv1_B\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel_i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m downward_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m downward_MI_i\n\u001b[1;32m    199\u001b[0m downward_loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/vol/bitbucket/dm2223/info-theory-experiments/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/dm2223/info-theory-experiments/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/vol/bitbucket/dm2223/info-theory-experiments/models.py:216\u001b[0m, in \u001b[0;36mDownwardSmileMIEstimator.forward\u001b[0;34m(self, v1, x0i)\u001b[0m\n\u001b[1;32m    213\u001b[0m x0i_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matom_encoder(x0i)\n\u001b[1;32m    215\u001b[0m scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(v1_encoded, x0i_encoded\u001b[38;5;241m.\u001b[39mt())\n\u001b[0;32m--> 216\u001b[0m MI \u001b[38;5;241m=\u001b[39m \u001b[43mestimate_mutual_information\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msmile\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclip\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m MI\n",
      "File \u001b[0;32m/vol/bitbucket/dm2223/info-theory-experiments/smile_estimator.py:157\u001b[0m, in \u001b[0;36mestimate_mutual_information\u001b[0;34m(estimator, scores, baseline_fn, alpha_logit, **kwargs)\u001b[0m\n\u001b[1;32m    155\u001b[0m     mi \u001b[38;5;241m=\u001b[39m js_lower_bound(scores)\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m estimator \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmile\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m--> 157\u001b[0m     mi \u001b[38;5;241m=\u001b[39m \u001b[43msmile_lower_bound\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m estimator \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdv\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    159\u001b[0m     mi \u001b[38;5;241m=\u001b[39m dv_upper_lower_bound(scores)\n",
      "File \u001b[0;32m/vol/bitbucket/dm2223/info-theory-experiments/smile_estimator.py:119\u001b[0m, in \u001b[0;36msmile_lower_bound\u001b[0;34m(f, clip)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    118\u001b[0m     f_ \u001b[38;5;241m=\u001b[39m f\n\u001b[0;32m--> 119\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[43mlogmeanexp_nodiag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    120\u001b[0m dv \u001b[38;5;241m=\u001b[39m (f\u001b[38;5;241m.\u001b[39mdiag()\u001b[38;5;241m.\u001b[39mmean() \u001b[38;5;241m-\u001b[39m z) \n\u001b[1;32m    122\u001b[0m js \u001b[38;5;241m=\u001b[39m js_fgan_lower_bound(f) \n",
      "File \u001b[0;32m/vol/bitbucket/dm2223/info-theory-experiments/smile_estimator.py:23\u001b[0m, in \u001b[0;36mlogmeanexp_nodiag\u001b[0;34m(x, dim, device)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     20\u001b[0m     dim \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     22\u001b[0m logsumexp \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlogsumexp(\n\u001b[0;32m---> 23\u001b[0m     x \u001b[38;5;241m-\u001b[39m torch\u001b[38;5;241m.\u001b[39mdiag(np\u001b[38;5;241m.\u001b[39minf \u001b[38;5;241m*\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m), dim\u001b[38;5;241m=\u001b[39mdim)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dim) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from models import SkipConnectionSupervenientFeatureNetwork\n",
    "from datasets import ECoGDataset\n",
    "from trainers import train_feature_network\n",
    "\n",
    "dataset = ECoGDataset()\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(dataset, batch_size=1000, shuffle=True)\n",
    "\n",
    "for seed in range(4):\n",
    "\n",
    "    torch.manual_seed(seed)\n",
    "\n",
    "    config = {\n",
    "        \"torch_seed\": seed,\n",
    "        \"dataset_type\": \"ecog\",\n",
    "        \"num_atoms\": 64,\n",
    "        \"batch_size\": 1000,\n",
    "        \"train_mode\": True,\n",
    "        \"train_model_B\": False,\n",
    "        \"adjust_Psi\": False,\n",
    "        \"clip\": 5,\n",
    "        \"feature_size\": 3,\n",
    "        \"epochs\": 30,\n",
    "        \"start_updating_f_after\": 300,\n",
    "        \"update_f_every_N_steps\": 5,\n",
    "        \"minimize_neg_terms_until\": 9999999999999,\n",
    "        \"downward_critics_config\": {\n",
    "            \"hidden_sizes_v_critic\": [512, 1024, 1024, 512],\n",
    "            \"hidden_sizes_xi_critic\": [512, 512, 512],\n",
    "            \"critic_output_size\": 32,\n",
    "            \"lr\": 1e-2,\n",
    "            \"bias\": True,\n",
    "            \"weight_decay\": 0,\n",
    "        },\n",
    "        \n",
    "        \"decoupled_critic_config\": {\n",
    "            \"hidden_sizes_encoder_1\": [512, 512, 512],\n",
    "            \"hidden_sizes_encoder_2\": [512, 512, 512],\n",
    "            \"critic_output_size\": 32,\n",
    "            \"lr\": 1e-2,\n",
    "            \"bias\": True,\n",
    "            \"weight_decay\": 0,\n",
    "        },\n",
    "        \"feature_network_config\": {\n",
    "            \"hidden_sizes\": [256, 256, 256, 256, 256],\n",
    "            \"lr\": 1e-3,\n",
    "            \"bias\": True,\n",
    "            \"weight_decay\": 1e-3,\n",
    "        }\n",
    "    }\n",
    "\n",
    "\n",
    "    skip_model = SkipConnectionSupervenientFeatureNetwork(\n",
    "        num_atoms=config['num_atoms'],\n",
    "        feature_size=config['feature_size'],\n",
    "        hidden_sizes=config['feature_network_config']['hidden_sizes'],\n",
    "        include_bias=config['feature_network_config']['bias'],\n",
    "    ).to(device)\n",
    "\n",
    "    # project_name = \"NEURIPS-FINAL-finding-emergent-ecog-features\"\n",
    "    # model_dir_prefix='NEURIPS-FINAL-ecog-f_network-A'\n",
    "    project_name = 'meme'\n",
    "    model_dir_prefix = 'meme_modle'\n",
    "    skip_model = train_feature_network(\n",
    "        config=config,\n",
    "        trainloader=trainloader,\n",
    "        feature_network_training=skip_model,\n",
    "        project_name=project_name,\n",
    "        model_dir_prefix='NEURIPS-FINAL-ecog-f_network-A'\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Oher shit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models import SkipConnectionSupervenientFeatureNetwork\n",
    "from datasets import ECoGDataset\n",
    "from trainers import train_feature_network\n",
    "\n",
    "seed = 0\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "\n",
    "\n",
    "dataset = ECoGDataset()\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "trainloader = torch.utils.data.DataLoader(dataset, batch_size=1000, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "config = {\n",
    "    \"torch_seed\": seed,\n",
    "    \"dataset_type\": \"ecog\",\n",
    "    \"num_atoms\": 64,\n",
    "    \"batch_size\": 1000,\n",
    "    \"train_mode\": True,\n",
    "    \"train_model_B\": False,\n",
    "    \"adjust_Psi\": True,\n",
    "    \"clip\": 5,\n",
    "    \"feature_size\": 3,\n",
    "    \"epochs\": 20,\n",
    "    \"start_updating_f_after\": 500,\n",
    "    \"update_f_every_N_steps\": 5,\n",
    "    \"minimize_neg_terms_until\": 0,\n",
    "    \"downward_critics_config\": {\n",
    "        \"hidden_sizes_v_critic\": [512, 1024, 1024, 512],\n",
    "        \"hidden_sizes_xi_critic\": [512, 512, 512],\n",
    "        \"critic_output_size\": 32,\n",
    "        \"lr\": 1e-3,\n",
    "        \"bias\": True,\n",
    "        \"weight_decay\": 0,\n",
    "    },\n",
    "    \"decoupled_critic_config\": {\n",
    "        \"hidden_sizes_encoder_1\": [512, 1024, 1024, 512],\n",
    "        \"hidden_sizes_encoder_2\": [512, 1024, 1024, 512],\n",
    "        \"critic_output_size\": 32,\n",
    "        \"lr\": 1e-3,\n",
    "        \"bias\": True,\n",
    "        \"weight_decay\": 0,\n",
    "    },\n",
    "    \"feature_network_config\": {\n",
    "        \"hidden_sizes\": [256, 256, 256, 256, 256],\n",
    "        \"lr\": 1e-4,\n",
    "        \"bias\": True,\n",
    "        \"weight_decay\": 1e-3,\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "model_path = \"/vol/bitbucket/dm2223/info-theory-experiments/models/feature_network_earthy-sun-6.pth\"\n",
    "\n",
    "\n",
    "feature_network = SkipConnectionSupervenientFeatureNetwork(\n",
    "    num_atoms=config['num_atoms'],\n",
    "    feature_size=config['feature_size'],\n",
    "    hidden_sizes=config['feature_network_config']['hidden_sizes'],\n",
    "    include_bias=config['feature_network_config']['bias'],\n",
    ").to(device)\n",
    "\n",
    "feature_network.load_state_dict(torch.load(model_path))\n",
    "\n",
    "project_name = \"training an ecog network on full Psi after infomin\"\n",
    "\n",
    "\n",
    "feature_network = train_feature_network(\n",
    "    config=config,\n",
    "    trainloader=trainloader,\n",
    "    feature_network_training=feature_network,\n",
    "    project_name=project_name\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
