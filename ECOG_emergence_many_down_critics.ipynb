{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7b00c40568b0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import lovely_tensors as lt\n",
    "import wandb\n",
    "import tqdm\n",
    "from models import (SupervenientFeatureNetwork,\n",
    "                    CLUB,\n",
    "                    DecoupledSmileMIEstimator,\n",
    "                    DownwardSmileMIEstimator,\n",
    "                    SkipConnectionSupervenientFeatureNetwork\n",
    "                    )\n",
    "lt.monkey_patch()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "from utils import ECoGDataset\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "print(device)\n",
    "\n",
    "torch.manual_seed(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a feature network for a given config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_feature_network(config, trainloader, feature_network):\n",
    "\n",
    "    wandb.init(project=\"ecog-dataset-neurips\", config=config)\n",
    "    # init weights to zero of the feature network\n",
    "\n",
    "    decoupled_MI_estimator = DecoupledSmileMIEstimator(\n",
    "        feature_size=config['feature_size'],\n",
    "        critic_output_size=config['critic_output_size'],\n",
    "        hidden_sizes_1=config['decoupled_critic_hidden_sizes_1'],\n",
    "        hidden_sizes_2=config['decoupled_critic_hidden_sizes_2'],\n",
    "        clip=config['clip'],\n",
    "        include_bias=config['bias']\n",
    "        ).to(device)\n",
    "    downward_MI_estimators = [\n",
    "        DownwardSmileMIEstimator(\n",
    "            feature_size=config['feature_size'],\n",
    "            critic_output_size=config['critic_output_size'],\n",
    "            hidden_sizes_v_critic=config['downward_hidden_sizes_v_critic'],\n",
    "            hidden_sizes_xi_critic=config['downward_hidden_sizes_xi_critic'],\n",
    "            clip=config['clip'],\n",
    "            include_bias=config['bias']\n",
    "            ).to(device) \n",
    "        for _ in range(config['num_atoms'])\n",
    "    ]\n",
    "    \n",
    "\n",
    "    feature_optimizer = torch.optim.Adam(\n",
    "        feature_network.parameters(),\n",
    "        lr=config[\"feature_lr\"],\n",
    "        weight_decay=config[\"weight_decay\"]\n",
    "    )\n",
    "    decoupled_optimizer = torch.optim.Adam(\n",
    "        decoupled_MI_estimator.parameters(),\n",
    "        lr=config[\"decoupled_critic_lr\"],\n",
    "        weight_decay=config[\"weight_decay\"]\n",
    "    )\n",
    "    downward_optims = [\n",
    "        torch.optim.Adam(\n",
    "            dc.parameters(),\n",
    "            lr=config[\"downward_lr\"],\n",
    "            weight_decay=config[\"weight_decay\"]\n",
    "        ) \n",
    "        for dc in downward_MI_estimators\n",
    "    ]\n",
    "\n",
    "\n",
    "    # TODO: figure out why only f network is being watched, I would like to keep a closer eye on the grad n params.\n",
    "    # TODO: Look at how GANs are trained with pytorch and make sure I'm not doing anything unreasonable.\n",
    "    # Eg, https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py \n",
    "    # ^ this does not require retain_graph=True, so maybe this can be optomized somehow\n",
    "    wandb.watch(feature_network, log='all')\n",
    "    wandb.watch(decoupled_MI_estimator, log=\"all\")\n",
    "    for dc in downward_MI_estimators:\n",
    "        wandb.watch(dc, log='all')\n",
    "\n",
    "    ##\n",
    "    ## TRAIN FEATURE NETWORK\n",
    "    ##\n",
    "\n",
    "    epochs = config['epochs']\n",
    "\n",
    "    step = 0\n",
    "\n",
    "    for _ in tqdm.tqdm(range(epochs), desc='Training'):\n",
    "        for batch_num, batch in enumerate(trainloader):\n",
    "            x0 = batch[:, 0].to(device).float()\n",
    "            x1 = batch[:, 1].to(device).float()\n",
    "            v0 = feature_network(x0).detach()\n",
    "            v1 = feature_network(x1).detach()\n",
    "\n",
    "            # update decoupled critic\n",
    "            decoupled_optimizer.zero_grad()\n",
    "            decoupled_MI = decoupled_MI_estimator(v0, v1)\n",
    "            decoupled_loss = -decoupled_MI\n",
    "            decoupled_loss.backward(retain_graph=True)\n",
    "            decoupled_optimizer.step()\n",
    "\n",
    "            # update each downward critic \n",
    "            for i in range(config['num_atoms']):\n",
    "                downward_optims[i].zero_grad()\n",
    "                channel_i = x0[:, i].unsqueeze(1).detach()\n",
    "                downward_MI_i = downward_MI_estimators[i](v1, channel_i)\n",
    "                downward_loss = - downward_MI_i\n",
    "                downward_loss.backward(retain_graph=True)\n",
    "                downward_optims[i].step()\n",
    "                wandb.log({\n",
    "                    f\"downward_MI_{i}\": downward_MI_i   \n",
    "                }, step=step)\n",
    "\n",
    "            # update feature network   \n",
    "            feature_optimizer.zero_grad()\n",
    "            channel_MIs = []\n",
    "\n",
    "            MIs = []\n",
    "            v0 = feature_network(x0)\n",
    "            v1 = feature_network(x1)\n",
    "\n",
    "            for i in range(config['num_atoms']):\n",
    "                channel_i = x0[:, i].unsqueeze(1)\n",
    "                channel_i_MI = downward_MI_estimators[i](v1, channel_i)\n",
    "                channel_MIs.append(channel_i_MI)\n",
    "                MIs.append(channel_i_MI)\n",
    "\n",
    "            sum_downward_MI = sum(channel_MIs)\n",
    "\n",
    "            decoupled_MI1 = decoupled_MI_estimator(v0, v1)\n",
    "\n",
    "            clipped_min_MIs = max(0, min(MIs))\n",
    "\n",
    "            Psi = decoupled_MI1 - sum_downward_MI + (config['num_atoms'] - 1) * clipped_min_MIs\n",
    "\n",
    "            # NOTE an experiment\n",
    "            feature_loss = sum_downward_MI \n",
    "\n",
    "\n",
    "            if config['start_updating_f_after'] < step:\n",
    "                if batch_num % config['update_f_every_N_steps'] == 0:\n",
    "                    feature_loss.backward(retain_graph=True)\n",
    "                    feature_optimizer.step()\n",
    "\n",
    "            wandb.log({\n",
    "                \"decoupled_MI\": decoupled_MI1,\n",
    "                \"sum_downward_MI\": sum_downward_MI,\n",
    "                \"Psi\": Psi,\n",
    "            }, step=step)\n",
    "\n",
    "\n",
    "            step += 1\n",
    "        \n",
    "    torch.save(feature_network.state_dict(), f\"models/ecog_feature_network_{wandb.run.name}.pth\")\n",
    "    \n",
    "    return feature_network\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"batch_size\": 1000,\n",
    "    \"num_atoms\": 64,\n",
    "    \"feature_size\": 3,\n",
    "    \"clip\": 5,\n",
    "    \"epochs\": 50,\n",
    "    \"critic_output_size\": 32,\n",
    "    \"downward_hidden_sizes_v_critic\": [512, 512, 512, 512],\n",
    "    \"downward_hidden_sizes_xi_critic\": [512, 512, 512],\n",
    "    \"feature_hidden_sizes\": [256, 256, 256, 256, 256],\n",
    "    \"decoupled_critic_hidden_sizes_1\": [512, 512, 512],\n",
    "    \"decoupled_critic_hidden_sizes_2\": [512, 512, 512],\n",
    "    \"feature_lr\": 1e-4,\n",
    "    \"decoupled_critic_lr\": 1e-3,\n",
    "    \"downward_lr\": 1e-3,    \n",
    "    \"bias\": True,\n",
    "    \"update_f_every_N_steps\": 5,\n",
    "    \"weight_decay\": 0,\n",
    "    \"start_updating_f_after\": 300,\n",
    "    \"add_spec_norm_downward\": False,\n",
    "    \"add_spec_norm_decoupled\": False\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = ECoGDataset()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:bssv2jau) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Psi</td><td>▁▁▄█▅▄▆▄▆▆▄▃▆▆▆▃▇▅▅▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇</td></tr><tr><td>decoupled_MI</td><td>▁▃▂▂▂▄▅▃▃▄▃▃▃▄▆█▁▁▂▂▂▂▂▃▂▂▂▂▂▂▂▃▂▂▁▁▁▁▁▁</td></tr><tr><td>downward_MI_0</td><td>▆█▄▁▃▃▃▄▃▁▃▄▂▂▁▂▂▂▃▁▂▂▁▂▂▁▁▁▁▁▁▁▁▁▁▂▁▂▁▁</td></tr><tr><td>downward_MI_1</td><td>▇█▅▂▄▆▆▄▄▄▆▇▃▄▄▅▃▄▅▃▃▃▃▃▃▂▂▂▂▂▂▂▂▂▂▂▃▃▃▁</td></tr><tr><td>downward_MI_10</td><td>▆█▃▁▃▄▄▃▃▂▄█▂▃▂▄▂▃▃▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▂▁</td></tr><tr><td>downward_MI_11</td><td>▅▇▄▁▃▄▂▃▃▁▃█▃▃▂▃▁▄▅▂▂▁▁▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>downward_MI_12</td><td>▇█▄▁▃▃▃▃▃▁▂▄▂▂▃▆▂▃▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>downward_MI_13</td><td>█▅▄▁▃▄▃▃▂▂▃▇▃▃▂▄▂▄▄▂▂▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▂▁▁</td></tr><tr><td>downward_MI_14</td><td>▆█▄▁▃▄▆▃▂▁▃▅▂▂▂▄▂▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>downward_MI_15</td><td>▆█▄▁▃▅▄▃▃▂▂▆▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>downward_MI_16</td><td>▆█▄▁▃▄▃▄▃▂▅▄▂▃▃█▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>downward_MI_17</td><td>▇█▄▁▂▄▃▄▃▃▇▆▃▄▄▃▂▄▅▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂</td></tr><tr><td>downward_MI_18</td><td>▇▆▅▁▂▅▄▂▄▂▅▆▂▂▃█▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>downward_MI_19</td><td>▇█▅▁▄▄▃▄▃▂▄▅▂▄▃▅▃▃▃▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▁▂▁</td></tr><tr><td>downward_MI_2</td><td>▇▇▅▁▄▅▄▅▄▂▇█▃▄▃▅▂▄▆▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▁</td></tr><tr><td>downward_MI_20</td><td>▆█▄▁▃▅▄▄▃▂▅▆▂▃▄▄▂▅▄▂▂▂▂▃▂▂▂▂▂▂▂▁▁▂▂▂▂▂▂▁</td></tr><tr><td>downward_MI_21</td><td>▇▆▄▁▄▅▄▅▃▂▆█▃▃▂▃▂▃▅▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂</td></tr><tr><td>downward_MI_22</td><td>▅█▃▁▃▄▃▃▃▁▂▄▂▃▃▆▂▂▂▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▂▁▁▁</td></tr><tr><td>downward_MI_23</td><td>█▇▅▁▃▅▄▄▃▅▅▄▂▃▄▄▃▄▄▂▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>downward_MI_24</td><td>██▅▂▃▆▄▅▃▃▆▅▃▃▃▄▃▃▃▃▃▃▃▃▃▃▃▃▁▂▂▂▂▂▂▂▃▃▃▃</td></tr><tr><td>downward_MI_25</td><td>██▄▁▄▄▃▃▃▂▄▇▂▃▃▄▁▂▄▂▃▂▂▂▁▁▁▂▁▁▁▁▁▁▁▁▁▁▁▂</td></tr><tr><td>downward_MI_26</td><td>▇█▄▁▃▄▅▄▃▂▄▆▂▃▃█▃▃▃▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>downward_MI_27</td><td>▇█▄▁▄▄▃▄▃▂▄▃▂▃▃█▃▃▃▂▂▂▂▂▁▂▂▂▁▁▂▁▁▁▂▂▂▂▁▂</td></tr><tr><td>downward_MI_28</td><td>▇█▄▁▂▅▃▄▂▂▄▄▃▃▃█▃▃▄▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>downward_MI_29</td><td>██▄▁▄▆▄▄▂▄▆▅▂▃▃▇▃▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>downward_MI_3</td><td>▅█▄▁▃▅▃▄▃▂▄▅▂▃▃▇▂▃▃▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>downward_MI_30</td><td>█▆▅▁▄▇▄▅▄▂▃▇▄▃▂▆▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>downward_MI_31</td><td>██▄▁▃▅▂▂▃▂▃▃▂▂▂▃▂▃▃▂▃▃▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>downward_MI_32</td><td>█▇▅▁▄▅▂▂▃▅▅▄▂▂▂▇▂▂▃▂▃▃▂▂▃▂▂▂▂▂▂▂▂▂▂▁▂▂▂▃</td></tr><tr><td>downward_MI_33</td><td>██▂▁▄▅▄▄▃▃▆▅▂▃▄▅▂▄▄▃▃▂▂▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>downward_MI_34</td><td>█▇▅▁▅▇▅▅▃▅▆▆▃▅▆█▃▄▅▃▄▃▃▄▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂</td></tr><tr><td>downward_MI_35</td><td>▆▇▄▁▄▅▃▄▃▂▆█▃▃▂▅▂▃▄▂▃▃▁▁▁▂▁▂▁▁▁▁▁▁▁▁▁▁▂▂</td></tr><tr><td>downward_MI_36</td><td>▆█▄▁▄▄▅▄▃▂▅█▃▃▃▄▂▄▄▂▂▂▂▃▂▂▂▂▂▂▂▁▁▁▂▂▂▂▂▁</td></tr><tr><td>downward_MI_37</td><td>▇█▄▁▄▄▃▄▃▄▄▄▂▃▄█▃▄▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁</td></tr><tr><td>downward_MI_38</td><td>██▄▁▄▄▆▄▃▂▃▆▃▄▄▄▂▃▃▁▂▂▂▂▂▁▂▂▁▁▂▁▁▁▂▁▂▁▁▁</td></tr><tr><td>downward_MI_39</td><td>▆▆▄▁▄▄▃▄▃▂▅▄▂▃▃█▃▄▄▂▂▂▂▂▂▂▂▁▂▂▂▂▂▁▂▂▂▂▂▁</td></tr><tr><td>downward_MI_4</td><td>██▄▁▃▄▃▄▃▂▅▇▃▃▂▅▂▅▅▂▂▂▁▂▁▁▁▂▁▁▁▁▁▁▁▂▁▂▂▁</td></tr><tr><td>downward_MI_40</td><td>██▅▁▃▅▃▅▄▃▅▆▃▄▃█▂▄▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>downward_MI_41</td><td>█▇▄▁▄▄▃▄▃▂▄▄▂▃▄▇▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>downward_MI_42</td><td>▇█▄▁▄▄▄▁▃▂▆▆▂▃▄▄▂▆▇▂▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>downward_MI_43</td><td>█▇▄▁▄▅▂▅▃▂▃▅▃▃▂▅▂▃▄▂▃▂▂▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂</td></tr><tr><td>downward_MI_44</td><td>█▇▄▁▄▄▃▄▃▂▅▆▂▃▄▄▂▄▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂</td></tr><tr><td>downward_MI_45</td><td>▆█▅▁▄▅▄▅▃▂▅▇▂▃▃▇▂▂▄▂▃▂▂▃▂▁▁▂▂▂▂▁▂▁▁▂▂▂▂▂</td></tr><tr><td>downward_MI_46</td><td>▅█▄▁▂▄▂▄▃▂▂▃▃▃▃▇▂▃▄▂▂▂▂▁▂▁▂▂▂▁▁▁▁▁▁▂▂▁▁▁</td></tr><tr><td>downward_MI_47</td><td>█▆▃▁▄▄▄▃▂▁▂▅▃▂▃▄▂▃▅▂▂▂▂▂▂▂▁▁▁▁▁▁▁▁▁▂▂▁▂▂</td></tr><tr><td>downward_MI_48</td><td>██▄▂▄▄▃▄▃▂▃▅▂▂▃▇▃▂▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂▃▂</td></tr><tr><td>downward_MI_49</td><td>██▅▁▄▆▃▅▃▂▆▇▃▂▂▄▂▃▄▂▃▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>downward_MI_5</td><td>▅█▄▁▂▄▄▄▄▂▅▅▂▁▁▃▁▂▄▁▂▁▂▂▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>downward_MI_50</td><td>▆▅▄▁▃▄▃▄▃▂▄▃▂▂▃█▃▂▃▃▃▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▁▂▂</td></tr><tr><td>downward_MI_51</td><td>█▇▄▁▃▅▆▄▃▇▂▄▂▄▅▄▃▃▄▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁</td></tr><tr><td>downward_MI_52</td><td>▆█▄▁▄▄▂▄▃▂▄▄▂▂▃▇▂▃▄▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▂▂▁▂▂</td></tr><tr><td>downward_MI_53</td><td>▇█▄▁▄▄▄▅▃▂▅▅▂▂▄▇▂▂▄▁▂▁▂▂▂▂▂▂▁▂▁▁▁▁▂▂▂▂▂▁</td></tr><tr><td>downward_MI_54</td><td>▇█▄▁▃▅▄▄▃▃▅▄▂▃▄▅▂▃▄▂▃▃▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>downward_MI_55</td><td>▆▆▅▃▄▅▄▅▄▃▅▅▃▄▄█▄▃▃▃▄▄▃▃▄▃▃▃▃▃▃▃▃▃▃▃▃▃▁▃</td></tr><tr><td>downward_MI_56</td><td>█▇▄▁▄▃▂▄▂▁▂▅▂▂▂▅▂▃▅▂▂▂▂▂▂▂▂▂▂▁▂▁▁▁▁▂▂▂▂▂</td></tr><tr><td>downward_MI_57</td><td>▇█▄▁▃▄▄▄▃▂▃▅▂▂▃█▂▃▃▂▂▂▂▂▂▂▁▂▁▂▂▂▂▁▂▂▂▂▁▂</td></tr><tr><td>downward_MI_58</td><td>█▆▄▁▃▄▃▄▃▂▄▄▂▃▄▆▂▃▃▂▂▂▂▃▂▂▂▂▂▁▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>downward_MI_59</td><td>▇█▄▁▄▄▄▄▂▂▄▄▂▃▄▇▂▂▄▂▂▂▂▂▁▂▂▂▁▁▂▁▁▁▂▂▂▂▂▁</td></tr><tr><td>downward_MI_6</td><td>██▅▁▄▆▄▅▄▃▆▆▂▄▃█▃▃▅▂▃▃▁▂▃▂▂▁▂▂▂▂▂▂▂▂▂▁▂▂</td></tr><tr><td>downward_MI_60</td><td>██▅▁▃▄▄▄▃▂▅▇▃▂▂▄▂▄▅▃▃▃▂▃▂▂▂▂▂▂▁▂▂▂▂▂▂▂▃▂</td></tr><tr><td>downward_MI_61</td><td>▆▆▄▁▄▄▃▄▃▂▄▄▁▃▅█▂▃▃▁▃▃▁▂▂▂▁▁▁▂▁▁▁▁▂▂▂▂▂▂</td></tr><tr><td>downward_MI_62</td><td>▆▆▄▁▃▅▃▄▃▃▆█▂▂▃▆▂▃▃▂▂▂▂▂▂▁▂▂▁▂▂▂▂▁▂▂▂▂▂▁</td></tr><tr><td>downward_MI_63</td><td>▅▅▄▁▃▃▄▄█▅▆▅▃▄▆█▅▆▆▅▅▅▅▇▇▁▆▅▅▄▆▅▄▅▆▄▄▄▆▄</td></tr><tr><td>downward_MI_7</td><td>██▄▁▄▆▃▅▃▂▂▄▃▃▂▂▂▄▆▂▂▂▁▃▂▂▂▁▂▂▂▁▁▁▂▂▂▂▂▂</td></tr><tr><td>downward_MI_8</td><td>██▄▁▃▅▃▄▃▂▅▅▂▂▂▃▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▁▂▂</td></tr><tr><td>downward_MI_9</td><td>▇█▃▁▃▅▅▃▃▂▅█▃▃▃▅▂▅▆▂▃▂▁▂▂▁▁▂▁▁▁▁▁▁▁▁▂▂▂▁</td></tr><tr><td>sum_downward_MI</td><td>██▄▁▄▅▄▄▃▂▅▆▃▃▃▆▂▃▄▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Psi</td><td>-1.64324</td></tr><tr><td>decoupled_MI</td><td>0.01554</td></tr><tr><td>downward_MI_0</td><td>0.02653</td></tr><tr><td>downward_MI_1</td><td>0.03032</td></tr><tr><td>downward_MI_10</td><td>0.01099</td></tr><tr><td>downward_MI_11</td><td>0.04352</td></tr><tr><td>downward_MI_12</td><td>0.01092</td></tr><tr><td>downward_MI_13</td><td>0.00119</td></tr><tr><td>downward_MI_14</td><td>0.00078</td></tr><tr><td>downward_MI_15</td><td>0.01046</td></tr><tr><td>downward_MI_16</td><td>0.0178</td></tr><tr><td>downward_MI_17</td><td>0.02251</td></tr><tr><td>downward_MI_18</td><td>0.00629</td></tr><tr><td>downward_MI_19</td><td>0.0313</td></tr><tr><td>downward_MI_2</td><td>0.001</td></tr><tr><td>downward_MI_20</td><td>0.02846</td></tr><tr><td>downward_MI_21</td><td>0.01061</td></tr><tr><td>downward_MI_22</td><td>-0.00717</td></tr><tr><td>downward_MI_23</td><td>0.03866</td></tr><tr><td>downward_MI_24</td><td>0.08896</td></tr><tr><td>downward_MI_25</td><td>0.0227</td></tr><tr><td>downward_MI_26</td><td>0.03355</td></tr><tr><td>downward_MI_27</td><td>0.03767</td></tr><tr><td>downward_MI_28</td><td>0.01472</td></tr><tr><td>downward_MI_29</td><td>0.03807</td></tr><tr><td>downward_MI_3</td><td>0.03571</td></tr><tr><td>downward_MI_30</td><td>0.0044</td></tr><tr><td>downward_MI_31</td><td>-0.00851</td></tr><tr><td>downward_MI_32</td><td>0.08682</td></tr><tr><td>downward_MI_33</td><td>0.03061</td></tr><tr><td>downward_MI_34</td><td>0.07023</td></tr><tr><td>downward_MI_35</td><td>0.00637</td></tr><tr><td>downward_MI_36</td><td>-0.00261</td></tr><tr><td>downward_MI_37</td><td>-0.00313</td></tr><tr><td>downward_MI_38</td><td>0.04037</td></tr><tr><td>downward_MI_39</td><td>0.02053</td></tr><tr><td>downward_MI_4</td><td>0.01091</td></tr><tr><td>downward_MI_40</td><td>-0.00326</td></tr><tr><td>downward_MI_41</td><td>0.10148</td></tr><tr><td>downward_MI_42</td><td>-0.00071</td></tr><tr><td>downward_MI_43</td><td>0.00162</td></tr><tr><td>downward_MI_44</td><td>0.04248</td></tr><tr><td>downward_MI_45</td><td>0.03432</td></tr><tr><td>downward_MI_46</td><td>0.00837</td></tr><tr><td>downward_MI_47</td><td>0.04262</td></tr><tr><td>downward_MI_48</td><td>0.03398</td></tr><tr><td>downward_MI_49</td><td>0.01531</td></tr><tr><td>downward_MI_5</td><td>0.00776</td></tr><tr><td>downward_MI_50</td><td>0.0684</td></tr><tr><td>downward_MI_51</td><td>0.00649</td></tr><tr><td>downward_MI_52</td><td>0.03856</td></tr><tr><td>downward_MI_53</td><td>0.01825</td></tr><tr><td>downward_MI_54</td><td>0.0247</td></tr><tr><td>downward_MI_55</td><td>0.02848</td></tr><tr><td>downward_MI_56</td><td>0.05611</td></tr><tr><td>downward_MI_57</td><td>0.03678</td></tr><tr><td>downward_MI_58</td><td>0.03057</td></tr><tr><td>downward_MI_59</td><td>0.029</td></tr><tr><td>downward_MI_6</td><td>0.04449</td></tr><tr><td>downward_MI_60</td><td>0.01984</td></tr><tr><td>downward_MI_61</td><td>0.09311</td></tr><tr><td>downward_MI_62</td><td>-0.0026</td></tr><tr><td>downward_MI_63</td><td>0.06349</td></tr><tr><td>downward_MI_7</td><td>0.04532</td></tr><tr><td>downward_MI_8</td><td>0.07705</td></tr><tr><td>downward_MI_9</td><td>-0.00274</td></tr><tr><td>sum_downward_MI</td><td>1.65878</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">amber-dew-9</strong> at: <a href='https://wandb.ai/dmcsharry/ecog-dataset-neurips/runs/bssv2jau' target=\"_blank\">https://wandb.ai/dmcsharry/ecog-dataset-neurips/runs/bssv2jau</a><br/> View project at: <a href='https://wandb.ai/dmcsharry/ecog-dataset-neurips' target=\"_blank\">https://wandb.ai/dmcsharry/ecog-dataset-neurips</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240506_051733-bssv2jau/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:bssv2jau). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/vol/bitbucket/dm2223/info-theory-experiments/wandb/run-20240506_062906-plzyd28y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dmcsharry/ecog-dataset-neurips/runs/plzyd28y' target=\"_blank\">expert-snow-10</a></strong> to <a href='https://wandb.ai/dmcsharry/ecog-dataset-neurips' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dmcsharry/ecog-dataset-neurips' target=\"_blank\">https://wandb.ai/dmcsharry/ecog-dataset-neurips</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dmcsharry/ecog-dataset-neurips/runs/plzyd28y' target=\"_blank\">https://wandb.ai/dmcsharry/ecog-dataset-neurips/runs/plzyd28y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 50/50 [1:03:49<00:00, 76.59s/it]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "feature_newtork = SkipConnectionSupervenientFeatureNetwork(\n",
    "    num_atoms=config['num_atoms'],\n",
    "    feature_size=config['feature_size'],\n",
    "    hidden_sizes=config['feature_hidden_sizes'],\n",
    "    include_bias=config['bias']\n",
    ").to(device)\n",
    "\n",
    "feature_network = train_feature_network(config, train_loader, feature_newtork)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:jkhdjcdp) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Psi</td><td>▂▅▇▄▅▆▄▆▅▇▆▆▇▆▆▇▄▇▇▆▇▇▇▇█▇▇█▇█▇▇▇▇▇▇█▇▁█</td></tr><tr><td>decoupled_MI</td><td>▁▃▅▅▅▅▆▅▆▆▆▆▆▆▆▇▇▆▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇</td></tr><tr><td>downward_MI_0</td><td>█▅▄▇▆▃▅▃▅▃▂▃▄▃▂▅█▂▂▃▃▁▂▃▂▂▂▁▃▂▃▂▂▂▂▄▂▃▇▂</td></tr><tr><td>downward_MI_1</td><td>█▃▃▄▄▃▂▃▃▂▁▂▃▂▂▃▆▂▂▃▃▁▂▃▁▂▂▁▃▁▂▂▂▂▂▃▂▂▅▂</td></tr><tr><td>downward_MI_10</td><td>█▄▄▃▄▅▂▅▅▄▃▂▄▄▃▅▇▃▂▄▃▁▃▃▂▂▃▁▄▂▃▃▃▃▂▄▃▂▃▃</td></tr><tr><td>downward_MI_11</td><td>▅▃▃▄▅▃▄▃▄▂▁▂▂▂▂▃▅▂▂▂▃▁▂▃▁▂▂▁▂▂▃▂▁▁▁▂▂▂█▂</td></tr><tr><td>downward_MI_12</td><td>▅▃▂▂▃▂▂▂▃▂▂▂▂▂▂▂▃▂▂▂▂▁▂▂▁▂▂▁▂▂▂▂▂▂▁▂▂▂█▂</td></tr><tr><td>downward_MI_13</td><td>█▅▄▅▆▄▄▅▄▄▂▃▄▄▃▇▇▃▃▅▄▁▃▄▂▂▃▁▄▂▄▄▃▃▂▄▃▂█▄</td></tr><tr><td>downward_MI_14</td><td>▄▂▃▄▃▂▃▃▃▂▂▂▂▃▂▂▃▂▁▂▂▁▂▂▁▁▂▁▂▁▂▂▂▁▁▂▂▂█▂</td></tr><tr><td>downward_MI_15</td><td>█▄▃▆█▄▂▄▄▄▃▃▃▄▃▃▅▃▃▃▄▁▃▄▂▂▃▁▃▂▃▃▂▂▂▃▂▃▇▃</td></tr><tr><td>downward_MI_16</td><td>█▅▄▄▄▃▂▂▅▂▂▃▃▃▂▄▅▂▂▃▄▁▂▂▁▁▁▁▃▁▂▂▁▁▂▃▂▂▄▂</td></tr><tr><td>downward_MI_17</td><td>█▄▃▃▅▃▂▂▄▂▂▃▃▃▂▃▄▂▂▃▄▁▂▂▁▁▁▁▃▁▂▂▂▁▂▂▁▂▃▂</td></tr><tr><td>downward_MI_18</td><td>█▅▄▄▆▄▂▂▅▃▂▃▃▂▂▅▇▂▂▃▅▁▂▂▁▁▁▁▃▁▂▂▂▁▂▃▂▂▄▂</td></tr><tr><td>downward_MI_19</td><td>█▄▄▅▄▄▂▂▄▃▁▂▃▃▃▄▅▂▂▃▄▁▂▃▁▂▁▁▃▁▂▂▂▁▂▂▂▂▂▂</td></tr><tr><td>downward_MI_2</td><td>█▆▄▅▆▃▃▃▅▃▂▃▃▃▃▄█▂▂▃▄▁▃▄▂▂▂▁▄▂▃▃▂▂▂▄▂▃▄▂</td></tr><tr><td>downward_MI_20</td><td>▇▅▄▆▄▃▂▂▅▃▂▂▄▂▂▄█▂▂▃▃▁▂▄▁▂▁▁▃▂▃▂▂▁▂▃▁▂▅▂</td></tr><tr><td>downward_MI_21</td><td>█▄▄▄▄▃▁▃▄▂▂▃▃▃▂▃▆▂▂▃▃▁▂▂▁▁▁▁▃▁▂▂▁▁▂▃▂▃▂▂</td></tr><tr><td>downward_MI_22</td><td>█▅▄▅▄▄▂▂▅▂▂▃▃▂▂▅▆▂▂▃▄▁▂▃▁▂▁▁▃▁▂▂▂▁▂▂▂▂▄▂</td></tr><tr><td>downward_MI_23</td><td>█▆▅▆▃▄▂▃▅▃▂▃▃▃▃▆▇▂▂▄▄▁▂▃▁▂▂▁▃▁▄▂▂▂▂▄▁▃▄▂</td></tr><tr><td>downward_MI_24</td><td>▃▂▂▃▂▂▂▂▂▂▁▂▂▁▁▂▂▁▁▂▂▁▁▂▁▁▁▁▁▁▂▁▁▁▁▂▁▂█▂</td></tr><tr><td>downward_MI_25</td><td>█▅▄▄▆▃▁▃▆▃▃▄▃▃▃▅▇▂▂▄▅▁▂▃▂▂▂▂▄▂▃▃▂▂▂▄▂▃▄▂</td></tr><tr><td>downward_MI_26</td><td>█▄▄▄▄▃▁▂▄▂▂▃▂▂▂▅▇▂▂▃▃▁▂▂▁▂▁▁▃▁▂▂▁▁▂▃▂▂▄▂</td></tr><tr><td>downward_MI_27</td><td>█▅▄▃▄▃▂▂▅▃▃▃▃▃▃▅▅▂▂▃▄▁▂▂▁▂▂▁▃▂▂▂▂▂▂▃▁▂▄▂</td></tr><tr><td>downward_MI_28</td><td>█▄▄▆▆▄▄▄▅▂▁▂▃▂▃▅▇▂▂▃▃▁▂▂▁▂▂▁▃▂▃▂▂▂▂▄▂▂▄▂</td></tr><tr><td>downward_MI_29</td><td>█▄▄▄▄▅▃▃▅▃▁▃▃▄▃▆█▂▂▄▄▁▂▂▁▂▂▁▄▁▂▃▂▂▂▃▁▃▆▂</td></tr><tr><td>downward_MI_3</td><td>█▄▄▆▅▃▃▃▄▃▂▂▃▃▂▅▆▂▂▃▃▁▃▂▁▂▂▁▄▁▃▃▂▂▂▄▂▂▆▂</td></tr><tr><td>downward_MI_30</td><td>▇▄▃▃▆▄▃▃▄▃▂▂▃▃▂▃▆▂▂▃▃▁▂▂▁▂▂▁▃▁▂▃▂▂▂▃▂▂█▂</td></tr><tr><td>downward_MI_31</td><td>█▄▄▃▄▄▄▄▄▃▁▃▃▃▃▄▇▂▂▄▃▁▂▃▁▂▂▁▃▂▂▃▂▂▂▃▂▃▇▂</td></tr><tr><td>downward_MI_32</td><td>▆▅▃▄▅▄▄▃▆▂▂▂▃▄▃▄▆▂▂▂▄▁▂▄▂▁▁▁▂▁▂▂▁▁▂▂▁▃█▂</td></tr><tr><td>downward_MI_33</td><td>▇▅▄▄▅▅▂▃▆▃▂▃▄▃▂▄▅▂▂▂▄▂▂▄▂▁▁▂▄▂▂▃▁▁▂▂▁▄█▂</td></tr><tr><td>downward_MI_34</td><td>▆▅▄▆▃▅▃▃█▃▃▂▄▃▃▅▆▂▂▂▄▂▂▄▂▂▁▁▄▁▂▂▂▁▂▂▁▃▇▂</td></tr><tr><td>downward_MI_35</td><td>█▅▅▅▆▄▂▄▇▃▂▃▄▄▃▆▅▂▃▃▄▂▂▄▁▁▁▁▄▂▂▂▂▁▂▂▁▃▆▂</td></tr><tr><td>downward_MI_36</td><td>█▅▄▆▃▃▂▂▅▂▂▃▄▂▂▄▇▃▂▂▄▁▂▃▁▂▁▁▃▁▂▂▂▁▂▃▂▂▃▂</td></tr><tr><td>downward_MI_37</td><td>▇▅▄▅▃▄▂▃▆▃▂▃▄▃▂▄█▂▁▃▄▁▂▃▁▁▁▁▄▁▂▂▁▁▂▂▁▂▄▂</td></tr><tr><td>downward_MI_38</td><td>█▅▅█▄▄▃▄█▃▂▃▄▃▃▅█▃▂▂▅▂▂▃▁▁▁▁▄▂▂▂▁▁▂▂▁▃▄▂</td></tr><tr><td>downward_MI_39</td><td>█▅▄▃▃▃▁▂▅▂▃▃▃▂▂▄▇▂▂▂▃▁▁▃▁▁▁▁▃▁▂▂▂▁▂▃▁▂▅▂</td></tr><tr><td>downward_MI_4</td><td>█▅▄▇▆▃▄▄▇▃▁▃▄▃▂▆▇▃▂▃▃▁▃▃▂▂▂▁▄▂▃▂▁▂▂▄▂▃▇▃</td></tr><tr><td>downward_MI_40</td><td>█▅▃▄▂▃▂▂▅▂▂▃▃▂▂▆▆▂▂▂▄▁▂▃▁▁▁▁▃▁▂▂▂▁▂▂▁▂▄▂</td></tr><tr><td>downward_MI_41</td><td>█▅▄▅▄▄▁▃▆▂▂▂▄▃▂▄▆▂▂▃▄▁▂▃▁▁▁▁▃▁▂▂▂▂▂▃▁▂▄▂</td></tr><tr><td>downward_MI_42</td><td>█▅▄▅▃▄▂▂▆▂▁▃▄▃▂▇█▂▂▃▅▁▂▄▁▁▁▁▃▁▂▂▁▁▂▃▁▂▄▂</td></tr><tr><td>downward_MI_43</td><td>█▄▄▅▃▄▂▃▆▂▂▃▃▂▃▅▅▂▂▂▄▁▂▃▁▁▁▁▄▁▂▂▂▁▂▃▁▂▅▂</td></tr><tr><td>downward_MI_44</td><td>█▅▄▅▃▄▂▃▆▃▃▂▄▃▂▄▅▃▂▃▄▁▂▃▁▁▁▁▃▂▂▂▁▁▂▃▁▂▂▂</td></tr><tr><td>downward_MI_45</td><td>█▄▄▃▄▃▂▂▆▂▂▂▃▃▃▄▅▂▂▃▃▁▁▂▁▁▁▁▃▁▂▂▁▁▂▂▁▂▂▂</td></tr><tr><td>downward_MI_46</td><td>█▅▄▅▆▄▃▃▅▃▂▃▃▃▂▄▇▂▂▃▅▁▂▃▁▂▁▁▃▁▃▂▂▁▂▃▁▃▆▂</td></tr><tr><td>downward_MI_47</td><td>█▅▄▅▃▄▃▃▄▃▁▂▃▂▃▄▇▂▂▃▄▁▂▃▁▂▂▁▃▂▂▃▂▂▂▃▂▂▄▂</td></tr><tr><td>downward_MI_48</td><td>▅▇▆█▅▆▄▃█▄▂▄▄▅▄▆▇▃▃▃▅▄▂▄▂▃▁▂▄▂▃▃▂▂▃▄▂▄▆▂</td></tr><tr><td>downward_MI_49</td><td>▃▄▄▄▄▃▂▂▅▂▂▃▂▃▂▂▅▂▂▂▂▂▂▃▁▁▁▁▂▂▂▂▂▁▂▂▁▂█▂</td></tr><tr><td>downward_MI_5</td><td>█▄▄▃█▄▃▅▄▄▂▂▄▅▄▆█▃▂▄▄▁▃▃▂▂▂▁▄▂▃▃▂▂▂▄▂▃▅▃</td></tr><tr><td>downward_MI_50</td><td>▃▅▄▄▃▄▃▂▅▂▂▄▂▂▃▃▅▂▂▂▂▂▂▃▁▁▁▁▂▁▂▂▂▂▁▂▁▂█▁</td></tr><tr><td>downward_MI_51</td><td>▂█▆▇▅▅▅▃█▁▃▇▃▁▅▄▇▄▄▃▂▃▂▆▁▂▁▁▃▂▄▂▄▃▁▅▂▄▇▂</td></tr><tr><td>downward_MI_52</td><td>▇█▆█▃▇▅▃█▄▂▃▄▅▄▅▇▄▃▂▅▄▂▄▂▃▁▂▅▂▃▄▃▁▃▃▃▅▅▃</td></tr><tr><td>downward_MI_53</td><td>▄▇▅▇▄▆▄▃█▂▂▄▃▄▄▅▇▃▃▂▄▃▂▄▂▂▁▁▄▂▃▃▃▁▂▃▂▄▄▂</td></tr><tr><td>downward_MI_54</td><td>▆▇▆▇▆▆▅▃█▂▃▅▃▄▅▃▇▃▃▄▃▃▂▅▁▂▁▁▃▂▃▂▄▂▂▃▂▄▆▂</td></tr><tr><td>downward_MI_55</td><td>▃▅▄▅▃▄▄▂▄▁▂▅▂▂▄▂▅▃▃▃▂▃▂▃▁▁▁▁▂▂▃▂▃▂▂▃▂▃█▂</td></tr><tr><td>downward_MI_56</td><td>█▇▆▇▅▇▄▄█▄▂▃▅▆▃▆▅▃▃▂▆▃▂▆▂▄▁▂▅▂▃▄▂▁▃▃▂▄█▃</td></tr><tr><td>downward_MI_57</td><td>▇▆▅▅▅▅▄▃▇▃▂▂▄▅▃▅█▃▃▂▄▃▂▄▂▃▁▂▄▂▂▃▂▁▃▃▂▄▆▂</td></tr><tr><td>downward_MI_58</td><td>█▆▅▅▆▅▃▃█▃▂▃▄▅▃▅▆▃▂▅▃▃▂▃▂▂▁▁▄▂▂▃▂▁▂▂▁▄▃▂</td></tr><tr><td>downward_MI_59</td><td>█▆▅▆▄▆▅▃▅▂▂▅▃▃▄▃▇▃▃▃▂▃▂▄▁▂▁▁▃▂▃▂▃▁▂▃▁▄▆▂</td></tr><tr><td>downward_MI_6</td><td>█▄▄▄▄▄▅▅▅▃▂▂▃▄▃▄▅▃▂▃▃▁▃▃▂▂▂▁▃▂▃▃▂▂▂▄▂▂▅▃</td></tr><tr><td>downward_MI_60</td><td>▇▆▄▆▃▅▄▃█▃▂▃▄▄▂▆▆▂▂▂▅▂▂▃▂▂▁▁▄▂▂▃▂▁▃▃▁▄▅▂</td></tr><tr><td>downward_MI_61</td><td>▇▆▅▇▄▆▆▄█▃▂▃▄▄▃▅▅▄▃▃▄▃▂▅▂▂▁▁▅▂▂▃▂▁▃▂▁▄▇▂</td></tr><tr><td>downward_MI_62</td><td>▆▆▅▄▃▅▄▃█▃▂▄▃▃▃▅█▃▃▃▄▃▂▃▂▂▁▁▄▂▂▃▂▁▂▃▂▅▆▂</td></tr><tr><td>downward_MI_63</td><td>▃▃▁▂▁▆▃▂▃▄▃▃▂▂▃▁▇▅▃▆▂▄▂▄▃▄▄▃▅▄▂▅▃▃▄▄▃▄█▃</td></tr><tr><td>downward_MI_7</td><td>█▄▄▃▆▄▅▅▄▃▂▂▃▃▃▅▇▃▂▄▃▁▃▃▂▂▃▁▃▂▄▃▃▂▂▄▂▂▆▃</td></tr><tr><td>downward_MI_8</td><td>▄▂▂▃▄▂▂▂▃▂▂▂▂▂▂▂▃▂▁▂▂▁▂▂▁▁▂▁▂▁▂▂▂▂▁▂▂▂█▂</td></tr><tr><td>downward_MI_9</td><td>█▄▄▄▄▄▅▅▄▃▂▂▃▃▂▄▆▂▂▃▃▁▃▄▂▂▂▁▃▂▃▃▂▂▂▃▂▂█▃</td></tr><tr><td>sum_downward_MI</td><td>█▅▄▅▅▄▃▃▆▃▂▃▃▃▃▅▇▂▂▃▄▁▂▃▁▂▁▁▃▂▃▂▂▁▂▃▂▃▇▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Psi</td><td>0.0554</td></tr><tr><td>decoupled_MI</td><td>1.79398</td></tr><tr><td>downward_MI_0</td><td>0.04298</td></tr><tr><td>downward_MI_1</td><td>0.05323</td></tr><tr><td>downward_MI_10</td><td>0.08059</td></tr><tr><td>downward_MI_11</td><td>0.09086</td></tr><tr><td>downward_MI_12</td><td>0.09581</td></tr><tr><td>downward_MI_13</td><td>0.08763</td></tr><tr><td>downward_MI_14</td><td>0.09374</td></tr><tr><td>downward_MI_15</td><td>0.08241</td></tr><tr><td>downward_MI_16</td><td>0.03855</td></tr><tr><td>downward_MI_17</td><td>0.03785</td></tr><tr><td>downward_MI_18</td><td>0.03058</td></tr><tr><td>downward_MI_19</td><td>0.03607</td></tr><tr><td>downward_MI_2</td><td>0.05611</td></tr><tr><td>downward_MI_20</td><td>0.03335</td></tr><tr><td>downward_MI_21</td><td>0.02737</td></tr><tr><td>downward_MI_22</td><td>0.04771</td></tr><tr><td>downward_MI_23</td><td>0.04373</td></tr><tr><td>downward_MI_24</td><td>0.0638</td></tr><tr><td>downward_MI_25</td><td>0.02659</td></tr><tr><td>downward_MI_26</td><td>0.03734</td></tr><tr><td>downward_MI_27</td><td>0.04027</td></tr><tr><td>downward_MI_28</td><td>0.02938</td></tr><tr><td>downward_MI_29</td><td>0.05327</td></tr><tr><td>downward_MI_3</td><td>0.05717</td></tr><tr><td>downward_MI_30</td><td>0.05607</td></tr><tr><td>downward_MI_31</td><td>0.05478</td></tr><tr><td>downward_MI_32</td><td>0.03934</td></tr><tr><td>downward_MI_33</td><td>0.03435</td></tr><tr><td>downward_MI_34</td><td>0.03795</td></tr><tr><td>downward_MI_35</td><td>0.03756</td></tr><tr><td>downward_MI_36</td><td>0.02624</td></tr><tr><td>downward_MI_37</td><td>0.02798</td></tr><tr><td>downward_MI_38</td><td>0.03054</td></tr><tr><td>downward_MI_39</td><td>0.02531</td></tr><tr><td>downward_MI_4</td><td>0.06961</td></tr><tr><td>downward_MI_40</td><td>0.04498</td></tr><tr><td>downward_MI_41</td><td>0.02193</td></tr><tr><td>downward_MI_42</td><td>0.04005</td></tr><tr><td>downward_MI_43</td><td>0.05189</td></tr><tr><td>downward_MI_44</td><td>0.04327</td></tr><tr><td>downward_MI_45</td><td>0.03115</td></tr><tr><td>downward_MI_46</td><td>0.03113</td></tr><tr><td>downward_MI_47</td><td>0.03388</td></tr><tr><td>downward_MI_48</td><td>0.03384</td></tr><tr><td>downward_MI_49</td><td>0.03057</td></tr><tr><td>downward_MI_5</td><td>0.06797</td></tr><tr><td>downward_MI_50</td><td>0.04241</td></tr><tr><td>downward_MI_51</td><td>0.08324</td></tr><tr><td>downward_MI_52</td><td>0.03335</td></tr><tr><td>downward_MI_53</td><td>0.03543</td></tr><tr><td>downward_MI_54</td><td>0.04255</td></tr><tr><td>downward_MI_55</td><td>0.06031</td></tr><tr><td>downward_MI_56</td><td>0.03344</td></tr><tr><td>downward_MI_57</td><td>0.03088</td></tr><tr><td>downward_MI_58</td><td>0.02804</td></tr><tr><td>downward_MI_59</td><td>0.03518</td></tr><tr><td>downward_MI_6</td><td>0.07955</td></tr><tr><td>downward_MI_60</td><td>0.03462</td></tr><tr><td>downward_MI_61</td><td>0.03134</td></tr><tr><td>downward_MI_62</td><td>0.02706</td></tr><tr><td>downward_MI_63</td><td>0.14446</td></tr><tr><td>downward_MI_7</td><td>0.077</td></tr><tr><td>downward_MI_8</td><td>0.09102</td></tr><tr><td>downward_MI_9</td><td>0.06678</td></tr><tr><td>sum_downward_MI</td><td>3.24575</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">twilight-dew-7</strong> at: <a href='https://wandb.ai/dmcsharry/ecog-dataset-neurips/runs/jkhdjcdp' target=\"_blank\">https://wandb.ai/dmcsharry/ecog-dataset-neurips/runs/jkhdjcdp</a><br/> View project at: <a href='https://wandb.ai/dmcsharry/ecog-dataset-neurips' target=\"_blank\">https://wandb.ai/dmcsharry/ecog-dataset-neurips</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240506_044307-jkhdjcdp/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:jkhdjcdp). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/vol/bitbucket/dm2223/info-theory-experiments/wandb/run-20240506_045612-5p6030cd</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dmcsharry/ecog-dataset-neurips/runs/5p6030cd' target=\"_blank\">lively-glade-8</a></strong> to <a href='https://wandb.ai/dmcsharry/ecog-dataset-neurips' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dmcsharry/ecog-dataset-neurips' target=\"_blank\">https://wandb.ai/dmcsharry/ecog-dataset-neurips</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dmcsharry/ecog-dataset-neurips/runs/5p6030cd' target=\"_blank\">https://wandb.ai/dmcsharry/ecog-dataset-neurips/runs/5p6030cd</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 15/15 [18:10<00:00, 72.71s/it]\n"
     ]
    }
   ],
   "source": [
    "feature_network = train_feature_network(config, train_loader, feature_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finding the MI between different channels\n",
    "\n",
    "I( xt_i ; xt_j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "dataset = torch.load(\"data/ecog_data_pairs.pth\")\n",
    "trainloader = torch.utils.data.DataLoader(dataset, batch_size=2000, shuffle=False)\n",
    "\n",
    "wandb.init(project=\"Interchannel MI\")\n",
    "\n",
    "channel_MI_estimator = DownwardSmileMIEstimator(\n",
    "    feature_size=1, # replacing the feature with a channel which is dim 1\n",
    "    critic_output_size=8,\n",
    "    hidden_sizes_v_critic=[64, 512, 1028, 512],\n",
    "    hidden_sizes_xi_critic=[64, 512, 1028, 512],\n",
    "    clip=5,\n",
    "    include_bias=True\n",
    ").to(device) \n",
    "\n",
    "channel_MI_optim = torch.optim.Adam(channel_MI_estimator.parameters(), lr=1e-5, weight_decay=1e-6)\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "channels = (1,2)\n",
    "\n",
    "for _ in tqdm.tqdm(range(epochs), desc='Training a SMILE estimator for interchannel MI'):\n",
    "    for batch_num, batch in enumerate(trainloader):\n",
    "        x0 = batch[:, 0].to(device).float()\n",
    "        x1 = batch[:, 1].to(device).float()\n",
    "\n",
    "        channel_i = x0[:, channels[0]].unsqueeze(1)\n",
    "        channel_j = x0[:, channels[1]].unsqueeze(1)\n",
    "\n",
    "        MI = channel_MI_estimator(channel_j, channel_i)\n",
    "        loss = -MI \n",
    "        loss.backward()\n",
    "        channel_MI_optim.step()\n",
    "        wandb.log({\n",
    "            \"Inter-channel MI\": MI\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimating Psi given a frozen feature network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def find_true_Psi(feature_network, feature_config, run_id=None):\n",
    "\n",
    "    config = {\n",
    "        \"batch_size\": 600,\n",
    "        \"num_atoms\": 64,\n",
    "        \"feature_size\": feature_config['feature_size'],\n",
    "        \"clip\": 5,\n",
    "        \"critic_output_size\": 16,\n",
    "        \"downward_hidden_sizes_v_critic\": [1028, 1028, 512, 64],\n",
    "        \"downward_hidden_sizes_xi_critic\": [512, 512, 512, 64],\n",
    "        \"decoupled_critic_hidden_sizes_1\": [1028, 1028, 512],\n",
    "        \"decoupled_critic_hidden_sizes_2\": [1028, 1028, 512],\n",
    "        \"decoupled_critic_lr\": 1e-4,\n",
    "        \"downward_lr\": 1e-4,\n",
    "        \"bias\": True,\n",
    "        \"weight_decay\": 1e-6,\n",
    "        \"original_run_id\": run_id,\n",
    "        \"add_spec_norm_downward\": False,\n",
    "        \"add_spec_norm_decoupled\": False\n",
    "    }\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "\n",
    "    wandb.init(project=\"Finding-true-Psi-for-f\", config=config, id=run_id)\n",
    "\n",
    "    decoupled_critic = DecoupledSmileMIEstimator(\n",
    "        feature_size=config['feature_size'],\n",
    "        critic_output_size=config['critic_output_size'],\n",
    "        hidden_sizes_1=config['decoupled_critic_hidden_sizes_1'],\n",
    "        hidden_sizes_2=config['decoupled_critic_hidden_sizes_2'],\n",
    "        clip=config['clip'],\n",
    "        include_bias=config['bias'],\n",
    "        add_spec_norm=config['add_spec_norm_decoupled']\n",
    "        ).to(device)\n",
    "\n",
    "    downward_critics = [\n",
    "        DownwardSmileMIEstimator(\n",
    "            feature_size=config['feature_size'],\n",
    "            critic_output_size=config['critic_output_size'],\n",
    "            hidden_sizes_v_critic=config['downward_hidden_sizes_v_critic'],\n",
    "            hidden_sizes_xi_critic=config['downward_hidden_sizes_xi_critic'],\n",
    "            clip=config['clip'],\n",
    "            include_bias=config['bias'],\n",
    "            add_spec_norm=config['add_spec_norm_downward']\n",
    "            ).to(device) \n",
    "        for _ in range(config['num_atoms'])\n",
    "    ]\n",
    "\n",
    "    downward_optims = [\n",
    "        torch.optim.Adam(\n",
    "            dc.parameters(),\n",
    "            lr=config[\"downward_lr\"],\n",
    "            weight_decay=config[\"weight_decay\"]\n",
    "        ) \n",
    "        for dc in downward_critics\n",
    "    ]\n",
    "\n",
    "    decoupled_optimizer = torch.optim.Adam(\n",
    "        decoupled_critic.parameters(),\n",
    "        lr=config[\"decoupled_critic_lr\"],\n",
    "        weight_decay=config[\"weight_decay\"]\n",
    "    )\n",
    "\n",
    "    # TODO: figure out why only f network is being watched, I would like to keep a closer eye on the grad n params.\n",
    "    # TODO: Look at how GANs are trained with pytorch and make sure I'm not doing anything unreasonable.\n",
    "    # Eg, https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py \n",
    "    # ^ this does not require retain_graph=True, so maybe this can be optomized somehow\n",
    "    wandb.watch(decoupled_critic, log=\"all\")\n",
    "    for dc in downward_critics:\n",
    "        wandb.watch(dc, log='all')\n",
    "\n",
    "    epochs = 5\n",
    "\n",
    "    for _ in tqdm.tqdm(range(epochs), desc='Training'):\n",
    "        for _, batch in enumerate(trainloader):\n",
    "            x0 = batch[:, 0].to(device).float()\n",
    "            x1 = batch[:, 1].to(device).float()\n",
    "\n",
    "            # update decoupled critic\n",
    "\n",
    "            v0 = feature_network(x0)\n",
    "            v1 = feature_network(x1) \n",
    "\n",
    "            decoupled_optimizer.zero_grad()\n",
    "            decoupled_MI = decoupled_critic(v0, v1)\n",
    "            decoupled_loss = -decoupled_MI\n",
    "            decoupled_loss.backward(retain_graph=True)\n",
    "            decoupled_optimizer.step()\n",
    "\n",
    "\n",
    "            # update each downward critic \n",
    "\n",
    "            MIs = []\n",
    "\n",
    "            for i in range(config['num_atoms']):\n",
    "                downward_optims[i].zero_grad()\n",
    "                channel_i = x0[:, i].unsqueeze(1)\n",
    "                downward_MI_i = downward_critics[i](v1, channel_i)\n",
    "                # add spectral norm to the loss\n",
    "                downward_loss = - downward_MI_i\n",
    "                downward_loss.backward(retain_graph=True)\n",
    "                downward_optims[i].step()\n",
    "                wandb.log({\n",
    "                    f\"downward_MI_{i}\": downward_MI_i   \n",
    "                })\n",
    "                MIs.append(downward_MI_i)\n",
    "\n",
    "            # update feature network   \n",
    "\n",
    "            min_MI = min(MIs)\n",
    "            clipped_min_MIs = max(0, min_MI)\n",
    "\n",
    "            sum_downward_MI = 0\n",
    "\n",
    "            for i in range(config['num_atoms']):\n",
    "                channel_i = x0[:, i].unsqueeze(1)\n",
    "                sum_downward_MI += downward_critics[i](v1, channel_i)\n",
    "\n",
    "            decoupled_MI1 = decoupled_critic(v0, v1)\n",
    "\n",
    "            Psi = decoupled_MI1 - sum_downward_MI + (config['num_atoms'] - 1) * clipped_min_MIs\n",
    "\n",
    "            wandb.log({\n",
    "                \"decoupled_MI\": decoupled_MI1,\n",
    "                \"sum_downward_MI\": sum_downward_MI,\n",
    "                \"Psi\": Psi,\n",
    "            })\n",
    "        \n",
    "    return Psi\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:2s6ogfcf) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">dainty-leaf-19</strong> at: <a href='https://wandb.ai/dmcsharry/Finding-true-Psi-for-f/runs/2s6ogfcf' target=\"_blank\">https://wandb.ai/dmcsharry/Finding-true-Psi-for-f/runs/2s6ogfcf</a><br/> View project at: <a href='https://wandb.ai/dmcsharry/Finding-true-Psi-for-f' target=\"_blank\">https://wandb.ai/dmcsharry/Finding-true-Psi-for-f</a><br/>Synced 4 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240506_074642-2s6ogfcf/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:2s6ogfcf). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.16.6"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/vol/bitbucket/dm2223/info-theory-experiments/wandb/run-20240506_074806-1baw9nyj</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dmcsharry/Finding-true-Psi-for-f/runs/1baw9nyj' target=\"_blank\">rose-snow-20</a></strong> to <a href='https://wandb.ai/dmcsharry/Finding-true-Psi-for-f' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dmcsharry/Finding-true-Psi-for-f' target=\"_blank\">https://wandb.ai/dmcsharry/Finding-true-Psi-for-f</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dmcsharry/Finding-true-Psi-for-f/runs/1baw9nyj' target=\"_blank\">https://wandb.ai/dmcsharry/Finding-true-Psi-for-f/runs/1baw9nyj</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|██        | 1/5 [02:56<11:45, 176.45s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 15\u001b[0m\n\u001b[1;32m     10\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/vol/bitbucket/dm2223/info-theory-experiments/models/ecog_feature_network_expert-snow-10.pth\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     13\u001b[0m feature_network\u001b[38;5;241m.\u001b[39mload_state_dict(torch\u001b[38;5;241m.\u001b[39mload(model_path))\n\u001b[0;32m---> 15\u001b[0m Psi \u001b[38;5;241m=\u001b[39m \u001b[43mfind_true_Psi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_network\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[39], line 102\u001b[0m, in \u001b[0;36mfind_true_Psi\u001b[0;34m(feature_network, feature_config, run_id)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;66;03m# add spectral norm to the loss\u001b[39;00m\n\u001b[1;32m    101\u001b[0m downward_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m downward_MI_i\n\u001b[0;32m--> 102\u001b[0m \u001b[43mdownward_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m downward_optims[i]\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    104\u001b[0m wandb\u001b[38;5;241m.\u001b[39mlog({\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdownward_MI_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m: downward_MI_i   \n\u001b[1;32m    106\u001b[0m })\n",
      "File \u001b[0;32m/vol/bitbucket/dm2223/info-theory-experiments/.venv/lib/python3.10/site-packages/torch/_tensor.py:522\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    514\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    515\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    520\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    521\u001b[0m     )\n\u001b[0;32m--> 522\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    524\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/dm2223/info-theory-experiments/.venv/lib/python3.10/site-packages/torch/autograd/__init__.py:266\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    261\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    263\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    264\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 266\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    267\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    268\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    269\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    270\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    271\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    272\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    274\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# load feature network\n",
    "feature_network = SkipConnectionSupervenientFeatureNetwork(\n",
    "    num_atoms=config['num_atoms'],\n",
    "    feature_size=config['feature_size'],\n",
    "    hidden_sizes=config['feature_hidden_sizes'],\n",
    "    include_bias=config['bias']\n",
    "    ).to(device)\n",
    "\n",
    "\n",
    "model_path = \"/vol/bitbucket/dm2223/info-theory-experiments/models/ecog_feature_network_expert-snow-10.pth\"\n",
    "\n",
    "\n",
    "feature_network.load_state_dict(torch.load(model_path))\n",
    "\n",
    "Psi = find_true_Psi(feature_network, feature_config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import optuna\n",
    "def objective(trial):\n",
    "\n",
    "    config = {\n",
    "        \"batch_size\": trial.suggest_categorical(\"batch_size\", [500, 1000, 2000]),\n",
    "        \"num_atoms\": 64,\n",
    "        \"feature_size\": trial.suggest_categorical(\"feature_size\", [2, 4, 8, 16]),\n",
    "        \"clip\": trial.suggest_int(\"clip\", 1, 10),\n",
    "        \"critic_output_size\": trial.suggest_int(\"critic_output_size\", 8, 64, log=True),\n",
    "        \"downward_hidden_sizes_v_critic\": [trial.suggest_int(\"downward_hidden_size_v\", 64, 512, log=True) for _ in range(3)],\n",
    "        \"downward_hidden_sizes_xi_critic\": [trial.suggest_int(\"downward_hidden_size_xi\", 32, 256, log=True) for _ in range(3)],\n",
    "        \"feature_hidden_sizes\": [trial.suggest_int(\"feature_hidden_size\", 256, 1024, log=True) for _ in range(4)],\n",
    "        \"decoupled_critis_hidden_sizes\": [trial.suggest_int(\"decoupled_critic_hidden_size\", 64, 512, log=True) for _ in range(3)],\n",
    "        \"feature_lr\": trial.suggest_float(\"feature_lr\", 1e-6, 1e-3, log=True),\n",
    "        \"decoupled_critic_lr\": trial.suggest_float(\"decoupled_critic_lr\", 1e-5, 1e-3, log=True),\n",
    "        \"downward_lr\": trial.suggest_float(\"downward_lr\", 1e-5, 1e-3, log=True),\n",
    "        \"bias\": True,\n",
    "        \"update_f_every_N_steps\": trial.suggest_int(\"update_f_every_N_steps\", 1, 20, log=True),\n",
    "        \"weight_decay\": trial.suggest_float(\"weight_decay\", 1e-9, 1e-4, log=True),\n",
    "    }\n",
    "\n",
    "    feature_network = train_feature_network(config)\n",
    "    Psi = find_true_Psi(feature_network, wandb.run.id, feature_config=config)\n",
    "\n",
    "    return Psi\n",
    "\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=20)  # Adjust the number of trials as needed\n",
    "\n",
    "# add the Psi value the params of the top 5 runs, then save\n",
    "\n",
    "import json\n",
    "\n",
    "# save best params\n",
    "with open(\"optuna_results/best_params.json\", \"w\") as f:\n",
    "    json.dump(study.best_params, f)\n",
    "\n",
    "# save second best params\n",
    "with open(\"optuna_results/second_best_params.json\", \"w\") as f:\n",
    "    json.dump(study.best_trials[1].params, f)\n",
    "\n",
    "# save third best params\n",
    "with open(\"optuna_results/third_best_params.json\", \"w\") as f:\n",
    "    json.dump(study.best_trials[2].params, f)\n",
    "\n",
    "# save fourth best params\n",
    "with open(\"optuna_results/fourth_best_params.json\", \"w\") as f:\n",
    "    json.dump(study.best_trials[3].params, f)\n",
    "\n",
    "# save fifth best params\n",
    "with open(\"optuna_results/fifth_best_params.json\", \"w\") as f:\n",
    "    json.dump(study.best_trials[4].params, f)\n",
    "\n",
    "\n",
    "top_5 = study.best_trials[:5]\n",
    "\n",
    "for i, trial in enumerate(top_5):\n",
    "    trial.params['Psi'] = trial.value\n",
    "    with open(f\"optuna_results/trial_{i}.json\", \"w\") as f:\n",
    "        json.dump(trial.params, f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hacky hyperparam search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
