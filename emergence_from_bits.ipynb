{"cells":[{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import torch\n","from bit_dataset import BitStringDataset\n","import lovely_tensors as lt\n","import wandb\n","import tqdm\n","from einops import rearrange, reduce, repeat\n","from models import (SupervenientFeatureNetwork,\n","                    CLUB,\n","                    DecoupledSmileMIEstimator,\n","                    DownwardSmileMIEstimator,\n","                    GeneralSmileMIEstimator\n","                    )\n","lt.monkey_patch()\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","torch.cuda.empty_cache()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def train_feature_network(config, trainloader):\n","\n","    wandb.init(project=\"bits-dataset-neurips\", config=config)\n","    # init weights to zero of the feature network\n","\n","    feature_network = SupervenientFeatureNetwork(\n","        num_atoms=config['num_atoms'],\n","        feature_size=config['feature_size'],\n","        hidden_sizes=config['feature_hidden_sizes'],\n","        include_bias=config['bias']\n","        ).to(device)\n","    decoupled_MI_estimator = DecoupledSmileMIEstimator(\n","        feature_size=config['feature_size'],\n","        critic_output_size=config['critic_output_size'],\n","        hidden_sizes=config['decoupled_critic_hidden_sizes'],\n","        clip=config['clip'],\n","        include_bias=config['bias']\n","        ).to(device)\n","    downward_MI_estimators = [\n","        DownwardSmileMIEstimator(\n","            feature_size=config['feature_size'],\n","            critic_output_size=config['critic_output_size'],\n","            hidden_sizes_v_critic=config['downward_hidden_sizes_v_critic'],\n","            hidden_sizes_xi_critic=config['downward_hidden_sizes_xi_critic'],\n","            clip=config['clip'],\n","            include_bias=config['bias']\n","            ).to(device) \n","        for _ in range(config['num_atoms'])\n","    ]\n","    \n","\n","    feature_optimizer = torch.optim.Adam(\n","        feature_network.parameters(),\n","        lr=config[\"feature_lr\"],\n","        weight_decay=config[\"weight_decay\"]\n","    )\n","    decoupled_optimizer = torch.optim.Adam(\n","        decoupled_MI_estimator.parameters(),\n","        lr=config[\"decoupled_critic_lr\"],\n","        weight_decay=config[\"weight_decay\"]\n","    )\n","    downward_optims = [\n","        torch.optim.Adam(\n","            dc.parameters(),\n","            lr=config[\"downward_lr\"],\n","            weight_decay=config[\"weight_decay\"]\n","        ) \n","        for dc in downward_MI_estimators\n","    ]\n","\n","\n","    # TODO: figure out why only f network is being watched, I would like to keep a closer eye on the grad n params.\n","    # TODO: Look at how GANs are trained with pytorch and make sure I'm not doing anything unreasonable.\n","    # Eg, https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py \n","    # ^ this does not require retain_graph=True, so maybe this can be optomized somehow\n","    wandb.watch(feature_network, log='all')\n","    wandb.watch(decoupled_MI_estimator, log=\"all\")\n","    for dc in downward_MI_estimators:\n","        wandb.watch(dc, log='all')\n","\n","    ##\n","    ## TRAIN FEATURE NETWORK\n","    ##\n","\n","    epochs = config['epochs']\n","\n","    for _ in tqdm.tqdm(range(epochs), desc='Training'):\n","        for batch_num, batch in enumerate(trainloader):\n","            x0 = batch[:, 0].to(device).float()\n","            x1 = batch[:, 1].to(device).float()\n","            v0 = feature_network(x0).detach()\n","            v1 = feature_network(x1).detach()\n","\n","            # update decoupled critic\n","            decoupled_optimizer.zero_grad()\n","            decoupled_MI = decoupled_MI_estimator(v0, v1)\n","            decoupled_loss = -decoupled_MI\n","            decoupled_loss.backward(retain_graph=True)\n","            decoupled_optimizer.step()\n","\n","            # update each downward critic \n","            for i in range(config['num_atoms']):\n","                downward_optims[i].zero_grad()\n","                channel_i = x0[:, i].unsqueeze(1).detach()\n","                downward_MI_i = downward_MI_estimators[i](v1, channel_i)\n","                downward_loss = - downward_MI_i\n","                downward_loss.backward(retain_graph=True)\n","                downward_optims[i].step()\n","                wandb.log({\n","                    f\"downward_MI_{i}\": downward_MI_i   \n","                })\n","\n","            # update feature network   \n","            feature_optimizer.zero_grad()\n","            channel_MIs = []\n","\n","            # MIs = []\n","            v0 = feature_network(x0)\n","            v1 = feature_network(x1)\n","\n","            for i in range(config['num_atoms']):\n","                channel_i = x0[:, i].unsqueeze(1)\n","                channel_i_MI = downward_MI_estimators[i](v1, channel_i)\n","                channel_MIs.append(channel_i_MI)\n","                # MIs.append(channel_i_MI)\n","\n","            sum_downward_MI = sum(channel_MIs)\n","\n","            decoupled_MI1 = decoupled_MI_estimator(v0, v1)\n","\n","            #clipped_min_MIs = max(0, min(MIs))\n","\n","            Psi = decoupled_MI1 - sum_downward_MI #+ (config['num_atoms'] - 1) * clipped_min_MIs\n","            feature_loss = -Psi\n","            if config['start_updating_f_after'] < batch_num:\n","                if batch_num % config['update_f_every_N_steps'] == 0:\n","                    feature_loss.backward(retain_graph=True)\n","                    feature_optimizer.step()\n","\n","            wandb.log({\n","                \"decoupled_MI\": decoupled_MI1,\n","                \"sum_downward_MI\": sum_downward_MI,\n","                \"Psi\": Psi,\n","            })\n","\n","\n","        \n","    torch.save(feature_network.state_dict(), f\"models/feature_network_{wandb.run.name}.pth\")\n","    \n","    return feature_network\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["config = {\n","    \"num_data_points\": int(1e6),\n","    \"extra_bit_correlation\": 0.99,\n","    \"parity_bit_correlation\": 0.99,\n","    \"epochs\": 1,\n","    \"batch_size\": 1000,\n","    \"num_atoms\": 6,\n","    \"feature_size\": 1,\n","    \"clip\": 5,\n","    \"critic_output_size\": 32,\n","    \"downward_hidden_sizes_v_critic\": [512, 512, 512, 256],\n","    \"downward_hidden_sizes_xi_critic\": [512, 512, 512, 256],\n","    \"feature_hidden_sizes\": [256, 256, 256],\n","    \"decoupled_critic_hidden_sizes\": [512, 512, 512],\n","    \"feature_lr\": 1e-5,\n","    \"decoupled_critic_lr\": 1e-4,\n","    \"downward_lr\": 1e-3,    \n","    \"bias\": True,\n","    \"update_f_every_N_steps\": 5,\n","    \"weight_decay\": 1e-5,\n","    \"start_updating_f_after\": 10,\n","}\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","torch.manual_seed(42)\n","\n","dataset = BitStringDataset(\n","    gamma_parity=config['parity_bit_correlation'],\n","    gamma_extra=config['extra_bit_correlation'],\n","    length=config['num_data_points'],\n",")\n","\n","trainloader = torch.utils.data.DataLoader(\n","    dataset,\n","    batch_size=config['batch_size'],\n","    shuffle=True,\n",")\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["feature_network = train_feature_network(config, trainloader)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def find_true_Psi(feature_network, run_id, feature_network_config, trainloader):\n","\n","    config = {\n","        \"epochs\": 1,\n","        \"batch_size\": 600,\n","        \"num_atoms\": 6,\n","        \"feature_size\": feature_network_config['feature_size'],\n","        \"clip\": 5,\n","        \"critic_output_size\": 16,\n","        \"downward_hidden_sizes_v_critic\": [512, 512, 128],\n","        \"downward_hidden_sizes_xi_critic\": [512, 512, 128],\n","        \"decoupled_critis_hidden_sizes\": [512, 512, 128],\n","        \"decoupled_critic_lr\": 1e-4,\n","        \"downward_lr\": 1e-4,\n","        \"bias\": True,\n","        \"weight_decay\": 1e-6,\n","        \"original_run_id\": run_id\n","    }\n","\n","    wandb.init(project=\"BITS_Finding-true-Psi-for-f\", config=config, id=run_id)\n","\n","    decoupled_critic = DecoupledSmileMIEstimator(\n","        feature_size=config['feature_size'],\n","        critic_output_size=config['critic_output_size'],\n","        hidden_sizes=config['decoupled_critis_hidden_sizes'],\n","        clip=config['clip'],\n","        include_bias=config['bias']\n","        ).to(device)\n","\n","    downward_critics = [\n","        DownwardSmileMIEstimator(\n","            feature_size=config['feature_size'],\n","            critic_output_size=config['critic_output_size'],\n","            hidden_sizes_v_critic=config['downward_hidden_sizes_v_critic'],\n","            hidden_sizes_xi_critic=config['downward_hidden_sizes_xi_critic'],\n","            clip=config['clip'],\n","            include_bias=config['bias']\n","            ).to(device) \n","        for _ in range(config['num_atoms'])\n","    ]\n","\n","    downward_optims = [\n","        torch.optim.Adam(\n","            dc.parameters(),\n","            lr=config[\"downward_lr\"],\n","            weight_decay=config[\"weight_decay\"]\n","        ) \n","        for dc in downward_critics\n","    ]\n","\n","    decoupled_optimizer = torch.optim.Adam(\n","        decoupled_critic.parameters(),\n","        lr=config[\"decoupled_critic_lr\"],\n","        weight_decay=config[\"weight_decay\"]\n","    )\n","\n","    # TODO: figure out why only f network is being watched, I would like to keep a closer eye on the grad n params.\n","    # TODO: Look at how GANs are trained with pytorch and make sure I'm not doing anything unreasonable.\n","    # Eg, https://github.com/eriklindernoren/PyTorch-GAN/blob/master/implementations/gan/gan.py \n","    # ^ this does not require retain_graph=True, so maybe this can be optomized somehow\n","    wandb.watch(decoupled_critic, log=\"all\")\n","    for dc in downward_critics:\n","        wandb.watch(dc, log='all')\n","\n","    epochs = config['epochs']\n","\n","    for _ in tqdm.tqdm(range(epochs), desc='Training'):\n","        for _, batch in enumerate(trainloader):\n","            x0 = batch[:, 0].to(device).float()\n","            x1 = batch[:, 1].to(device).float()\n","\n","            # update decoupled critic\n","\n","            v0 = feature_network(x0)\n","            v1 = feature_network(x1) \n","\n","            decoupled_optimizer.zero_grad()\n","            decoupled_MI = decoupled_critic(v0, v1)\n","            decoupled_loss = -decoupled_MI\n","            decoupled_loss.backward(retain_graph=True)\n","            decoupled_optimizer.step()\n","\n","\n","            # update each downward critic \n","\n","            MIs = []\n","\n","            for i in range(config['num_atoms']):\n","                downward_optims[i].zero_grad()\n","                channel_i = x0[:, i].unsqueeze(1)\n","                downward_MI_i = downward_critics[i](v1, channel_i)\n","                # add spectral norm to the loss\n","                downward_loss = - downward_MI_i\n","                downward_loss.backward(retain_graph=True)\n","                downward_optims[i].step()\n","                wandb.log({\n","                    f\"downward_MI_{i}\": downward_MI_i   \n","                })\n","                MIs.append(downward_MI_i)\n","\n","            # update feature network   \n","\n","            min_MI = min(MIs)\n","            clipped_min_MIs = max(0, min_MI)\n","\n","            sum_downward_MI = 0\n","\n","            for i in range(config['num_atoms']):\n","                channel_i = x0[:, i].unsqueeze(1)\n","                sum_downward_MI += downward_critics[i](v1, channel_i)\n","\n","            decoupled_MI1 = decoupled_critic(v0, v1)\n","\n","            Psi = decoupled_MI1 - sum_downward_MI + (config['num_atoms'] - 1) * clipped_min_MIs\n","\n","            wandb.log({\n","                \"decoupled_MI\": decoupled_MI1,\n","                \"sum_downward_MI\": sum_downward_MI,\n","                \"Psi\": Psi,\n","            })\n","        \n","    return Psi\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["Psi = find_true_Psi(feature_network, wandb.run.id, config, trainloader)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["\n","for batch in trainloader:\n","    x0 = batch[:, 0].to(device).float()\n","    x1 = batch[:, 1].to(device).float()\n","\n","    v0 = feature_network(x0).squeeze()\n","    xor_bits = reduce(x0[: , :5], 'b n -> b', 'sum') % 2\n","    extra_bits = x0[:, -1]\n","    \n","    break\n","\n","\n","from tabulate import tabulate\n","\n","# Assuming v0, xor_bits, and extra_bits are already defined as PyTorch tensors\n","\n","# Convert the tensors to Python lists\n","v0_list = v0.tolist()\n","xor_bits_list = xor_bits.tolist()\n","extra_bits_list = extra_bits.tolist()\n","# Create a list of lists containing the values at each index\n","table_data = [[i, round(v0_list[i],3), xor_bits_list[i], extra_bits_list[i]] for i in range(len(v0_list))]\n","# Define the table headers\n","headers = [\"Index\", \"v0\", \"xor_bits\", \"extra_bits\"]\n","# Print the table using tabulate\n","print(tabulate(table_data, headers, tablefmt=\"grid\"))\n","\n","\n","# plot v0_list histogram\n","import matplotlib.pyplot as plt\n","plt.hist(v0_list, bins=100)\n","plt.xlabel('v0')\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"venv","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":2}
