{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7fcb62bc3ef0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from custom_datasets import FMRIDatasetConcat\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_size, feature_size, hidden_sizes):\n",
    "        super(Autoencoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_sizes[0]),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(hidden_sizes[0], hidden_sizes[1]),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(hidden_sizes[1], hidden_sizes[2]),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(hidden_sizes[2], hidden_sizes[3]),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(hidden_sizes[3], feature_size)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(feature_size, hidden_sizes[3]),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(hidden_sizes[3], hidden_sizes[2]),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(hidden_sizes[2], hidden_sizes[1]),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(hidden_sizes[1], hidden_sizes[0]),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(hidden_sizes[0], input_size)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "seed = 42 \n",
    "torch.manual_seed(seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/75], Loss: 0.0935\n",
      "Epoch [2/75], Loss: 0.0896\n",
      "Epoch [3/75], Loss: 0.0763\n",
      "Epoch [4/75], Loss: 0.0730\n",
      "Epoch [5/75], Loss: 0.0684\n",
      "Epoch [6/75], Loss: 0.0656\n",
      "Epoch [7/75], Loss: 0.0670\n",
      "Epoch [8/75], Loss: 0.0651\n",
      "Epoch [9/75], Loss: 0.0634\n",
      "Epoch [10/75], Loss: 0.0581\n",
      "Epoch [11/75], Loss: 0.0557\n",
      "Epoch [12/75], Loss: 0.0562\n",
      "Epoch [13/75], Loss: 0.0563\n",
      "Epoch [14/75], Loss: 0.0523\n",
      "Epoch [15/75], Loss: 0.0537\n",
      "Epoch [16/75], Loss: 0.0537\n",
      "Epoch [17/75], Loss: 0.0533\n",
      "Epoch [18/75], Loss: 0.0536\n",
      "Epoch [19/75], Loss: 0.0524\n",
      "Epoch [20/75], Loss: 0.0540\n",
      "Epoch [21/75], Loss: 0.0552\n",
      "Epoch [22/75], Loss: 0.0584\n",
      "Epoch [23/75], Loss: 0.0553\n",
      "Epoch [24/75], Loss: 0.0564\n",
      "Epoch [25/75], Loss: 0.0539\n",
      "Epoch [26/75], Loss: 0.0538\n",
      "Epoch [27/75], Loss: 0.0520\n",
      "Epoch [28/75], Loss: 0.0539\n",
      "Epoch [29/75], Loss: 0.0523\n",
      "Epoch [30/75], Loss: 0.0534\n",
      "Epoch [31/75], Loss: 0.0541\n",
      "Epoch [32/75], Loss: 0.0527\n",
      "Epoch [33/75], Loss: 0.0528\n",
      "Epoch [34/75], Loss: 0.0517\n",
      "Epoch [35/75], Loss: 0.0515\n",
      "Epoch [36/75], Loss: 0.0532\n",
      "Epoch [37/75], Loss: 0.0520\n",
      "Epoch [38/75], Loss: 0.0531\n",
      "Epoch [39/75], Loss: 0.0519\n",
      "Epoch [40/75], Loss: 0.0514\n",
      "Epoch [41/75], Loss: 0.0527\n",
      "Epoch [42/75], Loss: 0.0513\n",
      "Epoch [43/75], Loss: 0.0485\n",
      "Epoch [44/75], Loss: 0.0509\n",
      "Epoch [45/75], Loss: 0.0532\n",
      "Epoch [46/75], Loss: 0.0518\n",
      "Epoch [47/75], Loss: 0.0515\n",
      "Epoch [48/75], Loss: 0.0500\n",
      "Epoch [49/75], Loss: 0.0518\n",
      "Epoch [50/75], Loss: 0.0499\n",
      "Epoch [51/75], Loss: 0.0519\n",
      "Epoch [52/75], Loss: 0.0515\n",
      "Epoch [53/75], Loss: 0.0532\n",
      "Epoch [54/75], Loss: 0.0519\n",
      "Epoch [55/75], Loss: 0.0498\n",
      "Epoch [56/75], Loss: 0.0496\n",
      "Epoch [57/75], Loss: 0.0492\n",
      "Epoch [58/75], Loss: 0.0507\n",
      "Epoch [59/75], Loss: 0.0509\n",
      "Epoch [60/75], Loss: 0.0480\n",
      "Epoch [61/75], Loss: 0.0525\n",
      "Epoch [62/75], Loss: 0.0498\n",
      "Epoch [63/75], Loss: 0.0510\n",
      "Epoch [64/75], Loss: 0.0488\n",
      "Epoch [65/75], Loss: 0.0504\n",
      "Epoch [66/75], Loss: 0.0516\n",
      "Epoch [67/75], Loss: 0.0511\n",
      "Epoch [68/75], Loss: 0.0471\n",
      "Epoch [69/75], Loss: 0.0521\n",
      "Epoch [70/75], Loss: 0.0503\n",
      "Epoch [71/75], Loss: 0.0501\n",
      "Epoch [72/75], Loss: 0.0498\n",
      "Epoch [73/75], Loss: 0.0500\n",
      "Epoch [74/75], Loss: 0.0484\n",
      "Epoch [75/75], Loss: 0.0495\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dataset = FMRIDatasetConcat()\n",
    "device = 'cuda'\n",
    "\n",
    "for i in range(1):\n",
    "    config = {\n",
    "        \"torch_seed\": seed,\n",
    "        \"dataset_type\": \"FMRI\",\n",
    "        \"nuem_atoms\": 100,\n",
    "        \"batch_size\": 1000,\n",
    "        \"train_mode\": True,\n",
    "        \"train_model_B\": False,\n",
    "        \"adjust_Psi\": False,\n",
    "        \"clip\": 5,\n",
    "        \"feature_size\": 3,\n",
    "        \"epochs\": 75,\n",
    "        \"hidden_sizes\": [256, 256, 256, 256, 256],\n",
    "        \"lr\": 1e-4,\n",
    "        \"weight_decay\": 1e-6,\n",
    "    }\n",
    "\n",
    "    torch.manual_seed(config['torch_seed'])\n",
    "    trainloader = DataLoader(dataset, batch_size=config['batch_size'], shuffle=True)\n",
    "\n",
    "    input_size = 100\n",
    "    model = Autoencoder(input_size, config['feature_size'], config['hidden_sizes']).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=config['lr'], weight_decay=config['weight_decay'])\n",
    "\n",
    "    for epoch in range(config['epochs']):\n",
    "        for data in trainloader:\n",
    "            data = data.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, data)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        print(f'Epoch [{epoch+1}/{config[\"epochs\"]}], Loss: {loss.item():.4f}')\n",
    "\n",
    "    torch.save(model.state_dict(), 'models/FMRI-data-autoencoder_model.pth')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# see if autoencoder rep is emergent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:w8nbhgbz) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Psi</td><td>▆▅█▅▅▅▄▄▃▃▃▂▃▂▃▃▂▃▂▃▃▂▃▂▂▂▂▂▁▂▂▂▂▂▂▂▂▁▂▂</td></tr><tr><td>decoupled_MI</td><td>▁▁▂▃▄▅▅▅▆▄▅▆▆▆▆▆▇▇▇█▇█▇▇█▇█▆█▇▇▇▇▇█▇▇▇██</td></tr><tr><td>downward_MI_0</td><td>▆▆▇▁▆▆▆▆▆▆▇▇▇█▇▇▇▇█▇█████▇██████████▇███</td></tr><tr><td>downward_MI_1</td><td>▅▅▁▆▆▆▆▆▆▆▆▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█████</td></tr><tr><td>downward_MI_10</td><td>▁▁▃▂▄▃▄▃▁▅▆▆▅▆▅▆▆▆▇▆▆▇▇▇▇▇▆▆▇▇▇▇█▇█▇▇▇██</td></tr><tr><td>downward_MI_11</td><td>▃▃▂▁▄▄▅▆▅▄▅▆▆▆▆▆▇▇▇▆▆▇▇▆▇▇▇▇▇█▇▇▇▇█▇▇██▇</td></tr><tr><td>downward_MI_12</td><td>▆▆▁▆▆▆▆▆▆▆▆▆▆▇▇▇▆▇▇▇▇▇▇▇█▇▇▇▇▇███▇██████</td></tr><tr><td>downward_MI_13</td><td>▃▃▁▃▄▄▅▅▄▆▆▆▆▆▆▆▆▆▅▆▇▆▇▇▇▆▇▇▇▆█▇█████▇█▇</td></tr><tr><td>downward_MI_14</td><td>▅▅▁▆▆▆▆▆▆▇▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█████</td></tr><tr><td>downward_MI_15</td><td>▅▆▁▆▆▆▆▆▆▇▇▆▇▇▇▇▇█▇▇▇█▇▇▇████▇█████████▇</td></tr><tr><td>downward_MI_16</td><td>▅▅▁▄▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇███▇██▇███▇██▇████</td></tr><tr><td>downward_MI_17</td><td>▆▆▁▆▆▆▆▆▇▇▆▇▇▇▇▇▇▇▇▇▇█▇▇████▇█▇█████████</td></tr><tr><td>downward_MI_18</td><td>▄▄▄▁▅▅▅▆▇▆▆▆▆▆▇▇▇▇▆▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇█▇█▇</td></tr><tr><td>downward_MI_19</td><td>▂▂▄▁▅▄▆▅▄▆▅▆▅▇▇▇▆▇▇▇▇▇▇▇█▇▇███▇███▆▇█▇██</td></tr><tr><td>downward_MI_2</td><td>▄▄▁▃▅▅▅▆▆▆▆▆▅▆▇▇▇▇▆▆▇▇▇▆▇▇▇▇▇▇▇▇█▇█████▇</td></tr><tr><td>downward_MI_20</td><td>▆▆▁▅▆▆▆▇▇▇▇▇▇▇▇███████▇██████▇██████████</td></tr><tr><td>downward_MI_21</td><td>▅▅▁▅▅▅▅▆▆▆▆▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇██▇▇▇▇▇█▇▇████</td></tr><tr><td>downward_MI_22</td><td>▄▄▁▂▅▅▅▅▆▆▅▆▇▇▅▆▇▇█▇▇▇▇▇▇▇▇█▇▇▇██▇▇▇▇█▇█</td></tr><tr><td>downward_MI_23</td><td>▅▅▁▆▆▅▅▆▆▆▇▆▇▇▇▇▇▇▇▇▇█▇▇█▇▇███▇█▇████▇██</td></tr><tr><td>downward_MI_24</td><td>▅▅▁▆▅▅▅▅▅▆▆▇▆▆▇▇▇▇▇▇▆▇▇▇▇▇▇█▇█▇████▇▇███</td></tr><tr><td>downward_MI_25</td><td>▅▅▇▁▅▆▆▆▆▆▆▆▇▇▆▇▇▇▇▇▇▇█▇▇▇██▇▇▇█▇███████</td></tr><tr><td>downward_MI_26</td><td>▁▁▁▁▂▂▃▄▄▅▄▅▆▄▄▅▅▆▅▆▅▅▆▆▅▅▆▆▆▅▅▆█▇▆▆▆▆▇▇</td></tr><tr><td>downward_MI_27</td><td>▂▂▃▁▂▃▃▄▅▄▅▆▃▇▅▆▅▆▆▇█▇▆▇▆▆▇▇▅▆▇▇▇▇▇▇▆▇█▇</td></tr><tr><td>downward_MI_28</td><td>▁▁▂▂▃▄▂▅▅▅▅▃▅▆▅▄▆▅▄▅▆▇▅▆▆▅▅▇▅▆▆▆▆▆▆▆▆▇▆█</td></tr><tr><td>downward_MI_29</td><td>▂▃▁▄▃▄▅▆▄▆▅▇▆▆▆▇▆▆▇▇▆▇▇▆▇▇▇█▇▇▇▇▇▆████▇▇</td></tr><tr><td>downward_MI_3</td><td>▁▂▃▂▄▅▁▅▄▄▄▅▄▅▆▆▇▆▇▇▅▇▇▇▇▆▆▆▆▇█▇▇█▇███▆▆</td></tr><tr><td>downward_MI_30</td><td>▁▂▂▁▄▃▄▄▅▄▄▆▅▆▅▆▆▆▇▅▆▆▆▆▇▆▇▆▆▅▇▆▇█▆▅▆▆▇▇</td></tr><tr><td>downward_MI_31</td><td>▁▁▂▂▄▅▃▆▅▅▆▂▆▇▆▆▇▆▇▇▇▆▇▇▆▅█▆▆▆▆▇▇█▇▆▇▇▇▆</td></tr><tr><td>downward_MI_32</td><td>▁▁▄▃▄▄▄▅▅▆▆▅▆▆▅▄▆▆█▅▆▆▆▇▄▅▇▆▆▆▇▆▇█▇▆▅▇▇▆</td></tr><tr><td>downward_MI_33</td><td>▂▂▁▄▄▄▅▃▅▆▅▆▅▅▆▇▆▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇████▇▆█▇</td></tr><tr><td>downward_MI_34</td><td>▅▅▁▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇██▇▇█▇████▇█▇████████</td></tr><tr><td>downward_MI_35</td><td>▁▁▃▄▄▄▄▄▅▅▅▄▅▅▅▆▆▇▆▇▆▇▇▇▆▇▇█▇▇▇▆█▇▆▇██▆▇</td></tr><tr><td>downward_MI_36</td><td>▁▁▂▃▄▄▆▂▄▅▅▆▆▅▆▆▆▇▇▄▆▇▆▇▇▇▇▇▆▇███████▆█▇</td></tr><tr><td>downward_MI_37</td><td>▁▁▄▁▅▃▄▄▅▅▅▆▆▆▆▆▆▆▆▇▆▇▇▇▇▇▇█▇▇▆▇▇▇▇█▇▇██</td></tr><tr><td>downward_MI_38</td><td>▁▁▂▄▂▄▅▅▄▄▅▅▆▆▇▅▆▆▇▆▆▆▆▇▇▇▇█▇▇▇▇▇██▇▆▆█▇</td></tr><tr><td>downward_MI_39</td><td>▂▂▃▁▄▄▅▄▄▆▅▆▆▅▆▅▆▆▇▇▆▆▇▆▇▆▆█▇▆█▇█▇█▆▇▆█▆</td></tr><tr><td>downward_MI_4</td><td>▅▅▁▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇███▇▇██▇</td></tr><tr><td>downward_MI_40</td><td>▆▆▆▁▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇██▇██▇███████████</td></tr><tr><td>downward_MI_41</td><td>▁▂▃▁▁▁▁▁▂▂▂▅▅▃▄▆▅▆▆▇▇▅▄▇▇▇▇█▇▅▇██▇▇▆▅▆█▇</td></tr><tr><td>downward_MI_42</td><td>▃▃▄▄▁▄▆▆▇▅▄▇▇▆▇▆▇▇▇█▇▇▇▇▇█████▇▇████▇███</td></tr><tr><td>downward_MI_43</td><td>▂▂▄▃▁▅▄▆▆▆▆▆▇▆▆▆▆▇▇▆▇▇▇▇▇▇▇▇▆▇▇█▇▇█▇▆██▇</td></tr><tr><td>downward_MI_44</td><td>▂▃▁▃▄▅▅▇▇▇▇▇▇█▆▇▇█▇█▇███▆▇▆▇▇▆██▇▆█▇▆███</td></tr><tr><td>downward_MI_45</td><td>▅▅▅▆▁▆▆▆▆▆▆▇▇▆▇▆▆▇▇▇▆▇▇▇▇▇██████▇██▇████</td></tr><tr><td>downward_MI_46</td><td>▄▄▁▆▆▅▅▆▇▅▅▇▇▇▇▇▆▇▇█▇▇██▇█▇█▇▇▇████▇█▇██</td></tr><tr><td>downward_MI_47</td><td>▁▁▂▄▄▄▄▅▅▅▅▇▆▇▆▅▅▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇█</td></tr><tr><td>downward_MI_48</td><td>▁▁▃▂▃▄▅▄▅▄▅▅▇▆▆▆▆▇▇▇▇▆▆▆▅▆▆▇▇█▇▇▇▇▇█▇█▇▇</td></tr><tr><td>downward_MI_49</td><td>▁▁▃▁▃▃▅▃▄▅▅▆▅▆▅▆▅▆▇▆▆▆▇▇▇▆▇▇▆▇▆█▇▇▇█▇▇▇▇</td></tr><tr><td>downward_MI_5</td><td>▃▃▅▁▅▅▅▆▅▆▆▆▆▆▅▄▇▇▇▆▇▆▆▆▇▇▇▇▇▇▇▇▇███▇█▇▇</td></tr><tr><td>downward_MI_50</td><td>▁▁▂▃▅▃▅▆▆▆▄▆▅▆▆▅▆▆▆▆▆▆▆▆▆▇▇▆▆▇▆▇▆▇▇▇▆█▇▇</td></tr><tr><td>downward_MI_51</td><td>▄▅▃▁▅▅▆▆▆▅▅▆▆▇▇▆▇▆▆▇▇▇▇▆▇▇▇▇▇▇▇██▆█▇▇███</td></tr><tr><td>downward_MI_52</td><td>▅▅▁▃▆▆▆▆▆▆▇▆▆▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇▇▇█▇████▇</td></tr><tr><td>downward_MI_53</td><td>▅▆▁▆▆▆▆▆▆▇▆▇▆▆▇▇▇▇▇█▇▇▇█▇▇██▇▇██████▇███</td></tr><tr><td>downward_MI_54</td><td>▅▅▁▁▅▅▆▆▇▆▅▆▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇███▇</td></tr><tr><td>downward_MI_55</td><td>▄▄▁▆▅▅▆▄▆▆▆▆▆▆▆▆▇▆▆▇▇▇▇▇▇█▇▇█▇▇▇▇███▇█▇▇</td></tr><tr><td>downward_MI_56</td><td>▅▅▁▂▅▅▅▆▆▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████</td></tr><tr><td>downward_MI_57</td><td>▄▄▂▁▅▄▆▆▆▅▆▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇█▇▇███▇█</td></tr><tr><td>downward_MI_58</td><td>▃▃▁▄▄▄▅▅▂▅▆▆▅▆▆▆▆▇▆▆▆▆▇▆▇▇▇▇▇▇▇▇█▇█▇▇█▇▇</td></tr><tr><td>downward_MI_59</td><td>▅▅▅▁▅▅▅▅▆▆▆▆▆▇▆▆▇▇▇▇▆▇▇▇▇█▇▇▇▇█▇▇███▇███</td></tr><tr><td>downward_MI_6</td><td>▃▃▅▁▆▅▆▆▅▆▆▇▆▆▆▆▇▆▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇█▇▇▇█</td></tr><tr><td>downward_MI_60</td><td>▂▂▂▁▄▄▅▅▄▆▆▆▆▆▅▆▆▆▇▅▇▇▇▅▇█▇▇▇▇▇▇█▇▇▇██▇▇</td></tr><tr><td>downward_MI_61</td><td>▁▂▂▃▃▃▄▅▅▆▆▄▃▆▆▆▄▆▇▄▇▇▇▆▇▇▆▇▇▇▇███▇▇█▇██</td></tr><tr><td>downward_MI_62</td><td>▃▄▁▅▄▅▅▅▆▅▅▆▆▆▆▆▇▇▇▆▇▇▇▆▇▇▇▆▇▇▇██▇██▇██▇</td></tr><tr><td>downward_MI_63</td><td>▁▂▂▂▃▄▁▅▅▅▄▆▆▆▅▆▆▆▆▆▆▆▆▃▇▇▇▇▇▇▇▇▇█▇▇▇███</td></tr><tr><td>downward_MI_64</td><td>▄▄▁▃▅▅▅▅▅▆▆▆▆▇▆▆▆▇▇▇▆▇▆▇▇▇▇▇▇▇▇▇█▇▇██▇██</td></tr><tr><td>downward_MI_65</td><td>▁▁▁▃▃▄▃▅▅▅▃▄▅▅▅▆▅▆▇▆▆▆▆▆▇▇▇▆▇▇▇▇▇█▇▇▇█▇█</td></tr><tr><td>downward_MI_66</td><td>▁▁▄▁▅▄▅▅▂▅▆▇▆▆▇▅▇▆▇▇▆▇▆▇▇▇▇▇▆▇▇▇█▇▇████▇</td></tr><tr><td>downward_MI_67</td><td>▅▅▁▆▆▆▆▆▇▇▆▆▇▇▇▇█▇▇▇▇▇▇███▇█████████████</td></tr><tr><td>downward_MI_68</td><td>▅▅▁▅▅▅▅▆▆▆▆▅▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇█▇▇▇▇▇▇█████▇</td></tr><tr><td>downward_MI_69</td><td>▅▅▃▁▆▆▆▆▆▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██████▇███▇</td></tr><tr><td>downward_MI_7</td><td>▅▅▁▁▆▆▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇▇██▇▇███</td></tr><tr><td>downward_MI_70</td><td>▆▇▁▇▇▇▇▇▇▇▇▇▇▇▇▇▇███████████████████████</td></tr><tr><td>downward_MI_71</td><td>▆▆▇▁▆▆▆▆▆▆▆▇▇▆▇▇▇▇▇▇▇█▇▇▇▇▇█▇██▇▇█▇▇█▇██</td></tr><tr><td>downward_MI_72</td><td>▇▇▁▇▇▇▇▇▇▇▇▇▇▇▇▇███▇█▇█▇████████████████</td></tr><tr><td>downward_MI_73</td><td>▄▄▂▁▅▅▅▅▅▆▅▆▇▇▇▆▇▇▇▇▇▇█▇▇▇▇█▇▇█▇▇▇▇▇███▇</td></tr><tr><td>downward_MI_74</td><td>▃▄▁▆▅▅▆▅▅▆▆▆▇▇▆▆▇▇▇▇▇█▇▇██▇███▇█████████</td></tr><tr><td>downward_MI_75</td><td>▃▃▁▄▅▅▅▆▅▆▆▇▆▆▆▆▇▇▆▆▆▇▇▇▆▇▇▇▇▇▇▇▇▇█▇█▇▇█</td></tr><tr><td>downward_MI_76</td><td>▅▅▅▁▅▅▅▅▅▅▅▆▆▆▆▆▇▇▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇█</td></tr><tr><td>downward_MI_77</td><td>▂▃▁▃▃▃▃▃▄▆▆▅▆▇▆▆▆▇▆▇▆▆▇▇▇▆▇▇▆▇▇▇▇▇▇▇█▇▇█</td></tr><tr><td>downward_MI_78</td><td>▄▄▁▄▅▄▅▆▇▅▆▇▆▇▆▆▆▇▆▆▆▆▇▇█▇▇▇▆▇█▇██▇▆▇▇▇▇</td></tr><tr><td>downward_MI_79</td><td>▁▁▂▃▃▃▆▆▅▅▄▆▇▇▇▆▇▆▆▇▇▇▇▇▇▇█▇▆▇▇▇▇▇█▇▇▆▇▇</td></tr><tr><td>downward_MI_8</td><td>▅▅▁▂▆▅▆▆▆▆▆▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇████▇███▇</td></tr><tr><td>downward_MI_80</td><td>▁▁▃▄▂▃▃▄▅▅▅▄▅▅▅▅▅▅▅▆▅▆▆▆▇▆▇█▅▇▇▆▆▆▆▆▇▇▇█</td></tr><tr><td>downward_MI_81</td><td>▃▃▁▄▄▄▅▅▅▆▅▆▆▆▇▇▇▆▇▆▆▇▇▇▇▇▇▇▇▇████▇▇▇▇█▇</td></tr><tr><td>downward_MI_82</td><td>▁▂▁▁▂▂▃▅▁▃▅▅▅▅▃▅▇▆▅▆▅▇▆▇▇▆▆▇▆▇█▆▇▆▆▆▆▇▇▇</td></tr><tr><td>downward_MI_83</td><td>▁▁▃▂▄▄▃▄▅▄▅▄▅▆▆▆▆▆▆▇▆▆▆▇▆▇▇▇▇▇█▇▇█▇▇███▇</td></tr><tr><td>downward_MI_84</td><td>▁▁▂▂▃▄▄▅▄▅▅▅▄▅▅▅▆▆▅▆▅▆▆▇▇▆▇▇▆▇▇▆▇▆▇▇█▆▆█</td></tr><tr><td>downward_MI_85</td><td>▁▁▂▂▄▄▅▆▅▅▅▅▆▆▆▇▇▆▆▇▆▆▆▇▆▇█▇▆▇▆▇▆▇█▇▇▇▇▇</td></tr><tr><td>downward_MI_86</td><td>▁▁▂▃▁▃▄▅▅▅▆▆▆▇▅▆▅▇▇▅▆▆▆▇▇▇▆▇▆▅▆██▇▇▇▇▆█▇</td></tr><tr><td>downward_MI_87</td><td>▁▁▁▁▂▂▄▅▅▅▅▅▄▅▅▆▆▇▃█▆█▆▇▇▆▇▇▅▇█▇█▆▇▇█▇▇▇</td></tr><tr><td>downward_MI_88</td><td>▁▁▁▁▂▃▃▅▄▅▆▅▆▆▆▅▆▇▆▇▆▆▇▆▇▇▆█▇█▇▇██▇██▇▇▇</td></tr><tr><td>downward_MI_89</td><td>▁▂▃▅▃▅▄▄▅▄▄▅▆▆▆▆▆▆▅▆▆▇▆▇▆▆▆▇▆▇▇▇▇▇▇█▇▆██</td></tr><tr><td>downward_MI_9</td><td>▁▁▃▂▃▅▄▅▆▆▆▄▆▆▆▆▅▆▆▅▅▇▆▆▇▇▆▇▇██▇██▇▇▇▇▇▇</td></tr><tr><td>downward_MI_90</td><td>▅▅▆▁▄▆▆▆▆▇▇▆▇▇▇▇▇▇▇▇▇█▇█▇▇██████████▇███</td></tr><tr><td>downward_MI_91</td><td>▄▄▁▅▅▅▅▅▅▅▆▆▅▆▇▇▇▆▆█▇▆▇▇▇▇█▇▇▇██████████</td></tr><tr><td>downward_MI_92</td><td>▁▁▃▃▄▃▅▅▅▅▅▆▆▅▆▅▆▆▅▆▆▆▆▅▆▇▆▆▇▇▇▆▇▆▆▆▆▆█▇</td></tr><tr><td>downward_MI_93</td><td>▂▂▃▃▁▃▅▆▆▆▅▆▆▇▆▇▇▇▆▇▇▇▇▇▇▆▇██▅█▇▇▇▇▇▅█▇▇</td></tr><tr><td>downward_MI_94</td><td>▁▂▃▃▃▄▃▆▆▆▄▄▆▆▆▇▆▆▆▇▆█▅▆▇▇▆▇▇██▇▇▇▇▆▇▇▇█</td></tr><tr><td>downward_MI_95</td><td>▁▁▂▂▃▄▅▄▅▆▆▅▇▅▆▆▆▆▆▇▇▇▇█▇▇▇▇▆▇▇█▇▇█▇▅███</td></tr><tr><td>downward_MI_96</td><td>▁▁▁▃▄▄▅▄▃▆▅▆▆▅▅▆▆▆▅▆▇▇▆▇▆▇▇▇▇▇▆▇▆▆▇▇▅▇██</td></tr><tr><td>downward_MI_97</td><td>▄▄▅▆▁▆▅▅▆▆▇▆▆▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇███████▇███</td></tr><tr><td>downward_MI_98</td><td>▄▅▁▃▅▅▅▆▆▆▆▆▇▇▆▇▇▇▇▇▇▇▇▇▇█▇█▇█▇███▇█████</td></tr><tr><td>downward_MI_99</td><td>▁▂▄▃▁▅▄▄▅▅▅▆▇▇▅▆▆▇▇▇▇▇▇█▇▇██▆█▇█▇▇▇█▇█▇▇</td></tr><tr><td>sum_downward_MI</td><td>▂▃▁▃▄▄▄▄▅▅▅▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇██████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Psi</td><td>-22.31507</td></tr><tr><td>decoupled_MI</td><td>3.57286</td></tr><tr><td>downward_MI_0</td><td>0.22857</td></tr><tr><td>downward_MI_1</td><td>0.60365</td></tr><tr><td>downward_MI_10</td><td>0.35539</td></tr><tr><td>downward_MI_11</td><td>0.65505</td></tr><tr><td>downward_MI_12</td><td>0.3758</td></tr><tr><td>downward_MI_13</td><td>0.43863</td></tr><tr><td>downward_MI_14</td><td>0.37779</td></tr><tr><td>downward_MI_15</td><td>0.40048</td></tr><tr><td>downward_MI_16</td><td>0.47301</td></tr><tr><td>downward_MI_17</td><td>0.4785</td></tr><tr><td>downward_MI_18</td><td>0.34289</td></tr><tr><td>downward_MI_19</td><td>0.28528</td></tr><tr><td>downward_MI_2</td><td>0.76005</td></tr><tr><td>downward_MI_20</td><td>0.26378</td></tr><tr><td>downward_MI_21</td><td>0.42008</td></tr><tr><td>downward_MI_22</td><td>0.34939</td></tr><tr><td>downward_MI_23</td><td>0.40823</td></tr><tr><td>downward_MI_24</td><td>0.23665</td></tr><tr><td>downward_MI_25</td><td>0.33984</td></tr><tr><td>downward_MI_26</td><td>0.11831</td></tr><tr><td>downward_MI_27</td><td>0.15853</td></tr><tr><td>downward_MI_28</td><td>0.21894</td></tr><tr><td>downward_MI_29</td><td>0.25033</td></tr><tr><td>downward_MI_3</td><td>0.18742</td></tr><tr><td>downward_MI_30</td><td>0.16487</td></tr><tr><td>downward_MI_31</td><td>0.17088</td></tr><tr><td>downward_MI_32</td><td>0.23438</td></tr><tr><td>downward_MI_33</td><td>0.41145</td></tr><tr><td>downward_MI_34</td><td>0.3027</td></tr><tr><td>downward_MI_35</td><td>0.30456</td></tr><tr><td>downward_MI_36</td><td>0.16426</td></tr><tr><td>downward_MI_37</td><td>0.47284</td></tr><tr><td>downward_MI_38</td><td>0.39432</td></tr><tr><td>downward_MI_39</td><td>0.25967</td></tr><tr><td>downward_MI_4</td><td>0.31489</td></tr><tr><td>downward_MI_40</td><td>0.47873</td></tr><tr><td>downward_MI_41</td><td>0.15402</td></tr><tr><td>downward_MI_42</td><td>0.24773</td></tr><tr><td>downward_MI_43</td><td>0.3433</td></tr><tr><td>downward_MI_44</td><td>0.17058</td></tr><tr><td>downward_MI_45</td><td>0.40425</td></tr><tr><td>downward_MI_46</td><td>0.33688</td></tr><tr><td>downward_MI_47</td><td>0.30212</td></tr><tr><td>downward_MI_48</td><td>0.38846</td></tr><tr><td>downward_MI_49</td><td>0.53877</td></tr><tr><td>downward_MI_5</td><td>0.56284</td></tr><tr><td>downward_MI_50</td><td>0.23595</td></tr><tr><td>downward_MI_51</td><td>0.75491</td></tr><tr><td>downward_MI_52</td><td>0.4224</td></tr><tr><td>downward_MI_53</td><td>0.19999</td></tr><tr><td>downward_MI_54</td><td>0.53987</td></tr><tr><td>downward_MI_55</td><td>0.68808</td></tr><tr><td>downward_MI_56</td><td>0.57888</td></tr><tr><td>downward_MI_57</td><td>0.70794</td></tr><tr><td>downward_MI_58</td><td>0.44459</td></tr><tr><td>downward_MI_59</td><td>0.46456</td></tr><tr><td>downward_MI_6</td><td>0.32002</td></tr><tr><td>downward_MI_60</td><td>0.63998</td></tr><tr><td>downward_MI_61</td><td>0.41902</td></tr><tr><td>downward_MI_62</td><td>0.5857</td></tr><tr><td>downward_MI_63</td><td>0.39798</td></tr><tr><td>downward_MI_64</td><td>0.41395</td></tr><tr><td>downward_MI_65</td><td>0.39777</td></tr><tr><td>downward_MI_66</td><td>0.35312</td></tr><tr><td>downward_MI_67</td><td>0.37977</td></tr><tr><td>downward_MI_68</td><td>0.49645</td></tr><tr><td>downward_MI_69</td><td>0.4918</td></tr><tr><td>downward_MI_7</td><td>0.63733</td></tr><tr><td>downward_MI_70</td><td>0.26492</td></tr><tr><td>downward_MI_71</td><td>0.55121</td></tr><tr><td>downward_MI_72</td><td>0.39053</td></tr><tr><td>downward_MI_73</td><td>0.26521</td></tr><tr><td>downward_MI_74</td><td>0.50429</td></tr><tr><td>downward_MI_75</td><td>0.53211</td></tr><tr><td>downward_MI_76</td><td>0.22426</td></tr><tr><td>downward_MI_77</td><td>0.30275</td></tr><tr><td>downward_MI_78</td><td>0.10663</td></tr><tr><td>downward_MI_79</td><td>0.24595</td></tr><tr><td>downward_MI_8</td><td>0.69516</td></tr><tr><td>downward_MI_80</td><td>0.57957</td></tr><tr><td>downward_MI_81</td><td>0.47513</td></tr><tr><td>downward_MI_82</td><td>0.23957</td></tr><tr><td>downward_MI_83</td><td>0.52551</td></tr><tr><td>downward_MI_84</td><td>0.21831</td></tr><tr><td>downward_MI_85</td><td>0.47517</td></tr><tr><td>downward_MI_86</td><td>0.23754</td></tr><tr><td>downward_MI_87</td><td>0.23232</td></tr><tr><td>downward_MI_88</td><td>0.38952</td></tr><tr><td>downward_MI_89</td><td>0.37827</td></tr><tr><td>downward_MI_9</td><td>0.43892</td></tr><tr><td>downward_MI_90</td><td>0.31895</td></tr><tr><td>downward_MI_91</td><td>0.31315</td></tr><tr><td>downward_MI_92</td><td>0.42026</td></tr><tr><td>downward_MI_93</td><td>0.29498</td></tr><tr><td>downward_MI_94</td><td>0.32246</td></tr><tr><td>downward_MI_95</td><td>0.3688</td></tr><tr><td>downward_MI_96</td><td>0.39438</td></tr><tr><td>downward_MI_97</td><td>0.33737</td></tr><tr><td>downward_MI_98</td><td>0.36358</td></tr><tr><td>downward_MI_99</td><td>0.4006</td></tr><tr><td>sum_downward_MI</td><td>39.38725</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">cosmic-waterfall-10</strong> at: <a href='https://wandb.ai/dmcsharry/NEURIPS-testing-if-encoder-rep-is-emergent/runs/w8nbhgbz' target=\"_blank\">https://wandb.ai/dmcsharry/NEURIPS-testing-if-encoder-rep-is-emergent/runs/w8nbhgbz</a><br/> View project at: <a href='https://wandb.ai/dmcsharry/NEURIPS-testing-if-encoder-rep-is-emergent' target=\"_blank\">https://wandb.ai/dmcsharry/NEURIPS-testing-if-encoder-rep-is-emergent</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240806_200419-w8nbhgbz/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:w8nbhgbz). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/info-theory-experiments/wandb/run-20240806_200506-ehgoqnva</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dmcsharry/NEURIPS-testing-if-encoder-rep-is-emergent/runs/ehgoqnva' target=\"_blank\">leafy-galaxy-11</a></strong> to <a href='https://wandb.ai/dmcsharry/NEURIPS-testing-if-encoder-rep-is-emergent' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dmcsharry/NEURIPS-testing-if-encoder-rep-is-emergent' target=\"_blank\">https://wandb.ai/dmcsharry/NEURIPS-testing-if-encoder-rep-is-emergent</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dmcsharry/NEURIPS-testing-if-encoder-rep-is-emergent/runs/ehgoqnva' target=\"_blank\">https://wandb.ai/dmcsharry/NEURIPS-testing-if-encoder-rep-is-emergent/runs/ehgoqnva</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/10 [00:00<?, ?it/s]/tmp/ipykernel_175051/1013419446.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('models/FMRI-data-autoencoder_model.pth'))\n",
      "Training: 100%|██████████| 10/10 [02:20<00:00, 14.00s/it]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Psi</td><td>█▅▃▂▃▂▂▂▂▂▂▂▂▂▁▂▁▂▂▂▂▁▂▂▂▁▂▁▂▁▁▁▂▂▂▁▂▂▁▂</td></tr><tr><td>decoupled_MI</td><td>▁▃▄▄▄▄▄▅▅▅▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇███▇██▇█</td></tr><tr><td>downward_MI_0</td><td>▁▄▅▆▆▆▆▆▇▆▆▆▇▆▇▆▆▇▆▆▇▆▆▆▇▇▇▇█▆▇▆▇▇▆▇▆▇█▆</td></tr><tr><td>downward_MI_1</td><td>▁▅▅▅▆▇▇▇█▇▇█▇▇▇█▇▇▇██▇█▇▇▇███▇▇█▇█▇██▇▇▇</td></tr><tr><td>downward_MI_10</td><td>▂▁▃▅▄▆▆▆▇▆▇▇▇▇█▇▇█▇▇█▇▇█▇▇████▇█▇▇█▇▇▇▇█</td></tr><tr><td>downward_MI_11</td><td>▁▅▆▆▆▇▆▇▇▇▇█▇██▇▇█▇█▇███▇██▇██████████▇█</td></tr><tr><td>downward_MI_12</td><td>▁▅▅▅▆▆▇▇▇▇▇▇▇▇██▇▇██▇███▇▇█████████████▇</td></tr><tr><td>downward_MI_13</td><td>▁▄▅▅▅▇▇▆▆▇▇▇▇▇█▆▇▇▇▇█▇█▇▇▇█▇▇█▇█▆█▇█▇▇██</td></tr><tr><td>downward_MI_14</td><td>▁▃▃▄▅▆▅▅▇▇▇▇▇▇▇▇▇▇▆▆▇▇▇▇▇▇█▇▇█▇█▆██▇▆▇▇▇</td></tr><tr><td>downward_MI_15</td><td>▁▃▃▄▅▆▆▅▆▆▇▇▆▇▆▆▇▆▇▇▇▇▇▇▇▆▇▇▇▇█▇▆▆▇█▇▇█▇</td></tr><tr><td>downward_MI_16</td><td>▁▂▄▅▄▆▆▅▆▇▇▇▇▇▇▇▇█▇█▇▇▇▇▇█▇▇██▇▇▇▇▇█▇▇▇▇</td></tr><tr><td>downward_MI_17</td><td>▁▁▁▄▅▅▅▆▇▆▇▆▇▇▆▇▇▇▇█▇█▇▆▇▇▇▇█▇▇▇▇▇██▇▇▇▇</td></tr><tr><td>downward_MI_18</td><td>▁▄▆▆▆▇▇▆▆▇▇▇▇█▇▇█▇▇█▇▇▇▇▇██▇▇▇▇█▇▇▇█▇███</td></tr><tr><td>downward_MI_19</td><td>▁▂▃▄▃▅▅▄▅▅▆▅▅▆▆▆▅▆▅▆▆█▆▅▇▇▇▆▇█▆▆▆█▇▇▆▆▆▆</td></tr><tr><td>downward_MI_2</td><td>▁▆▇▇▇▇▇▇▇███████████████████████████████</td></tr><tr><td>downward_MI_20</td><td>▁▄▅▆▇▇▇▆▇▇▇▆▇█▇▇▇▇▇█▇█▇▇█████▇█▇█▇▇███▇█</td></tr><tr><td>downward_MI_21</td><td>▁▅▆▇▇▇▇▇▇▇▇▇▇▇▇███▇█▇▇█▇██▇▇███▇██▇█████</td></tr><tr><td>downward_MI_22</td><td>▁▄▅▅▇▇▇▇▇▇▇▇▇█▇▇▇█▇█▇▇█▇▇█▇▇██▇▇█▇█████▇</td></tr><tr><td>downward_MI_23</td><td>▁▄▅▆▆▆▇▆▆▇▇▇▇▇▇▇▇▇▆▇█▇██▇█▇▇█████▇█▇▇██▇</td></tr><tr><td>downward_MI_24</td><td>▂▁▄▅▅▅▆▅▆▅▅▇▇▆▆▆▆▆▆▇▇▇▆█▇▇▇▇▇▇▇▆▆▆▆▇▆▇█▆</td></tr><tr><td>downward_MI_25</td><td>▁▅▅▆▇▆▇▇▇▇▇██▇█▇▇█▇▇█▇███▇█████▇▇▇█████▇</td></tr><tr><td>downward_MI_26</td><td>▁▂▄▄▄▄▅▆▅▅▆▅▇▆▅▆▅▇▅▆▇▅▇▆▆▇▇▆█▇█▆▇▆▆█▇▇▆▇</td></tr><tr><td>downward_MI_27</td><td>▁▂▅▄▄▅▅▅▅▆▆▆▇▅▆▅▅▆▅▅▇▅▅▆▇▆▅▆▇▆▇▆▇▇▆▇▇█▅▆</td></tr><tr><td>downward_MI_28</td><td>▂▁▂▄▅▅▆▅▅▆▄▅▇▅█▆▇▆▅▆▆▇▆█▆██▇██▇▆▅▆▇██▆█▆</td></tr><tr><td>downward_MI_29</td><td>▁▄▆▅▆▆▆▆▆▇▇▆▇▇█▇▇▇▇▇█▇█▇▇▇▇▆█▇▆█▇█▇████▇</td></tr><tr><td>downward_MI_3</td><td>▁▃▄▅▆▃▅▅▇▅▅▇▅▇▆▆▅▆▆█▇▇▅▆▆▆▅▇▆▇▇▇▆▆▆▇█▇▇█</td></tr><tr><td>downward_MI_30</td><td>▁▃▅▅▅▅▆▅▅▆▆▆▆▅▆▆▆▆▆▆▆▆▇▇▆▅▇▇▇▆▇▅▇▄▇▆▆█▆▆</td></tr><tr><td>downward_MI_31</td><td>▁▄▄▅▆▆▆▅▆▆▆▆▆▆▆▆▇▆▇▇▇▆▇█▇▆▇▆▇▇█▅▇▆▇█▇▇█▇</td></tr><tr><td>downward_MI_32</td><td>▁▄▄▄▅▅▆▆▆▇▆▇▆▆▇▆▆▆▇▆▆▇▆▇▆▆▇▇▇▇▇▇▆▆▇▇▆█▆▆</td></tr><tr><td>downward_MI_33</td><td>▁▂▄▄▅▆▆▆▇▇▆▆▆▇▆▇▇▇▇▇▇▇▇▆▇▇▆▇▇▇█▇▇▇▇▇█▇▆▇</td></tr><tr><td>downward_MI_34</td><td>▁▃▄▄▄▅▅▅▅▆▅▅▅▆▆▆▅▅▆▆▅▆▆▆▆▆▅▅▅█▇▅▆▆▆▆▆▆▆▆</td></tr><tr><td>downward_MI_35</td><td>▁▄▆▆▆▇▇▇▆▇▇▇▇▇▇█▇▇▇▇▇█▇▇█▆█▇█▇█▇▇▇█▇▇███</td></tr><tr><td>downward_MI_36</td><td>▃▁▁▂▅▅▄▅▅▆▆▆▆▇▇▆▆▇▆▆▆▇▆▅▆▅▇▆▇▇███▆█▇▇▇▇█</td></tr><tr><td>downward_MI_37</td><td>▁▄▄▄▆▆▆▆▇▆▆▇▇▇▇▇▇▇▇▇▇█▇▇▇▇█▇▇▇█▇▇▇█▇▇▇█▇</td></tr><tr><td>downward_MI_38</td><td>▁▂▃▄▅▅▅▅▆▆▅▆▆▆▇▆▇▆▆▇▇▆▆▆▇▆▇▆▆▆█▆▇▇█▇▆▇▇▆</td></tr><tr><td>downward_MI_39</td><td>▁▄▆▆▆▆▆▆▆▆▆▆▇▆▇▇▇▇▇▇▇▆▇▇▇▇▇▆▇▇█▇▇▇▇█▇▇▇▇</td></tr><tr><td>downward_MI_4</td><td>▁▃▄▄▆▆▅▇█▇▆▇▆▆▆▇▇█▇██▇▇▆█▇▇▇▇▇▇█▇██▇██▇▇</td></tr><tr><td>downward_MI_40</td><td>▁▁▁▃▅▆▆▅▆▆▆▇▆▇▇▇▇▆▇▇▇▇▇▇▇▆███▇█▇█▇██▇██▇</td></tr><tr><td>downward_MI_41</td><td>▁▁▄▄▅▄▅▆▄▅▆▆▆▅▅▅▆▅▇▅▄▅▆▆▆▄▆▇█▅▇▆█▆▇▆▇▇▆▆</td></tr><tr><td>downward_MI_42</td><td>▁▄▄▄▅▅▆▅▆▇▅▆▆▆▇▆▆▇▇▆▆▇▆▇▇▅▆▇▆▇█▆█▆█▆▇█▇▇</td></tr><tr><td>downward_MI_43</td><td>▁▂▃▄▅▆▆▅▆▆▆▆▆▆▆▆█▆▆▇▇▇██▇▆▇▇█▆▆▇▇▆▇▇▆█▇▇</td></tr><tr><td>downward_MI_44</td><td>▁▁▁▅▃▅▆▅▆▆▅▅▅▇█▄▆▆▇█▇▅▇▇▇▅█▇█▆▇▆▇▆▇▇▇▇▅▇</td></tr><tr><td>downward_MI_45</td><td>▁▂▁▃▄▅▅▆▆▆▆▇▆▆▇▅▇▆▇▇▇▇▇▇▆▆▇▇▆▇▆▆█▆█▆▆▇▇▆</td></tr><tr><td>downward_MI_46</td><td>▁▅▆▆▆▇▇▆▇▇▇▇█▇█▇█▇█▇███▇▇▇█▇███▇█▇█▇█▇▇▇</td></tr><tr><td>downward_MI_47</td><td>▁▂▄▄▅▅▅▆▆▆▅▆▇█▇▅▇▆▇▇▇▇▇▇▅▇▇▇▇██▇███▇██▆▇</td></tr><tr><td>downward_MI_48</td><td>▁▄▆▅▆▆▆▆▇▇▇▇▇▇▇▇▇█▇▆█▇▇▇▇▇▇▇█▇▇▇█▇▇█▇▆█▇</td></tr><tr><td>downward_MI_49</td><td>▂▁▃▃▄▅▅▅▆▇▆▆▆▇▆▇▆▆▆▇▆▇█▇▇▇█▇█▆▇▇▇▆█▇▆▆▇▇</td></tr><tr><td>downward_MI_5</td><td>▁▃▄▃▅▆▆▇▆▆▆▆▆▆▇█▆▇▇▆▇▇▆▆▇█▇▇▆▇▇█▆▇▇▇▇▇█▇</td></tr><tr><td>downward_MI_50</td><td>▁▄▅▆▅▆▆▆▇▅▇▆▇▆▇▆▇▆▇▇▆█▅▇▇▆▇▇▇▆▇▆▆▇▆▇▇▇▇▅</td></tr><tr><td>downward_MI_51</td><td>▁▂▅▅▆▅▇▇▇▇▆▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇▇▇█▇█▇█▇█▇██▇</td></tr><tr><td>downward_MI_52</td><td>▁▅▅▆▆▆▆▆▇▆▇▇▇█▇▇▇▇▇▇█▇▇▇▇▇▇▇▇▇██▇█▇█▇██▇</td></tr><tr><td>downward_MI_53</td><td>▁▁▄▃▄▄▂▅▆▆▅▇▆▆▆▆▅▆▄▆▇▆▆▅▅▇▅▆▆▅▆▇▇▇▆▆█▆▆█</td></tr><tr><td>downward_MI_54</td><td>▁▁▃▅▅▆▆▆▆▆█▇▇▇▇▇▇▇▇▇▇█▆▆▇▇▇██▇██▇▇█▇█▇██</td></tr><tr><td>downward_MI_55</td><td>▁▅▆▇▇▇▇▇▇▇██▇███▇█████▇▇▇██████████▇████</td></tr><tr><td>downward_MI_56</td><td>▁▅▆▆▆▇▇▇▇▇██▇████▇▇███████▇█▇███▇█▇█████</td></tr><tr><td>downward_MI_57</td><td>▁▅▅▅▆▆▆▇▇▇▇▇▇██▇▇█▇██▇▇▇▇█▇█████▇██▇████</td></tr><tr><td>downward_MI_58</td><td>▁▅▅▆▆▇▇▇▇▇▇▇▇██▇██▇██▇██▇█████▇█▇█████▇▇</td></tr><tr><td>downward_MI_59</td><td>▁▂▅▅▅▆▇▆▆▇▆█▇▇▇▆▇█▇▇▇▇▇▇▆▇▇▇▇▇▇▇▆▇▇▇▇█▇▇</td></tr><tr><td>downward_MI_6</td><td>▁▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇███▇█▇█▇█▇▇███▇█▇█▇██▇</td></tr><tr><td>downward_MI_60</td><td>▁▅▆▇▇▇▇▇▇▇▇█████████████████████████████</td></tr><tr><td>downward_MI_61</td><td>▁▂▂▃▃▄▄▆▆▆▇▇▆▆▇█▆▇▇█▇▇▆▇▆▆█▇█▇▇▆▇█▇▇███▇</td></tr><tr><td>downward_MI_62</td><td>▁▃▄▅▅▆▆▇▇▇▇█▇█▇▇▇▇▇▇███▇▇▇█▇▇██▇███▇█▇▇▇</td></tr><tr><td>downward_MI_63</td><td>▁▃▄▅▅▇▆▆▇▇▇▇▇▇▇▇▇▆▄▇█▆▇▇▇▇█▇▇▇█▇▇█▇▇▇▇▇▇</td></tr><tr><td>downward_MI_64</td><td>▁▅▅▆▆▆▇▇▇██▇▇███▇▇▇▇█▇▇██████▇█▇▇█▇█▇███</td></tr><tr><td>downward_MI_65</td><td>▁▃▄▄▅▆▆▆▇▆▆▇▇▆▇▆▇▇▆▆▇▇▆▇▆▇█▇█▇▇▆▇█▆▇▆▇▆▇</td></tr><tr><td>downward_MI_66</td><td>▁▅▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇█▇▇▇▇█▇▇▇█████▇█████▇</td></tr><tr><td>downward_MI_67</td><td>▁▅▅▇▇▇▇▇▇███▇█████████▇█████████████████</td></tr><tr><td>downward_MI_68</td><td>▁▅▅▆▆▇▇▇▇▇▇▇▇█▇█▇▇▇▇█▇▇██▇▇██▇██▇▇▇██▇█▇</td></tr><tr><td>downward_MI_69</td><td>▁▄▅▆▇▇▇▇▇▇█▇▇████▇▇▇█████▇███▇██████████</td></tr><tr><td>downward_MI_7</td><td>▁▅▆▆▇▇▇▇▇▇██▇███████████████████████████</td></tr><tr><td>downward_MI_70</td><td>▁▆▆▇▇▇▇▇▇██▇▇█████▇▇██▇█████████████████</td></tr><tr><td>downward_MI_71</td><td>▂▁▃▅▄▅▅▆▆▆▆▇▇▇▇▇▇▇▇▇▆▇▇█▇▇▇██▇█▇▇██▇█▇█▇</td></tr><tr><td>downward_MI_72</td><td>▁▆▆▇▇▇▇▇▇███████████████████████████████</td></tr><tr><td>downward_MI_73</td><td>▁▅▅▆▇▇▇▇▇▇▇▇█▇▇▇█▇▇▇▇▇▇▇▇██▇█▇█████████▇</td></tr><tr><td>downward_MI_74</td><td>▁▄▅▆▅▆▇▇▇▇▇▇▇▇▆▇▇█▇▇█▇██▇▇▇███▇▇▇▇██▇██▇</td></tr><tr><td>downward_MI_75</td><td>▁▄▅▆▆▆▆▆▇▇▇█▇▇▇▇▇█▇▇█▇▇██▇█▇███▇▇▇█▇███▇</td></tr><tr><td>downward_MI_76</td><td>▁▁▃▅▆▅▆▅▇▆▆▇▇▆▇▆▆▇▆▅▆▇▆█▆▇█▇▆▆▆▇▅▆▇▇▇▇█▆</td></tr><tr><td>downward_MI_77</td><td>▁▂▄▄▅▅▅▄▆▆▆▇▆▆▅▆▆▇▆▇▆▇█▇▇▇█▆▇█▆▇▆▇▇▇▇█▇▆</td></tr><tr><td>downward_MI_78</td><td>▁▁▁▃▃▄▄▄▅▄▆▆▆▅▆▅▅▅▇▆▆▅▅▅▅▄▆▇█▅▅▅▇▆▆▅▇█▆▆</td></tr><tr><td>downward_MI_79</td><td>▁▂▃▄▅▆▆▆▇▅▇▆▇▇▇▇▇▇▇▇█▇▇██▇███▇█▇███▇███▇</td></tr><tr><td>downward_MI_8</td><td>▁▅▆▅▆▆▆▇▇▇▇█▇▇██▇▇▇▇█▇█▇▇█▇█▇▇██▇▇▇█████</td></tr><tr><td>downward_MI_80</td><td>▁▁▂▄▅▅▄▆▆▇▆▇▇▇▇▆▇▇▇▇▇▇██▇▇▇████▇▇▇█▇▇█▇▇</td></tr><tr><td>downward_MI_81</td><td>▁▅▆▆▆▇▇▇▇▇▇█▇█████▇█████▇█▇█████████████</td></tr><tr><td>downward_MI_82</td><td>▁▄▅▆▆▆▆▆▆▇▆▆▇▆▇▇▇▇▇▇▇▆▇▇▇▆▇█▆▇▇▇▇▆▇▆▇▇▇▇</td></tr><tr><td>downward_MI_83</td><td>▁▄▄▅▆▆▇▆▇▇▇▇▇▇▇█▇▇█▇▇█▇█▇▇▇▇███▇▇▇▇▇█▇██</td></tr><tr><td>downward_MI_84</td><td>▁▂▄▅▆▅▆▆▆▆▅▇▇▆▇▇▇▇▆▇▇▆▇▇█▇███▇▇█▇▇▇█▇█▇▇</td></tr><tr><td>downward_MI_85</td><td>▁▃▁▅▅▅▅▆▇▇▇▇▇▆▇▆▇▇▇▇▆▇▇▇▇▆▇█████▇▇▇▇▇▇▇▇</td></tr><tr><td>downward_MI_86</td><td>▁▃▅▄▆▆▅▅▆▇▆▆▆▆▇▆▆▇▇▇▇▆▇▆▆▆▇▇▇▇██▇▆█▇▇▇▇█</td></tr><tr><td>downward_MI_87</td><td>▁▃▅▅▅▆▆▆▆▆▆▆▇▆▆▇▇█▇▇▆▇▆██▇▇█▇█▇█▇▇▇██▇▆▇</td></tr><tr><td>downward_MI_88</td><td>▁▅▅▆▆▇▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇██▇▇▇▇██▇▇▇█▇</td></tr><tr><td>downward_MI_89</td><td>▁▄▅▆▆▆▆▇▆▇▇█▇▇▇▇█▇▇▇▇▇█▇▇▇▇██▇██▇▇████▇▇</td></tr><tr><td>downward_MI_9</td><td>▁▅▆▆▆▆▇▇▇▇▇▇▇▇▇▇▇▇█▇█▇▇▇▇██▇▇███▇███▇█▇▇</td></tr><tr><td>downward_MI_90</td><td>▁▆▆▆▇▇▇▇▇▇▇▇▇█▇▇▇▇▇▇█▇▇██▇▇██▇██▇▇█▇████</td></tr><tr><td>downward_MI_91</td><td>▂▁▃▅▄▆▅▅▇▆▅▅▆▇▆▇▆▆▆▇▇▆▇▆▆▇▇▅▇▇█▆▆▆█▇▆▇▇▅</td></tr><tr><td>downward_MI_92</td><td>▁▂▂▄▄▅▅▆▇▆▆▆▆▇▆▇▇▇▇▇▇▆▇▇▇▇█▇█▇█▇▇▇█▇▇██▆</td></tr><tr><td>downward_MI_93</td><td>▁▃▄▅▅▆▆▆▆▇▆▇▇▆▇▇▇▇▇▇▇▇▇▇▇▆▇██▇▇▇▆▇▇▇██▇▇</td></tr><tr><td>downward_MI_94</td><td>▁▂▄▄▄▄▅▅▅▆▆▅▆▅▇▆▇▄▇▅▇▆▅▅▅▅▆▇▇▇█▆▆▇▇▆█▇▇▅</td></tr><tr><td>downward_MI_95</td><td>▁▃▄▅▆▆▆▆▇▆▇▆▆▇▇▇▇▇▇▇▇▇██▆▆▇▇▇▇▇▇▇▇▆▇▇█▇▇</td></tr><tr><td>downward_MI_96</td><td>▁▂▃▅▄▅▅▅▅▇▆▇▇▇▆▆█▆▇▆▇▇▇▇▇▇█▇▇▇█▇▇▇█▇██▇▇</td></tr><tr><td>downward_MI_97</td><td>▁▂▄▄▄▅▅▇▆▅▆▇▆▆▆▇▇▇▆▆▇▆▇▇▆▇▇▇▆▇██▆▇▇▆▇▆█▆</td></tr><tr><td>downward_MI_98</td><td>▁▅▆▆▆▆▇▆▇▆▇▇▇▇▇▇▇▇▇▇▇▇▇▇▇▆▇▇▇▇▇▇▇▇▇▇▇▇█▇</td></tr><tr><td>downward_MI_99</td><td>▁▄▅▆▆▆▆▆▇▇▇▆▆▇▆▇▇▆▇▇▇▇▇█▇██▇▇▇█▇▇▇█▇▇▇█▇</td></tr><tr><td>sum_downward_MI</td><td>▁▄▅▅▆▆▆▇▇▇▇▇▇▇█▇██▇██████▇██████████████</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Psi</td><td>-25.74913</td></tr><tr><td>decoupled_MI</td><td>5.85125</td></tr><tr><td>downward_MI_0</td><td>0.28967</td></tr><tr><td>downward_MI_1</td><td>0.76854</td></tr><tr><td>downward_MI_10</td><td>0.44843</td></tr><tr><td>downward_MI_11</td><td>0.60899</td></tr><tr><td>downward_MI_12</td><td>0.50459</td></tr><tr><td>downward_MI_13</td><td>0.48552</td></tr><tr><td>downward_MI_14</td><td>0.48614</td></tr><tr><td>downward_MI_15</td><td>0.474</td></tr><tr><td>downward_MI_16</td><td>0.56091</td></tr><tr><td>downward_MI_17</td><td>0.61972</td></tr><tr><td>downward_MI_18</td><td>0.36491</td></tr><tr><td>downward_MI_19</td><td>0.41655</td></tr><tr><td>downward_MI_2</td><td>0.81437</td></tr><tr><td>downward_MI_20</td><td>0.3582</td></tr><tr><td>downward_MI_21</td><td>0.42298</td></tr><tr><td>downward_MI_22</td><td>0.46526</td></tr><tr><td>downward_MI_23</td><td>0.47459</td></tr><tr><td>downward_MI_24</td><td>0.26314</td></tr><tr><td>downward_MI_25</td><td>0.39088</td></tr><tr><td>downward_MI_26</td><td>0.19409</td></tr><tr><td>downward_MI_27</td><td>0.17327</td></tr><tr><td>downward_MI_28</td><td>0.25896</td></tr><tr><td>downward_MI_29</td><td>0.19743</td></tr><tr><td>downward_MI_3</td><td>0.22333</td></tr><tr><td>downward_MI_30</td><td>0.19503</td></tr><tr><td>downward_MI_31</td><td>0.18136</td></tr><tr><td>downward_MI_32</td><td>0.28244</td></tr><tr><td>downward_MI_33</td><td>0.51083</td></tr><tr><td>downward_MI_34</td><td>0.38684</td></tr><tr><td>downward_MI_35</td><td>0.35583</td></tr><tr><td>downward_MI_36</td><td>0.16137</td></tr><tr><td>downward_MI_37</td><td>0.62052</td></tr><tr><td>downward_MI_38</td><td>0.45233</td></tr><tr><td>downward_MI_39</td><td>0.31386</td></tr><tr><td>downward_MI_4</td><td>0.3357</td></tr><tr><td>downward_MI_40</td><td>0.64774</td></tr><tr><td>downward_MI_41</td><td>0.23979</td></tr><tr><td>downward_MI_42</td><td>0.36349</td></tr><tr><td>downward_MI_43</td><td>0.36057</td></tr><tr><td>downward_MI_44</td><td>0.22297</td></tr><tr><td>downward_MI_45</td><td>0.57576</td></tr><tr><td>downward_MI_46</td><td>0.40236</td></tr><tr><td>downward_MI_47</td><td>0.33736</td></tr><tr><td>downward_MI_48</td><td>0.45608</td></tr><tr><td>downward_MI_49</td><td>0.52044</td></tr><tr><td>downward_MI_5</td><td>0.61205</td></tr><tr><td>downward_MI_50</td><td>0.31961</td></tr><tr><td>downward_MI_51</td><td>0.83208</td></tr><tr><td>downward_MI_52</td><td>0.49349</td></tr><tr><td>downward_MI_53</td><td>0.221</td></tr><tr><td>downward_MI_54</td><td>0.67329</td></tr><tr><td>downward_MI_55</td><td>0.65976</td></tr><tr><td>downward_MI_56</td><td>0.69761</td></tr><tr><td>downward_MI_57</td><td>0.84845</td></tr><tr><td>downward_MI_58</td><td>0.51937</td></tr><tr><td>downward_MI_59</td><td>0.5708</td></tr><tr><td>downward_MI_6</td><td>0.42631</td></tr><tr><td>downward_MI_60</td><td>0.63482</td></tr><tr><td>downward_MI_61</td><td>0.46364</td></tr><tr><td>downward_MI_62</td><td>0.60794</td></tr><tr><td>downward_MI_63</td><td>0.46181</td></tr><tr><td>downward_MI_64</td><td>0.43503</td></tr><tr><td>downward_MI_65</td><td>0.5142</td></tr><tr><td>downward_MI_66</td><td>0.39911</td></tr><tr><td>downward_MI_67</td><td>0.46627</td></tr><tr><td>downward_MI_68</td><td>0.62854</td></tr><tr><td>downward_MI_69</td><td>0.63935</td></tr><tr><td>downward_MI_7</td><td>0.91974</td></tr><tr><td>downward_MI_70</td><td>0.40382</td></tr><tr><td>downward_MI_71</td><td>0.57945</td></tr><tr><td>downward_MI_72</td><td>0.49264</td></tr><tr><td>downward_MI_73</td><td>0.32815</td></tr><tr><td>downward_MI_74</td><td>0.63004</td></tr><tr><td>downward_MI_75</td><td>0.55873</td></tr><tr><td>downward_MI_76</td><td>0.36564</td></tr><tr><td>downward_MI_77</td><td>0.33275</td></tr><tr><td>downward_MI_78</td><td>0.13682</td></tr><tr><td>downward_MI_79</td><td>0.38049</td></tr><tr><td>downward_MI_8</td><td>0.90225</td></tr><tr><td>downward_MI_80</td><td>0.60896</td></tr><tr><td>downward_MI_81</td><td>0.52148</td></tr><tr><td>downward_MI_82</td><td>0.34187</td></tr><tr><td>downward_MI_83</td><td>0.55842</td></tr><tr><td>downward_MI_84</td><td>0.34117</td></tr><tr><td>downward_MI_85</td><td>0.43196</td></tr><tr><td>downward_MI_86</td><td>0.25454</td></tr><tr><td>downward_MI_87</td><td>0.25908</td></tr><tr><td>downward_MI_88</td><td>0.49374</td></tr><tr><td>downward_MI_89</td><td>0.41954</td></tr><tr><td>downward_MI_9</td><td>0.54093</td></tr><tr><td>downward_MI_90</td><td>0.34643</td></tr><tr><td>downward_MI_91</td><td>0.41019</td></tr><tr><td>downward_MI_92</td><td>0.48742</td></tr><tr><td>downward_MI_93</td><td>0.37146</td></tr><tr><td>downward_MI_94</td><td>0.32759</td></tr><tr><td>downward_MI_95</td><td>0.39742</td></tr><tr><td>downward_MI_96</td><td>0.45943</td></tr><tr><td>downward_MI_97</td><td>0.41023</td></tr><tr><td>downward_MI_98</td><td>0.45506</td></tr><tr><td>downward_MI_99</td><td>0.45707</td></tr><tr><td>sum_downward_MI</td><td>46.51118</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">leafy-galaxy-11</strong> at: <a href='https://wandb.ai/dmcsharry/NEURIPS-testing-if-encoder-rep-is-emergent/runs/ehgoqnva' target=\"_blank\">https://wandb.ai/dmcsharry/NEURIPS-testing-if-encoder-rep-is-emergent/runs/ehgoqnva</a><br/> View project at: <a href='https://wandb.ai/dmcsharry/NEURIPS-testing-if-encoder-rep-is-emergent' target=\"_blank\">https://wandb.ai/dmcsharry/NEURIPS-testing-if-encoder-rep-is-emergent</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240806_200506-ehgoqnva/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "\n",
    "def get_autoencoder_representation(batch_data):\n",
    "    \"\"\"\n",
    "    Function to get the autoencoder representation of a batch of data.\n",
    "\n",
    "    Parameters:\n",
    "    - batch_data: Tensor containing the batch of data.\n",
    "\n",
    "    Returns:\n",
    "    - encoded_data: Tensor containing the autoencoder representations.\n",
    "    \"\"\"\n",
    "    # Load the model\n",
    "    model = Autoencoder(input_size, config['feature_size'], config['hidden_sizes']).to(device)\n",
    "    model.load_state_dict(torch.load('models/FMRI-data-autoencoder_model.pth'))\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        batch_data = batch_data.to(device)\n",
    "        encoded_data = model.encoder(batch_data)\n",
    "\n",
    "    return encoded_data\n",
    "\n",
    "\n",
    "\n",
    "project_name = \"NEURIPS-testing-if-encoder-rep-is-emergent\"\n",
    "\n",
    "config_test = {    \n",
    "    \"torch_seed\": seed,\n",
    "    \"dataset_type\": \"FMRI\",\n",
    "    \"num_atoms\": 100,\n",
    "    \"batch_size\": 1000,\n",
    "    \"train_mode\": False,\n",
    "    \"train_model_B\": False,\n",
    "    \"adjust_Psi\": True,\n",
    "    \"clip\": 5,\n",
    "    \"feature_size\": 3,\n",
    "    \"epochs\": 10,\n",
    "    \"start_updating_f_after\": 100,\n",
    "    \"update_f_every_N_steps\": 5,\n",
    "    \"minimize_neg_terms_until\": 0,\n",
    "    \"downward_critics_config\": {\n",
    "        \"hidden_sizes_v_critic\": [512, 1024, 1024, 512],\n",
    "        \"hidden_sizes_xi_critic\": [512, 512, 512],\n",
    "        \"critic_output_size\": 32,\n",
    "        \"lr\": 1e-3,\n",
    "        \"bias\": True,\n",
    "        \"weight_decay\": 0,\n",
    "    },\n",
    "    \n",
    "    \"decoupled_critic_config\": {\n",
    "        \"hidden_sizes_encoder_1\": [512, 512, 512],\n",
    "        \"hidden_sizes_encoder_2\": [512, 512, 512],\n",
    "        \"critic_output_size\": 32,\n",
    "        \"lr\": 1e-3,\n",
    "        \"bias\": True,\n",
    "        \"weight_decay\": 0,\n",
    "    },\n",
    "    \"feature_network_config\": {\n",
    "        \"hidden_sizes\": [256, 256, 256, 256, 256],\n",
    "        \"lr\": 1e-4,\n",
    "        \"bias\": True,\n",
    "        \"weight_decay\": 1e-6,\n",
    "    }\n",
    "}\n",
    "\n",
    "from trainer_for_RNN_rep_network import train_feature_network\n",
    "\n",
    "out = train_feature_network(\n",
    "    config=config_test,\n",
    "    trainloader=trainloader,\n",
    "    feature_network_training=get_autoencoder_representation,\n",
    "    project_name=project_name,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_258201/549845170.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  encoder.load_state_dict(torch.load(model_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/75, Loss: 0.09397035092115402\n",
      "Epoch 2/75, Loss: 0.09205859899520874\n",
      "Epoch 3/75, Loss: 0.09147310256958008\n",
      "Epoch 4/75, Loss: 0.09102212637662888\n",
      "Epoch 5/75, Loss: 0.09073235094547272\n",
      "Epoch 6/75, Loss: 0.09073550254106522\n",
      "Epoch 7/75, Loss: 0.09081390500068665\n",
      "Epoch 8/75, Loss: 0.09087085723876953\n",
      "Epoch 9/75, Loss: 0.09087539464235306\n",
      "Epoch 10/75, Loss: 0.0908716544508934\n",
      "Epoch 11/75, Loss: 0.09086067229509354\n",
      "Epoch 12/75, Loss: 0.09085692465305328\n",
      "Epoch 13/75, Loss: 0.09085702896118164\n",
      "Epoch 14/75, Loss: 0.09086278825998306\n",
      "Epoch 15/75, Loss: 0.09086089581251144\n",
      "Epoch 16/75, Loss: 0.0908709317445755\n",
      "Epoch 17/75, Loss: 0.09086737781763077\n",
      "Epoch 18/75, Loss: 0.0908680260181427\n",
      "Epoch 19/75, Loss: 0.09086751937866211\n",
      "Epoch 20/75, Loss: 0.09086717665195465\n",
      "Epoch 21/75, Loss: 0.09086473286151886\n",
      "Epoch 22/75, Loss: 0.09086532890796661\n",
      "Epoch 23/75, Loss: 0.09086686372756958\n",
      "Epoch 24/75, Loss: 0.09086661040782928\n",
      "Epoch 25/75, Loss: 0.09086403250694275\n",
      "Epoch 26/75, Loss: 0.09086406975984573\n",
      "Epoch 27/75, Loss: 0.09086274355649948\n",
      "Epoch 28/75, Loss: 0.09086685627698898\n",
      "Epoch 29/75, Loss: 0.09085991978645325\n",
      "Epoch 30/75, Loss: 0.09086267650127411\n",
      "Epoch 31/75, Loss: 0.0908607617020607\n",
      "Epoch 32/75, Loss: 0.09086114913225174\n",
      "Epoch 33/75, Loss: 0.09086021035909653\n",
      "Epoch 34/75, Loss: 0.09086212515830994\n",
      "Epoch 35/75, Loss: 0.09086249768733978\n",
      "Epoch 36/75, Loss: 0.09086381644010544\n",
      "Epoch 37/75, Loss: 0.09086239337921143\n",
      "Epoch 38/75, Loss: 0.09086335450410843\n",
      "Epoch 39/75, Loss: 0.09086301177740097\n",
      "Epoch 40/75, Loss: 0.09087128937244415\n",
      "Epoch 41/75, Loss: 0.09087488055229187\n",
      "Epoch 42/75, Loss: 0.09086458384990692\n",
      "Epoch 43/75, Loss: 0.09086563438177109\n",
      "Epoch 44/75, Loss: 0.0908629447221756\n",
      "Epoch 45/75, Loss: 0.09086597710847855\n",
      "Epoch 46/75, Loss: 0.0908629447221756\n",
      "Epoch 47/75, Loss: 0.0908643826842308\n",
      "Epoch 48/75, Loss: 0.09086132794618607\n",
      "Epoch 49/75, Loss: 0.09086047857999802\n",
      "Epoch 50/75, Loss: 0.0908583551645279\n",
      "Epoch 51/75, Loss: 0.090856172144413\n",
      "Epoch 52/75, Loss: 0.09085550904273987\n",
      "Epoch 53/75, Loss: 0.09085380285978317\n",
      "Epoch 54/75, Loss: 0.09085087478160858\n",
      "Epoch 55/75, Loss: 0.09084925800561905\n",
      "Epoch 56/75, Loss: 0.09084836393594742\n",
      "Epoch 57/75, Loss: 0.09084958583116531\n",
      "Epoch 58/75, Loss: 0.09084835648536682\n",
      "Epoch 59/75, Loss: 0.09084568172693253\n",
      "Epoch 60/75, Loss: 0.09084561467170715\n",
      "Epoch 61/75, Loss: 0.09084462374448776\n",
      "Epoch 62/75, Loss: 0.09084002673625946\n",
      "Epoch 63/75, Loss: 0.0908372700214386\n",
      "Epoch 64/75, Loss: 0.0908331498503685\n",
      "Epoch 65/75, Loss: 0.09083173424005508\n",
      "Epoch 66/75, Loss: 0.0908268541097641\n",
      "Epoch 67/75, Loss: 0.0908239334821701\n",
      "Epoch 68/75, Loss: 0.09082349389791489\n",
      "Epoch 69/75, Loss: 0.09081985801458359\n",
      "Epoch 70/75, Loss: 0.09082113951444626\n",
      "Epoch 71/75, Loss: 0.09081622958183289\n",
      "Epoch 72/75, Loss: 0.09081441164016724\n",
      "Epoch 73/75, Loss: 0.09081031382083893\n",
      "Epoch 74/75, Loss: 0.09080857038497925\n",
      "Epoch 75/75, Loss: 0.09080412238836288\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from models import SkipConnectionSupervenientFeatureNetwork\n",
    "\n",
    "# Load the pre-trained model\n",
    "model_path = 'models/NEURIPS-FMRI-model-A-jumping-serenity-12.pth'\n",
    "encoder = SkipConnectionSupervenientFeatureNetwork(\n",
    "    num_atoms=config_test['num_atoms'],\n",
    "    feature_size=config_test['feature_size'],\n",
    "    hidden_sizes=config_test['feature_network_config']['hidden_sizes'],\n",
    "    include_bias=config_test['feature_network_config']['bias']\n",
    ").to(device)\n",
    "encoder.load_state_dict(torch.load(model_path))\n",
    "encoder.eval()  # Set the encoder to evaluation mode\n",
    "\n",
    "# Define the MLP decoder\n",
    "class MLPDecoder(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(MLPDecoder, self).__init__()\n",
    "        layers = []\n",
    "        for hidden_size in hidden_sizes:\n",
    "            layers.append(nn.Linear(input_size, hidden_size))\n",
    "            layers.append(nn.ReLU())\n",
    "            input_size = hidden_size\n",
    "        layers.append(nn.Linear(input_size, output_size))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "# Initialize the decoder\n",
    "decoder = MLPDecoder(\n",
    "    input_size=3,\n",
    "    hidden_sizes=[256, 256, 256, 256, 256],\n",
    "    output_size=100\n",
    ").to(device)\n",
    "\n",
    "# Define the loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(decoder.parameters(), lr=1e-4, weight_decay=1e-6)\n",
    "\n",
    "# Training loop\n",
    "epochs = 75\n",
    "for epoch in range(epochs):\n",
    "    for i, batch in enumerate(trainloader):\n",
    "        x0 = batch[:, 0].to(device).float()\n",
    "        x1 = batch[:, 1].to(device).float()\n",
    "\n",
    "        # Get the representation from the encoder\n",
    "        with torch.no_grad():\n",
    "            representation = encoder(x0)\n",
    "\n",
    "        # Predict the next time step\n",
    "        prediction = decoder(representation)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = criterion(prediction, x1)\n",
    "        if i == 0:\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss.item()}')\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# now lets predict longer term dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_258201/3245615182.py:13: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  autoencoder.load_state_dict(torch.load(autoencoder_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch [1/29], Loss: 0.0948\n",
      "Epoch [1/30], Batch [11/29], Loss: 0.0858\n",
      "Epoch [1/30], Batch [21/29], Loss: 0.0925\n",
      "Epoch [2/30], Batch [1/29], Loss: 0.0932\n",
      "Epoch [2/30], Batch [11/29], Loss: 0.0851\n",
      "Epoch [2/30], Batch [21/29], Loss: 0.0920\n",
      "Epoch [3/30], Batch [1/29], Loss: 0.0929\n",
      "Epoch [3/30], Batch [11/29], Loss: 0.0850\n",
      "Epoch [3/30], Batch [21/29], Loss: 0.0918\n",
      "Epoch [4/30], Batch [1/29], Loss: 0.0928\n",
      "Epoch [4/30], Batch [11/29], Loss: 0.0850\n",
      "Epoch [4/30], Batch [21/29], Loss: 0.0917\n",
      "Epoch [5/30], Batch [1/29], Loss: 0.0927\n",
      "Epoch [5/30], Batch [11/29], Loss: 0.0850\n",
      "Epoch [5/30], Batch [21/29], Loss: 0.0917\n",
      "Epoch [6/30], Batch [1/29], Loss: 0.0926\n",
      "Epoch [6/30], Batch [11/29], Loss: 0.0851\n",
      "Epoch [6/30], Batch [21/29], Loss: 0.0917\n",
      "Epoch [7/30], Batch [1/29], Loss: 0.0925\n",
      "Epoch [7/30], Batch [11/29], Loss: 0.0851\n",
      "Epoch [7/30], Batch [21/29], Loss: 0.0916\n",
      "Epoch [8/30], Batch [1/29], Loss: 0.0925\n",
      "Epoch [8/30], Batch [11/29], Loss: 0.0851\n",
      "Epoch [8/30], Batch [21/29], Loss: 0.0916\n",
      "Epoch [9/30], Batch [1/29], Loss: 0.0925\n",
      "Epoch [9/30], Batch [11/29], Loss: 0.0852\n",
      "Epoch [9/30], Batch [21/29], Loss: 0.0916\n",
      "Epoch [10/30], Batch [1/29], Loss: 0.0924\n",
      "Epoch [10/30], Batch [11/29], Loss: 0.0852\n",
      "Epoch [10/30], Batch [21/29], Loss: 0.0916\n",
      "Epoch [11/30], Batch [1/29], Loss: 0.0924\n",
      "Epoch [11/30], Batch [11/29], Loss: 0.0852\n",
      "Epoch [11/30], Batch [21/29], Loss: 0.0915\n",
      "Epoch [12/30], Batch [1/29], Loss: 0.0924\n",
      "Epoch [12/30], Batch [11/29], Loss: 0.0852\n",
      "Epoch [12/30], Batch [21/29], Loss: 0.0915\n",
      "Epoch [13/30], Batch [1/29], Loss: 0.0924\n",
      "Epoch [13/30], Batch [11/29], Loss: 0.0852\n",
      "Epoch [13/30], Batch [21/29], Loss: 0.0915\n",
      "Epoch [14/30], Batch [1/29], Loss: 0.0924\n",
      "Epoch [14/30], Batch [11/29], Loss: 0.0852\n",
      "Epoch [14/30], Batch [21/29], Loss: 0.0915\n",
      "Epoch [15/30], Batch [1/29], Loss: 0.0924\n",
      "Epoch [15/30], Batch [11/29], Loss: 0.0852\n",
      "Epoch [15/30], Batch [21/29], Loss: 0.0915\n",
      "Epoch [16/30], Batch [1/29], Loss: 0.0924\n",
      "Epoch [16/30], Batch [11/29], Loss: 0.0852\n",
      "Epoch [16/30], Batch [21/29], Loss: 0.0914\n",
      "Epoch [17/30], Batch [1/29], Loss: 0.0924\n",
      "Epoch [17/30], Batch [11/29], Loss: 0.0852\n",
      "Epoch [17/30], Batch [21/29], Loss: 0.0914\n",
      "Epoch [18/30], Batch [1/29], Loss: 0.0924\n",
      "Epoch [18/30], Batch [11/29], Loss: 0.0852\n",
      "Epoch [18/30], Batch [21/29], Loss: 0.0914\n",
      "Epoch [19/30], Batch [1/29], Loss: 0.0924\n",
      "Epoch [19/30], Batch [11/29], Loss: 0.0852\n",
      "Epoch [19/30], Batch [21/29], Loss: 0.0914\n",
      "Epoch [20/30], Batch [1/29], Loss: 0.0924\n",
      "Epoch [20/30], Batch [11/29], Loss: 0.0852\n",
      "Epoch [20/30], Batch [21/29], Loss: 0.0914\n",
      "Epoch [21/30], Batch [1/29], Loss: 0.0924\n",
      "Epoch [21/30], Batch [11/29], Loss: 0.0852\n",
      "Epoch [21/30], Batch [21/29], Loss: 0.0914\n",
      "Epoch [22/30], Batch [1/29], Loss: 0.0924\n",
      "Epoch [22/30], Batch [11/29], Loss: 0.0852\n",
      "Epoch [22/30], Batch [21/29], Loss: 0.0914\n",
      "Epoch [23/30], Batch [1/29], Loss: 0.0924\n",
      "Epoch [23/30], Batch [11/29], Loss: 0.0852\n",
      "Epoch [23/30], Batch [21/29], Loss: 0.0913\n",
      "Epoch [24/30], Batch [1/29], Loss: 0.0924\n",
      "Epoch [24/30], Batch [11/29], Loss: 0.0852\n",
      "Epoch [24/30], Batch [21/29], Loss: 0.0913\n",
      "Epoch [25/30], Batch [1/29], Loss: 0.0925\n",
      "Epoch [25/30], Batch [11/29], Loss: 0.0852\n",
      "Epoch [25/30], Batch [21/29], Loss: 0.0913\n",
      "Epoch [26/30], Batch [1/29], Loss: 0.0925\n",
      "Epoch [26/30], Batch [11/29], Loss: 0.0852\n",
      "Epoch [26/30], Batch [21/29], Loss: 0.0913\n",
      "Epoch [27/30], Batch [1/29], Loss: 0.0925\n",
      "Epoch [27/30], Batch [11/29], Loss: 0.0852\n",
      "Epoch [27/30], Batch [21/29], Loss: 0.0913\n",
      "Epoch [28/30], Batch [1/29], Loss: 0.0925\n",
      "Epoch [28/30], Batch [11/29], Loss: 0.0852\n",
      "Epoch [28/30], Batch [21/29], Loss: 0.0913\n",
      "Epoch [29/30], Batch [1/29], Loss: 0.0925\n",
      "Epoch [29/30], Batch [11/29], Loss: 0.0852\n",
      "Epoch [29/30], Batch [21/29], Loss: 0.0913\n",
      "Epoch [30/30], Batch [1/29], Loss: 0.0925\n",
      "Epoch [30/30], Batch [11/29], Loss: 0.0852\n",
      "Epoch [30/30], Batch [21/29], Loss: 0.0913\n"
     ]
    }
   ],
   "source": [
    "from custom_datasets import FMRIDatasetConcatNoPrepareBatch\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "device = 'cuda'\n",
    "\n",
    "dataset = FMRIDatasetConcatNoPrepareBatch()\n",
    "trainloader = DataLoader(dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "# Load the pre-trained autoencoder\n",
    "autoencoder_path = 'models/FMRI-data-autoencoder_model.pth'\n",
    "autoencoder = Autoencoder(input_size=100, feature_size=3, hidden_sizes=[256, 256, 256, 256, 256]).to(device)\n",
    "autoencoder.load_state_dict(torch.load(autoencoder_path))\n",
    "autoencoder.eval()\n",
    "\n",
    "\n",
    "# Extract the encoder from the autoencoder\n",
    "encoder = autoencoder.encoder\n",
    "\n",
    "# Freeze the encoder parameters\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Define a new decoder for predicting n time steps ahead\n",
    "n = 20\n",
    "future_decoder = MLPDecoder(\n",
    "    input_size=3,\n",
    "    hidden_sizes=[256, 256, 256, 256, 256],\n",
    "    output_size=100\n",
    ").to(device)\n",
    "\n",
    "# Define the loss function and optimizer for the future decoder\n",
    "future_criterion = nn.MSELoss()\n",
    "future_optimizer = torch.optim.Adam(future_decoder.parameters(), lr=1e-4, weight_decay=1e-6)\n",
    "\n",
    "# Training loop for the future decoder\n",
    "future_epochs = 30\n",
    "for epoch in range(future_epochs):\n",
    "    for i, batch in enumerate(trainloader):\n",
    "        start = batch[:-n]\n",
    "        future = batch[n:]\n",
    "\n",
    "        pairs = torch.stack([start, future], dim=1).float().to(device)\n",
    "\n",
    "        x0 = pairs[:, 0]\n",
    "        xN = pairs[:, 1]\n",
    "        \n",
    "        # Predict xN from x0 using the future decoder\n",
    "        encoded_x0 = encoder(x0)\n",
    "        predicted_xN = future_decoder(encoded_x0)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = future_criterion(predicted_xN, xN)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        future_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        future_optimizer.step()\n",
    "\n",
    "        # Print the loss for every 10th batch\n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{future_epochs}], Batch [{i+1}/{len(trainloader)}], Loss: {loss.item():.4f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_258201/417848396.py:9: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  feature_network.load_state_dict(torch.load(feature_network_path))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/30], Batch [1/29], Loss: 0.0948\n",
      "Epoch [1/30], Batch [11/29], Loss: 0.0859\n",
      "Epoch [1/30], Batch [21/29], Loss: 0.0926\n",
      "Epoch [2/30], Batch [1/29], Loss: 0.0933\n",
      "Epoch [2/30], Batch [11/29], Loss: 0.0850\n",
      "Epoch [2/30], Batch [21/29], Loss: 0.0922\n",
      "Epoch [3/30], Batch [1/29], Loss: 0.0930\n",
      "Epoch [3/30], Batch [11/29], Loss: 0.0850\n",
      "Epoch [3/30], Batch [21/29], Loss: 0.0922\n",
      "Epoch [4/30], Batch [1/29], Loss: 0.0930\n",
      "Epoch [4/30], Batch [11/29], Loss: 0.0850\n",
      "Epoch [4/30], Batch [21/29], Loss: 0.0922\n",
      "Epoch [5/30], Batch [1/29], Loss: 0.0930\n",
      "Epoch [5/30], Batch [11/29], Loss: 0.0850\n",
      "Epoch [5/30], Batch [21/29], Loss: 0.0922\n",
      "Epoch [6/30], Batch [1/29], Loss: 0.0930\n",
      "Epoch [6/30], Batch [11/29], Loss: 0.0850\n",
      "Epoch [6/30], Batch [21/29], Loss: 0.0922\n",
      "Epoch [7/30], Batch [1/29], Loss: 0.0930\n",
      "Epoch [7/30], Batch [11/29], Loss: 0.0850\n",
      "Epoch [7/30], Batch [21/29], Loss: 0.0923\n",
      "Epoch [8/30], Batch [1/29], Loss: 0.0930\n",
      "Epoch [8/30], Batch [11/29], Loss: 0.0850\n",
      "Epoch [8/30], Batch [21/29], Loss: 0.0923\n",
      "Epoch [9/30], Batch [1/29], Loss: 0.0929\n",
      "Epoch [9/30], Batch [11/29], Loss: 0.0850\n",
      "Epoch [9/30], Batch [21/29], Loss: 0.0923\n",
      "Epoch [10/30], Batch [1/29], Loss: 0.0929\n",
      "Epoch [10/30], Batch [11/29], Loss: 0.0850\n",
      "Epoch [10/30], Batch [21/29], Loss: 0.0923\n",
      "Epoch [11/30], Batch [1/29], Loss: 0.0929\n",
      "Epoch [11/30], Batch [11/29], Loss: 0.0850\n",
      "Epoch [11/30], Batch [21/29], Loss: 0.0923\n",
      "Epoch [12/30], Batch [1/29], Loss: 0.0929\n",
      "Epoch [12/30], Batch [11/29], Loss: 0.0850\n",
      "Epoch [12/30], Batch [21/29], Loss: 0.0923\n",
      "Epoch [13/30], Batch [1/29], Loss: 0.0929\n",
      "Epoch [13/30], Batch [11/29], Loss: 0.0850\n",
      "Epoch [13/30], Batch [21/29], Loss: 0.0923\n",
      "Epoch [14/30], Batch [1/29], Loss: 0.0929\n",
      "Epoch [14/30], Batch [11/29], Loss: 0.0850\n",
      "Epoch [14/30], Batch [21/29], Loss: 0.0924\n",
      "Epoch [15/30], Batch [1/29], Loss: 0.0929\n",
      "Epoch [15/30], Batch [11/29], Loss: 0.0850\n",
      "Epoch [15/30], Batch [21/29], Loss: 0.0924\n",
      "Epoch [16/30], Batch [1/29], Loss: 0.0929\n",
      "Epoch [16/30], Batch [11/29], Loss: 0.0850\n",
      "Epoch [16/30], Batch [21/29], Loss: 0.0924\n",
      "Epoch [17/30], Batch [1/29], Loss: 0.0929\n",
      "Epoch [17/30], Batch [11/29], Loss: 0.0849\n",
      "Epoch [17/30], Batch [21/29], Loss: 0.0924\n",
      "Epoch [18/30], Batch [1/29], Loss: 0.0928\n",
      "Epoch [18/30], Batch [11/29], Loss: 0.0849\n",
      "Epoch [18/30], Batch [21/29], Loss: 0.0924\n",
      "Epoch [19/30], Batch [1/29], Loss: 0.0928\n",
      "Epoch [19/30], Batch [11/29], Loss: 0.0849\n",
      "Epoch [19/30], Batch [21/29], Loss: 0.0924\n",
      "Epoch [20/30], Batch [1/29], Loss: 0.0928\n",
      "Epoch [20/30], Batch [11/29], Loss: 0.0849\n",
      "Epoch [20/30], Batch [21/29], Loss: 0.0924\n",
      "Epoch [21/30], Batch [1/29], Loss: 0.0928\n",
      "Epoch [21/30], Batch [11/29], Loss: 0.0849\n",
      "Epoch [21/30], Batch [21/29], Loss: 0.0924\n",
      "Epoch [22/30], Batch [1/29], Loss: 0.0927\n",
      "Epoch [22/30], Batch [11/29], Loss: 0.0848\n",
      "Epoch [22/30], Batch [21/29], Loss: 0.0924\n",
      "Epoch [23/30], Batch [1/29], Loss: 0.0927\n",
      "Epoch [23/30], Batch [11/29], Loss: 0.0848\n",
      "Epoch [23/30], Batch [21/29], Loss: 0.0924\n",
      "Epoch [24/30], Batch [1/29], Loss: 0.0927\n",
      "Epoch [24/30], Batch [11/29], Loss: 0.0848\n",
      "Epoch [24/30], Batch [21/29], Loss: 0.0924\n",
      "Epoch [25/30], Batch [1/29], Loss: 0.0926\n",
      "Epoch [25/30], Batch [11/29], Loss: 0.0848\n",
      "Epoch [25/30], Batch [21/29], Loss: 0.0925\n",
      "Epoch [26/30], Batch [1/29], Loss: 0.0926\n",
      "Epoch [26/30], Batch [11/29], Loss: 0.0847\n",
      "Epoch [26/30], Batch [21/29], Loss: 0.0925\n",
      "Epoch [27/30], Batch [1/29], Loss: 0.0926\n",
      "Epoch [27/30], Batch [11/29], Loss: 0.0847\n",
      "Epoch [27/30], Batch [21/29], Loss: 0.0925\n",
      "Epoch [28/30], Batch [1/29], Loss: 0.0925\n",
      "Epoch [28/30], Batch [11/29], Loss: 0.0847\n",
      "Epoch [28/30], Batch [21/29], Loss: 0.0925\n",
      "Epoch [29/30], Batch [1/29], Loss: 0.0925\n",
      "Epoch [29/30], Batch [11/29], Loss: 0.0847\n",
      "Epoch [29/30], Batch [21/29], Loss: 0.0925\n",
      "Epoch [30/30], Batch [1/29], Loss: 0.0925\n",
      "Epoch [30/30], Batch [11/29], Loss: 0.0846\n",
      "Epoch [30/30], Batch [21/29], Loss: 0.0925\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the new model\n",
    "feature_network_path = 'models/NEURIPS-FMRI-model-A-jumping-serenity-12.pth'\n",
    "feature_network = SkipConnectionSupervenientFeatureNetwork(\n",
    "    num_atoms=100,\n",
    "    feature_size=3,\n",
    "    hidden_sizes=[256, 256, 256, 256, 256],\n",
    "    include_bias=True\n",
    ").to(device)\n",
    "feature_network.load_state_dict(torch.load(feature_network_path))\n",
    "\n",
    "encoder = feature_network\n",
    "\n",
    "# Freeze the encoder parameters\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Define a new decoder for predicting n time steps ahead\n",
    "n = 20\n",
    "future_decoder = MLPDecoder(\n",
    "    input_size=3,\n",
    "    hidden_sizes=[256, 256, 256, 256, 256],\n",
    "    output_size=100\n",
    ").to(device)\n",
    "\n",
    "# Define the loss function and optimizer for the future decoder\n",
    "future_criterion = nn.MSELoss()\n",
    "future_optimizer = torch.optim.Adam(future_decoder.parameters(), lr=1e-4, weight_decay=1e-6)\n",
    "\n",
    "# Training loop for the future decoder\n",
    "future_epochs = 30\n",
    "for epoch in range(future_epochs):\n",
    "    for i, batch in enumerate(trainloader):\n",
    "        start = batch[:-n]\n",
    "        future = batch[n:]\n",
    "\n",
    "        pairs = torch.stack([start, future], dim=1).float().to(device)\n",
    "\n",
    "        x0 = pairs[:, 0]\n",
    "        xN = pairs[:, 1]\n",
    "        \n",
    "        # Predict xN from x0 using the future decoder\n",
    "        encoded_x0 = encoder(x0)\n",
    "        predicted_xN = future_decoder(encoded_x0)\n",
    "\n",
    "        # Compute the loss\n",
    "        loss = future_criterion(predicted_xN, xN)\n",
    "\n",
    "        # Backpropagation and optimization\n",
    "        future_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        future_optimizer.step()\n",
    "\n",
    "        # Print the loss for every 10th batch\n",
    "        if i % 10 == 0:\n",
    "            print(f'Epoch [{epoch+1}/{future_epochs}], Batch [{i+1}/{len(trainloader)}], Loss: {loss.item():.4f}')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
