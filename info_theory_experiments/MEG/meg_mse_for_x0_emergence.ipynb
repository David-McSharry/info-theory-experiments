{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory set to: /vol/bitbucket/dm2223/info-theory-experiments\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "def setup_project_root(start_path='.'):\n",
    "    \"\"\"Find the project root, set it as the current working directory, and add it to sys.path.\"\"\"\n",
    "    current_path = os.path.abspath(start_path)\n",
    "    while True:\n",
    "        if '.git' in os.listdir(current_path):\n",
    "            project_root = current_path\n",
    "            break\n",
    "        parent_path = os.path.dirname(current_path)\n",
    "        if parent_path == current_path:  # We've reached the root directory\n",
    "            raise Exception(\"Could not find project root (.git directory not found)\")\n",
    "        current_path = parent_path\n",
    "    \n",
    "    # Change the current working directory to the project root\n",
    "    os.chdir(project_root)\n",
    "    print(f\"Current working directory set to: {os.getcwd()}\")\n",
    "\n",
    "    # Add project root to sys.path if it's not already there\n",
    "    if project_root not in sys.path:\n",
    "        sys.path.insert(0, project_root)\n",
    "        print(f\"Added {project_root} to sys.path\")\n",
    "\n",
    "# sets the current working directory to the project root\n",
    "setup_project_root()\n",
    "\n",
    "# Don't cache imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# In this notebook we train the feature network to learn a feature that cannot be deocded with to predict x0 instead of x1\n",
    "This is becuase I believe this is closer to what we want, v0 has unique info about the next time step that is not contained in any x0^i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/5\n",
      "Epoch [1/5], Decoder Loss: 0.9892, Encoder Loss: -1.1596, MI: 1.9466\n",
      "Starting epoch 2/5\n",
      "Epoch [2/5], Decoder Loss: 0.9896, Encoder Loss: -1.2283, MI: 2.3914\n",
      "Starting epoch 3/5\n",
      "Epoch [3/5], Decoder Loss: 0.9897, Encoder Loss: -1.2352, MI: 2.1369\n",
      "Starting epoch 4/5\n",
      "Epoch [4/5], Decoder Loss: 0.9899, Encoder Loss: -1.2379, MI: 2.4622\n",
      "Starting epoch 5/5\n",
      "Epoch [5/5], Decoder Loss: 0.9901, Encoder Loss: -1.2396, MI: 2.3393\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from info_theory_experiments.custom_datasets import MegDataset\n",
    "from info_theory_experiments.models import GeneralSmileMIEstimator\n",
    "\n",
    "class MLPEncoder(nn.Module):\n",
    "    def __init__(self, input_dim=116, latent_dim=1):\n",
    "        super(MLPEncoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, latent_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "class MLPDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim=1, output_dim=116):\n",
    "        super(MLPDecoder, self).__init__()\n",
    "        self.latent_dim = latent_dim\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 2048),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(2048, 1024),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(x)\n",
    "\n",
    "# Set device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "train_dataset = MegDataset()\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=128, shuffle=True)\n",
    "\n",
    "# Initialize the models, loss function, and optimizers\n",
    "encoder = MLPEncoder().to(device)\n",
    "decoder = MLPDecoder().to(device)\n",
    "mse_loss = nn.MSELoss()\n",
    "optimizer_encoder = optim.Adam(encoder.parameters(), lr=1e-4)\n",
    "optimizer_decoder = optim.Adam(decoder.parameters(), lr=1e-3)\n",
    "\n",
    "decoupled_estimator = GeneralSmileMIEstimator(\n",
    "    x_dim=1,\n",
    "    y_dim=1,\n",
    "    critic_output_size=32,\n",
    "    x_critics_hidden_sizes=[256, 256],\n",
    "    y_critics_hidden_sizes=[256, 256],\n",
    "    clip=5,\n",
    "    include_bias=True,\n",
    ").to(device)\n",
    "\n",
    "decoupled_optimizer = optim.Adam(decoupled_estimator.parameters(), lr=1e-3)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    print(f'Starting epoch {epoch+1}/{num_epochs}')\n",
    "    total_decoder_loss = 0\n",
    "    total_encoder_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data0 = data[:, 0].to(device)\n",
    "        data1 = data[:, 1].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        data0_pred = decoder(encoder(data0))\n",
    "\n",
    "        # Decoder that predicts x0 loss (minimize MSE)\n",
    "        decoder_loss = mse_loss(data0_pred, data0)\n",
    "\n",
    "        # Update decoder\n",
    "        if batch_idx % 1 == 0:\n",
    "            optimizer_decoder.zero_grad()\n",
    "            decoder_loss.backward()\n",
    "            optimizer_decoder.step()\n",
    "\n",
    "        # update decoupled estimator\n",
    "        decoupled_optimizer.zero_grad()\n",
    "        v0 = encoder(data0)\n",
    "        v1 = encoder(data1)\n",
    "        mi = decoupled_estimator(v0, v1)\n",
    "        decoupled_loss = -mi # maximize MI\n",
    "        decoupled_loss.backward()\n",
    "        decoupled_optimizer.step()\n",
    "\n",
    "        # Recompute the forward pass for encoder loss\n",
    "        if batch_idx % 1 == 0:\n",
    "            optimizer_encoder.zero_grad()\n",
    "            data0_pred = decoder(encoder(data0))\n",
    "            v0 = encoder(data0)\n",
    "            v1 = encoder(data1)\n",
    "            mi = decoupled_estimator(v0, v1)\n",
    "            encoder_loss = - 0.1 * mi - mse_loss(data0_pred, data0)\n",
    "            # Update encoder\n",
    "            optimizer_encoder.zero_grad()\n",
    "            encoder_loss.backward()\n",
    "            optimizer_encoder.step()\n",
    "        \n",
    "        total_decoder_loss += decoder_loss.item()\n",
    "        total_encoder_loss += encoder_loss.item()\n",
    "    \n",
    "    # Print epoch statistics\n",
    "    avg_decoder_loss = total_decoder_loss / len(train_loader)\n",
    "    avg_encoder_loss = total_encoder_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Decoder Loss: {avg_decoder_loss:.4f}, Encoder Loss: {avg_encoder_loss:.4f}, MI: {mi.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Average Loss: 0.9898\n",
      "Epoch [2/2], Average Loss: 0.9895\n",
      "Decoder training completed.\n",
      "Test Loss: 0.9807\n"
     ]
    }
   ],
   "source": [
    "# Train decoder with MSE loss using the trained encoder\n",
    "\n",
    "# Freeze encoder parameters\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Initialize decoder (using the MLPDecoder class from the notebook)\n",
    "decoder = MLPDecoder(latent_dim=encoder.latent_dim, output_dim=116).to(device)\n",
    "\n",
    "# Define optimizer for decoder\n",
    "optimizer_decoder = optim.Adam(decoder.parameters(), lr=1e-3)\n",
    "\n",
    "# Define loss function\n",
    "mse_loss = nn.MSELoss()\n",
    "\n",
    "num_epochs = 2  # Adjust as needed\n",
    "for epoch in range(num_epochs):\n",
    "    total_loss = 0\n",
    "    for batch_idx, data in enumerate(train_loader):\n",
    "        data0 = data[:, 0].to(device)\n",
    "        data1 = data[:, 1].to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        with torch.no_grad():\n",
    "            encoded = encoder(data0)\n",
    "        decoded = decoder(encoded)\n",
    "\n",
    "        # Compute loss\n",
    "        loss = mse_loss(decoded, data1)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        optimizer_decoder.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_decoder.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Print epoch statistics\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}')\n",
    "\n",
    "# Unfreeze encoder parameters (if needed for future use)\n",
    "for param in encoder.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "print(\"Decoder training completed.\")\n",
    "\n",
    "# Optionally, you can evaluate the model here\n",
    "# For example:\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "with torch.no_grad():\n",
    "    test_data = next(iter(train_loader))\n",
    "    test_input = test_data[:, 0].to(device)\n",
    "    test_target = test_data[:, 1].to(device)\n",
    "    encoded = encoder(test_input)\n",
    "    reconstructed = decoder(encoded)\n",
    "    test_loss = mse_loss(reconstructed, test_target)\n",
    "    print(f\"Test Loss: {test_loss.item():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mdmcsharry\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.7 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/vol/bitbucket/dm2223/info-theory-experiments/wandb/run-20240828_031623-wn4ghcqy</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dmcsharry/meg-min-mse-_with_x0-max-mi-verification/runs/wn4ghcqy' target=\"_blank\">grateful-blaze-1</a></strong> to <a href='https://wandb.ai/dmcsharry/meg-min-mse-_with_x0-max-mi-verification' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dmcsharry/meg-min-mse-_with_x0-max-mi-verification' target=\"_blank\">https://wandb.ai/dmcsharry/meg-min-mse-_with_x0-max-mi-verification</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dmcsharry/meg-min-mse-_with_x0-max-mi-verification/runs/wn4ghcqy' target=\"_blank\">https://wandb.ai/dmcsharry/meg-min-mse-_with_x0-max-mi-verification/runs/wn4ghcqy</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/2 [06:13<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 36\u001b[0m\n\u001b[1;32m      4\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtorch_seed\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed,\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset_type\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeg\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     31\u001b[0m     },\n\u001b[1;32m     32\u001b[0m }\n\u001b[1;32m     34\u001b[0m project_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeg-min-mse-_with_x0-max-mi-verification\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 36\u001b[0m _, _ \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_feature_network\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfeature_network_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     40\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/dm2223/info-theory-experiments/info_theory_experiments/trainers.py:192\u001b[0m, in \u001b[0;36mtrain_feature_network\u001b[0;34m(config, trainloader, feature_network_training, project_name, feature_network_A, model_dir_prefix)\u001b[0m\n\u001b[1;32m    190\u001b[0m downward_optims[i]\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m    191\u001b[0m channel_i \u001b[38;5;241m=\u001b[39m x0[:, i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mdetach()\n\u001b[0;32m--> 192\u001b[0m downward_MI_i \u001b[38;5;241m=\u001b[39m \u001b[43mdownward_MI_estimators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv1_B\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel_i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    193\u001b[0m downward_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m downward_MI_i\n\u001b[1;32m    194\u001b[0m downward_loss\u001b[38;5;241m.\u001b[39mbackward(retain_graph\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m/vol/bitbucket/dm2223/info-theory-experiments/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/dm2223/info-theory-experiments/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/vol/bitbucket/dm2223/info-theory-experiments/info_theory_experiments/models.py:213\u001b[0m, in \u001b[0;36mDownwardSmileMIEstimator.forward\u001b[0;34m(self, v1, x0i)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, v1, x0i):\n\u001b[1;32m    212\u001b[0m     v1_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mv_encoder(v1)\n\u001b[0;32m--> 213\u001b[0m     x0i_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43matom_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx0i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    215\u001b[0m     scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(v1_encoded, x0i_encoded\u001b[38;5;241m.\u001b[39mt())\n\u001b[1;32m    216\u001b[0m     MI \u001b[38;5;241m=\u001b[39m estimate_mutual_information(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msmile\u001b[39m\u001b[38;5;124m'\u001b[39m, scores, clip\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclip)\n",
      "File \u001b[0;32m/vol/bitbucket/dm2223/info-theory-experiments/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/dm2223/info-theory-experiments/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/vol/bitbucket/dm2223/info-theory-experiments/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/vol/bitbucket/dm2223/info-theory-experiments/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/vol/bitbucket/dm2223/info-theory-experiments/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/vol/bitbucket/dm2223/info-theory-experiments/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from info_theory_experiments.trainers import train_feature_network\n",
    "seed = 3\n",
    "torch.manual_seed(seed)\n",
    "config = {\n",
    "    \"torch_seed\": seed,\n",
    "    \"dataset_type\": \"meg\",\n",
    "    \"num_atoms\": 116,\n",
    "    \"batch_size\": 1000,\n",
    "    \"train_mode\": False,\n",
    "    \"train_model_B\": False,\n",
    "    \"adjust_Psi\": True,\n",
    "    \"clip\": 5,\n",
    "    \"feature_size\": 1,\n",
    "    \"epochs\": 2,\n",
    "    \"downward_critics_config\": {\n",
    "        \"hidden_sizes_v_critic\": [512, 1024, 1024, 512],\n",
    "        \"hidden_sizes_xi_critic\": [512, 512, 512],\n",
    "        \"critic_output_size\": 32,\n",
    "        \"lr\": 1e-3,\n",
    "        \"bias\": True,\n",
    "        \"weight_decay\": 0,\n",
    "    },\n",
    "    \n",
    "    \"decoupled_critic_config\": {\n",
    "        \"hidden_sizes_encoder_1\": [512, 512, 512],\n",
    "        \"hidden_sizes_encoder_2\": [512, 512, 512],\n",
    "        \"critic_output_size\": 32,\n",
    "        \"lr\": 1e-3,\n",
    "        \"bias\": True,\n",
    "        \"weight_decay\": 0,\n",
    "    },\n",
    "}\n",
    "\n",
    "project_name = \"meg-min-mse-_with_x0-max-mi-verification\"\n",
    "\n",
    "_, _ = train_feature_network(\n",
    "    config=config,\n",
    "    trainloader=train_loader,\n",
    "    feature_network_training=encoder,\n",
    "    project_name=project_name,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
