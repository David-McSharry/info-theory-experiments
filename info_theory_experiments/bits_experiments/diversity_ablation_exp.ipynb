{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory set to: /vol/bitbucket/dm2223/info-theory-experiments\n",
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "\n",
    "def setup_project_root(start_path='.'):\n",
    "    \"\"\"Find the project root, set it as the current working directory, and add it to sys.path.\"\"\"\n",
    "    current_path = os.path.abspath(start_path)\n",
    "    while True:\n",
    "        if '.git' in os.listdir(current_path):\n",
    "            project_root = current_path\n",
    "            break\n",
    "        parent_path = os.path.dirname(current_path)\n",
    "        if parent_path == current_path:  # We've reached the root directory\n",
    "            raise Exception(\"Could not find project root (.git directory not found)\")\n",
    "        current_path = parent_path\n",
    "    \n",
    "    # Change the current working directory to the project root\n",
    "    os.chdir(project_root)\n",
    "    print(f\"Current working directory set to: {os.getcwd()}\")\n",
    "\n",
    "    # Add project root to sys.path if it's not already there\n",
    "    if project_root not in sys.path:\n",
    "        sys.path.insert(0, project_root)\n",
    "        print(f\"Added {project_root} to sys.path\")\n",
    "\n",
    "# sets the current working directory to the project root\n",
    "setup_project_root()\n",
    "\n",
    "# Don't cache imports\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/vol/bitbucket/dm2223/info-theory-experiments/info_theory_experiments/bits_experiments\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'custom_datasets'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(os\u001b[38;5;241m.\u001b[39mgetcwd())\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mcustom_datasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m BitStringDataset\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SupervenientFeatureNetwork\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'custom_datasets'"
     ]
    }
   ],
   "source": [
    "from custom_datasets import BitStringDataset\n",
    "from models import SupervenientFeatureNetwork\n",
    "import torch\n",
    "from models import SkipConnectionSupervenientFeatureNetwork\n",
    "from trainers import train_feature_network\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment to show that diversity loss term is needed\n",
    "This will work by training 10 different feature networks, freezing them, running them in eval mode, and showing they all learn the same bit\n",
    "\n",
    "When we verify we will use weak critics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Finishing last run (ID:o4sgif87) before initializing another..."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Psi</td><td>▂▁▅▇▇▇█▇▆█▆▇▆▇▆▄▅▆▅▅▅▄▄▅▂▅▅▅▅▅▄▂▅▃▄▅▃▄▃▅</td></tr><tr><td>bonus_bit_MI</td><td>▁▁▁▂▄▇▇▂▇▆▇▄▃▇█▇▇▇▇▇▆▆▁█▇█▇▇█▇▇▄▅▇█▆▅█▇▇</td></tr><tr><td>decoupled_MI</td><td>▂▁▅▇▆▇█▇▆█▆████▇█▆▇▇█▇▇█▅█████▇▅▇▇██▇█▆█</td></tr><tr><td>downward_MI_0</td><td>▅▁█▆▇▇▇▇██▇▇▇▇▇█▇▇▇█▇▇▇▇█▇▇▆▇███▇▇▇▇█▇▇█</td></tr><tr><td>downward_MI_1</td><td>▁▄▇▄▃▅▅▅▄▅▃▅▅▅▅▅▅▅▄▅▅▅█▅▇▄▅█▅▄▅▅▅▅▅▄▆▆▄▅</td></tr><tr><td>downward_MI_2</td><td>█▄▂▁▃▂▂▁▂▃▁▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂</td></tr><tr><td>downward_MI_3</td><td>▇▅▂▄█▅▅▅▅▅▅▅▅▅▅▅▅▅▁▄▅▅▅▅▅▅▅▅▅▄▅▅▅▅▅▅▅▅▅▅</td></tr><tr><td>downward_MI_4</td><td>█▂▂▅▂▁▂▂▃▂▂▂▂▂▂▂▂▂▁▂▂▂▂▂▂▃▂▂▂▂▂▂▂▂▂▂▂▂▂▂</td></tr><tr><td>downward_MI_5</td><td>▁▁▁▁▁▁▁▁▂▂▂▂▃▃▄▆▄▅▄▂▅▆▇▄▄▇▅▆▆▆▇▇▅▆▇▆██▇▆</td></tr><tr><td>extra_bit_MI</td><td>▁▁▁▁▁▂▃▄▅▄▆▃▅▆▅▇▆▅▅▂▇█▇▃▅▇▇▇▇▆█▇▇▇█▇████</td></tr><tr><td>sum_downward_MI</td><td>▁▁▁▁▁▁▁▁▂▂▃▂▄▃▅▆▆▁▆▄▇▇▆▇▆▇▅▇▅▆▇▇▆▇▇▆██▇▇</td></tr><tr><td>xor_MI</td><td>▁▆▆▇▇███▇█▆█▇██▇▇▄█▇█▇██▅▇█▇▇▇▇▅█▆▇█▇█▆▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Psi</td><td>0.48571</td></tr><tr><td>bonus_bit_MI</td><td>0.48987</td></tr><tr><td>decoupled_MI</td><td>0.89267</td></tr><tr><td>downward_MI_0</td><td>-1e-05</td></tr><tr><td>downward_MI_1</td><td>0.0</td></tr><tr><td>downward_MI_2</td><td>-0.0</td></tr><tr><td>downward_MI_3</td><td>-0.0</td></tr><tr><td>downward_MI_4</td><td>-1e-05</td></tr><tr><td>downward_MI_5</td><td>0.48272</td></tr><tr><td>extra_bit_MI</td><td>0.54451</td></tr><tr><td>sum_downward_MI</td><td>0.40696</td></tr><tr><td>xor_MI</td><td>0.99078</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">fallen-jazz-5</strong> at: <a href='https://wandb.ai/dmcsharry/NEURIPS-diversity-ablation-test/runs/o4sgif87' target=\"_blank\">https://wandb.ai/dmcsharry/NEURIPS-diversity-ablation-test/runs/o4sgif87</a><br/> View project at: <a href='https://wandb.ai/dmcsharry/NEURIPS-diversity-ablation-test' target=\"_blank\">https://wandb.ai/dmcsharry/NEURIPS-diversity-ablation-test</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240802_191522-o4sgif87/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "The new W&B backend becomes opt-out in version 0.18.0; try it out with `wandb.require(\"core\")`! See https://wandb.me/wandb-core for more information."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Successfully finished last run (ID:o4sgif87). Initializing new run:<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.5"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/ubuntu/info-theory-experiments/wandb/run-20240802_191831-dmej7kcz</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/dmcsharry/NEURIPS-diversity-ablation-training/runs/dmej7kcz' target=\"_blank\">vivid-water-6</a></strong> to <a href='https://wandb.ai/dmcsharry/NEURIPS-diversity-ablation-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/dmcsharry/NEURIPS-diversity-ablation-training' target=\"_blank\">https://wandb.ai/dmcsharry/NEURIPS-diversity-ablation-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/dmcsharry/NEURIPS-diversity-ablation-training/runs/dmej7kcz' target=\"_blank\">https://wandb.ai/dmcsharry/NEURIPS-diversity-ablation-training/runs/dmej7kcz</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|████      | 2/5 [01:36<02:24, 48.25s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 65\u001b[0m\n\u001b[1;32m     56\u001b[0m skip_model \u001b[38;5;241m=\u001b[39m SkipConnectionSupervenientFeatureNetwork(\n\u001b[1;32m     57\u001b[0m     num_atoms\u001b[38;5;241m=\u001b[39mbits_config_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_atoms\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     58\u001b[0m     feature_size\u001b[38;5;241m=\u001b[39mbits_config_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_size\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     59\u001b[0m     hidden_sizes\u001b[38;5;241m=\u001b[39mbits_config_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_network_config\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_sizes\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     60\u001b[0m     include_bias\u001b[38;5;241m=\u001b[39mbits_config_train[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfeature_network_config\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     61\u001b[0m )\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     63\u001b[0m project_name_train \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNEURIPS-diversity-ablation-training\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 65\u001b[0m skip_model \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_feature_network\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbits_config_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfeature_network_training\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mskip_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m        \u001b[49m\u001b[43mproject_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mproject_name_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_dir_prefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m     71\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m bits_config_test \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     74\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgamma_parity\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.99\u001b[39m,\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgamma_extra\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m0.99\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m         }\n\u001b[1;32m    113\u001b[0m }\n\u001b[1;32m    115\u001b[0m project_name_test \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNEURIPS-diversity-ablation-test\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/info-theory-experiments/trainers.py:223\u001b[0m, in \u001b[0;36mtrain_feature_network\u001b[0;34m(config, trainloader, feature_network_training, project_name, feature_network_A, model_dir_prefix)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_atoms\u001b[39m\u001b[38;5;124m'\u001b[39m]): \u001b[38;5;66;03m# finds sum of downward terms\u001b[39;00m\n\u001b[1;32m    222\u001b[0m     channel_i \u001b[38;5;241m=\u001b[39m x0[:, i]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 223\u001b[0m     channel_i_MI \u001b[38;5;241m=\u001b[39m \u001b[43mdownward_MI_estimators\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv1_B\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchannel_i\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    224\u001b[0m     downward_MIs_post_update\u001b[38;5;241m.\u001b[39mappend(channel_i_MI)\n\u001b[1;32m    226\u001b[0m sum_downward_MI_post_update \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(downward_MIs_post_update)\n",
      "File \u001b[0;32m~/info-theory-experiments/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/info-theory-experiments/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/info-theory-experiments/models.py:212\u001b[0m, in \u001b[0;36mDownwardSmileMIEstimator.forward\u001b[0;34m(self, v1, x0i)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, v1, x0i):\n\u001b[0;32m--> 212\u001b[0m     v1_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mv_encoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    213\u001b[0m     x0i_encoded \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39matom_encoder(x0i)\n\u001b[1;32m    215\u001b[0m     scores \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(v1_encoded, x0i_encoded\u001b[38;5;241m.\u001b[39mt())\n",
      "File \u001b[0;32m~/info-theory-experiments/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/info-theory-experiments/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/info-theory-experiments/venv/lib/python3.10/site-packages/torch/nn/modules/container.py:219\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 219\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/info-theory-experiments/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/info-theory-experiments/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/info-theory-experiments/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:117\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "for seed in range(6,10):\n",
    "    bits_config_train = {\n",
    "            \"gamma_parity\": 0.99,\n",
    "            \"gamma_extra\": 0.99,\n",
    "            \"dataset_length\": 1000000,\n",
    "            \"torch_seed\": seed,\n",
    "            \"dataset_type\": \"bits\",\n",
    "            \"num_atoms\": 6,\n",
    "            \"batch_size\": 1000,\n",
    "            \"train_mode\": True,\n",
    "            \"train_model_B\": False,\n",
    "            \"adjust_Psi\": False,\n",
    "            \"clip\": 5,\n",
    "            \"feature_size\": 1,\n",
    "            \"epochs\": 5,\n",
    "            \"start_updating_f_after\": 1000,\n",
    "            \"update_f_every_N_steps\": 5,\n",
    "            \"minimize_neg_terms_until\": 0,\n",
    "            \"downward_critics_config\": {\n",
    "                \"hidden_sizes_v_critic\": [512, 512, 512, 256],\n",
    "                \"hidden_sizes_xi_critic\": [512, 512, 512, 256],\n",
    "                \"critic_output_size\": 32,\n",
    "                \"lr\": 1e-3,\n",
    "                \"bias\": True,\n",
    "                \"weight_decay\": 0,\n",
    "            },\n",
    "            \n",
    "            \"decoupled_critic_config\": {\n",
    "                \"hidden_sizes_encoder_1\": [512, 512, 512],\n",
    "                \"hidden_sizes_encoder_2\": [512, 512, 512],\n",
    "                \"critic_output_size\": 32,\n",
    "                \"lr\": 1e-3,\n",
    "                \"bias\": True,\n",
    "                \"weight_decay\": 0,\n",
    "            },\n",
    "            \"feature_network_config\": {\n",
    "                \"hidden_sizes\": [256, 256],\n",
    "                \"lr\": 1e-4,\n",
    "                \"bias\": True,\n",
    "                \"weight_decay\": 0.00001,\n",
    "            }\n",
    "    }\n",
    "\n",
    "    dataset = BitStringDataset(\n",
    "        gamma_extra=bits_config_train[\"gamma_extra\"],\n",
    "        gamma_parity=bits_config_train[\"gamma_parity\"],\n",
    "        length=bits_config_train[\"dataset_length\"],\n",
    "    )\n",
    "\n",
    "    trainloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=bits_config_train[\"batch_size\"], shuffle=False\n",
    "    )\n",
    "\n",
    "    skip_model = SkipConnectionSupervenientFeatureNetwork(\n",
    "        num_atoms=bits_config_train['num_atoms'],\n",
    "        feature_size=bits_config_train['feature_size'],\n",
    "        hidden_sizes=bits_config_train['feature_network_config']['hidden_sizes'],\n",
    "        include_bias=bits_config_train['feature_network_config']['bias'],\n",
    "    ).to(device)\n",
    "\n",
    "    project_name_train = \"NEURIPS-diversity-ablation-training\"\n",
    "\n",
    "    skip_model = train_feature_network(\n",
    "            config=bits_config_train,\n",
    "            trainloader=trainloader,\n",
    "            feature_network_training=skip_model,\n",
    "            project_name=project_name_train,\n",
    "            model_dir_prefix=None\n",
    "    )\n",
    "\n",
    "    bits_config_test = {\n",
    "            \"gamma_parity\": 0.99,\n",
    "            \"gamma_extra\": 0.99,\n",
    "            \"dataset_length\": 1000000,\n",
    "            \"torch_seed\": seed,\n",
    "            \"dataset_type\": \"bits\",\n",
    "            \"num_atoms\": 6,\n",
    "            \"batch_size\": 1000,\n",
    "            \"train_mode\": False,\n",
    "            \"train_model_B\": False,\n",
    "            \"adjust_Psi\": False,\n",
    "            \"clip\": 5,\n",
    "            \"feature_size\": 1,\n",
    "            \"epochs\": 2,\n",
    "            \"start_updating_f_after\": 1000,\n",
    "            \"update_f_every_N_steps\": 5,\n",
    "            \"minimize_neg_terms_until\": 0,\n",
    "            \"downward_critics_config\": {\n",
    "                \"hidden_sizes_v_critic\": [256, 256, 256],\n",
    "                \"hidden_sizes_xi_critic\": [256, 256, 256],\n",
    "                \"critic_output_size\": 32,\n",
    "                \"lr\": 1e-3,\n",
    "                \"bias\": True,\n",
    "                \"weight_decay\": 0,\n",
    "            },\n",
    "            \n",
    "            \"decoupled_critic_config\": {\n",
    "                \"hidden_sizes_encoder_1\": [256, 256],\n",
    "                \"hidden_sizes_encoder_2\": [256, 256],\n",
    "                \"critic_output_size\": 32,\n",
    "                \"lr\": 1e-3,\n",
    "                \"bias\": True,\n",
    "                \"weight_decay\": 0,\n",
    "            },\n",
    "            \"feature_network_config\": {\n",
    "                \"hidden_sizes\": [256, 256],\n",
    "                \"lr\": 1e-4,\n",
    "                \"bias\": True,\n",
    "                \"weight_decay\": 0.00001,\n",
    "            }\n",
    "    }\n",
    "\n",
    "    project_name_test = \"NEURIPS-diversity-ablation-test\"\n",
    "\n",
    "    skil_model = train_feature_network(\n",
    "            config=bits_config_test,\n",
    "            trainloader=trainloader,\n",
    "            feature_network_training=skip_model,\n",
    "            project_name=project_name_test,\n",
    "            model_dir_prefix=None\n",
    "    )\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
